Nice — I’ll (A) explain at a high level what I’m going to change and why, then (B) give a ready-to-run amended version of your script that implements those three targeted changes:

Convert ASPP and decoder convs to depthwise-separable convolutions (fewer params, similar power).

Add a third ASPP dilation rate (18) to improve multi-scale context.

Extend the backbone by one stage (keep an extra layer from MobileNetV3 to improve semantics).

I also make the build robust by querying the backbone’s actual feature channel counts at init time using a small dummy tensor — this prevents shape mismatches across torchvision versions. I kept the rest of your design (mid skip, single-stage decoder) intact to minimize parameter increases.

A — What each change does (technical summary)

Depthwise-separable convs (SepConv)

Regular 3×3 conv with in→out does both cross-channel mixing and spatial filtering in one step.

Depthwise-separable: first a depthwise conv (per-channel spatial filtering) then a pointwise 1×1 conv (cross-channel mixing).

Benefit: far fewer parameters and FLOPs for the same kernel size, typically similar representational power when used carefully.

Where used: every ASPP atrous branch and the decoder conv blocks.

ASPP rates = {6, 12, 18}

Adds a third receptive-field scale so the module better captures small → large context sizes.

We keep each branch relatively narrow (out_channels=64 for fused output) to limit parameter growth.

Extend backbone one stage

Keep one more entry from backbone.features (we use :11 instead of :10). That gives slightly deeper, more semantic-high features.

To avoid hard-coding channel sizes, the model measures the channel width of the low/mid/high taps using a dummy tensor during __init__, then builds ASPP/decoder sized to match.

B — Amended code

Copy this code into your project file (replace the old model). It is self-contained and includes the depthwise separable implementations, ASPP with three rates, automatic channel probing, and the extended backbone stage.

"""
Ultra-Compact MobileNetV3-Small Segmentation Model (v2)
Changes from original:
- Extended backbone by one stage (uses first 11 feature blocks)
- ASPP uses 3 dilation rates (6,12,18)
- ASPP and decoder convolutional blocks use depthwise-separable convs (SepConv)
- Channel sizes dynamically determined by a dummy forward pass during __init__
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights


# -----------------------
# Utility: Depthwise Separable Conv Block
# -----------------------
class SepConv(nn.Module):
    """
    Depthwise separable convolution block: Depthwise (spatial) -> Pointwise (1x1) -> BN -> Activation
    Can accept dilation for the depthwise conv.
    """
    def __init__(self, in_ch, out_ch, kernel_size=3, padding=1, dilation=1, bias=False, activation=nn.ReLU):
        super(SepConv, self).__init__()
        # Depthwise conv (per-channel spatial filtering)
        self.depthwise = nn.Conv2d(in_ch, in_ch, kernel_size=kernel_size,
                                   padding=padding, dilation=dilation, groups=in_ch, bias=bias)
        self.bn1 = nn.BatchNorm2d(in_ch)
        # Pointwise (1x1) conv to mix channels
        self.pointwise = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=bias)
        self.bn2 = nn.BatchNorm2d(out_ch)
        self.act = activation(inplace=True) if activation is not None else nn.Identity()

    def forward(self, x):
        x = self.depthwise(x)
        x = self.bn1(x)
        x = self.act(x)
        x = self.pointwise(x)
        x = self.bn2(x)
        x = self.act(x)
        return x


# -----------------------
# ASPP Module (with separable convs)
# -----------------------
class ASPP(nn.Module):
    """
    Lightweight ASPP using depthwise-separable atrous branches.
    branches:
      - 1x1 conv (pointwise)
      - SepConv (dilation=r) for each rate
      - global avg pool -> 1x1 conv
    After concat -> 1x1 project -> BN -> ReLU -> Dropout
    """
    def __init__(self, in_channels, out_channels, rates=(6, 12, 18)):
        super(ASPP, self).__init__()
        self.branches = nn.ModuleList()

        # 1x1 conv branch (implemented as pointwise conv)
        self.branches.append(nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        ))

        # Atrous separable conv branches
        for rate in rates:
            # depthwise sep conv with dilation
            padding = rate
            self.branches.append(SepConv(in_channels, out_channels, kernel_size=3, padding=padding, dilation=rate))

        # Global pooling branch
        self.branches.append(nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        ))

        num_branches = len(self.branches)
        self.project = nn.Sequential(
            nn.Conv2d(out_channels * num_branches, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5)
        )

    def forward(self, x):
        h, w = x.shape[2:]
        outs = []
        for i, branch in enumerate(self.branches):
            out = branch(x)
            # The last branch is global pool which we upsample to (h,w)
            if i == len(self.branches) - 1:
                out = F.interpolate(out, size=(h, w), mode='bilinear', align_corners=False)
            outs.append(out)
        x = torch.cat(outs, dim=1)
        x = self.project(x)
        return x


# -----------------------
# The revised UltraCompactSegmentationModel
# -----------------------
class UltraCompactSegmentationModel(nn.Module):
    """
    Revised ultra-compact segmentation model:
      - Extended backbone: keeps first 11 feature blocks from mobilenet_v3_small
      - ASPP with rates [6,12,18] and separable convs
      - Decoder uses separable convs
      - Dynamically probes channel widths (low/mid/high) at init time using a dummy pass
    """
    def __init__(self, num_classes=21, pretrained=True, aspp_out=64, mid_proj_ch=24):
        super(UltraCompactSegmentationModel, self).__init__()
        # Load backbone (pretrained weights optional)
        if pretrained:
            weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1
            backbone = mobilenet_v3_small(weights=weights)
        else:
            backbone = mobilenet_v3_small(weights=None)

        # Keep a truncated backbone: first 11 feature blocks (extended by one stage vs original)
        # Using 11 rather than 10 to capture slightly deeper semantics.
        self.features = nn.Sequential(*list(backbone.features)[:11])

        # We'll discover channel widths by running a dummy forward through self.features
        # This avoids hard-coded channel numbers and works across torchvision versions.
        self._probe_feature_channels()

        # ASPP (use high_ch discovered)
        self.aspp = ASPP(in_channels=self.high_ch, out_channels=aspp_out, rates=(6, 12, 18))

        # mid projection: take discovered mid_ch -> mid_proj_ch
        self.mid_conv = nn.Sequential(
            nn.Conv2d(self.mid_ch, mid_proj_ch, kernel_size=1, bias=False),
            nn.BatchNorm2d(mid_proj_ch),
            nn.ReLU(inplace=True)
        )
        self.mid_proj_ch = mid_proj_ch

        # Decoder: concatenation of ASPP(out=aspp_out) upsampled to mid size + mid_proj_ch
        decoder_in_ch = aspp_out + mid_proj_ch
        # Use separable conv blocks in decoder (two small SepConv blocks for better mixing)
        self.decoder = nn.Sequential(
            SepConv(decoder_in_ch, 64, kernel_size=3, padding=1),
            SepConv(64, 64, kernel_size=3, padding=1),
        )

        # Final classifier head
        self.classifier = nn.Sequential(
            nn.Dropout(0.3),  # lower dropout than before
            nn.Conv2d(64, num_classes, kernel_size=1)
        )

        # Initialize new weights
        self._init_weights()

    def _probe_feature_channels(self):
        """
        Run a tiny dummy pass through self.features to find low/mid/high channel widths.
        Captures:
          - low_feat at i==1 (early)
          - mid_feat at i==3 (middle)
          - high_feat: final output of truncated features
        """
        self.features.eval()
        with torch.no_grad():
            # dummy size matches typical input; smaller spatial dims save compute for init-time probe
            dummy = torch.zeros(1, 3, 224, 224)
            x = dummy
            low_feat = None
            mid_feat = None
            for i, layer in enumerate(self.features):
                x = layer(x)
                if i == 1:
                    low_feat = x
                if i == 3:
                    mid_feat = x
            high_feat = x

        # Save discovered channel widths
        # Fallbacks in case features were not captured — should not happen on standard mobilenet_v3_small
        self.low_ch = low_feat.shape[1] if low_feat is not None else 16
        self.mid_ch = mid_feat.shape[1] if mid_feat is not None else 24
        self.high_ch = high_feat.shape[1]

    def _init_weights(self):
        """Initialize weights for new layers only (not the pretrained backbone)."""
        for module in [self.aspp, self.mid_conv, self.decoder, self.classifier]:
            for m in module.modules():
                if isinstance(m, nn.Conv2d):
                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.BatchNorm2d):
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)

    def forward(self, x, return_features=False):
        input_shape = x.shape[-2:]
        low_feat = None
        mid_feat = None
        high_feat = None

        # run through truncated backbone and capture taps
        for i, layer in enumerate(self.features):
            x = layer(x)
            if i == 1:
                low_feat = x  # stride ~4
            elif i == 3:
                mid_feat = x  # stride ~8
        high_feat = x  # final truncated feature (stride ~16)

        # project mid-level features
        mid_proj = self.mid_conv(mid_feat)

        # ASPP on high-level features
        x_aspp = self.aspp(high_feat)

        # Upsample ASPP output to mid spatial size and fuse
        x = F.interpolate(x_aspp, size=mid_proj.shape[-2:], mode='bilinear', align_corners=False)
        x = torch.cat([x, mid_proj], dim=1)

        # Decoder (separable convs)
        x = self.decoder(x)

        # Upsample to original resolution and classify
        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)
        output = self.classifier(x)

        if return_features:
            features = {
                'low': low_feat,
                'mid': mid_feat,
                'high': high_feat
            }
            return output, features

        return output


def count_parameters(model):
    """Count the number of trainable parameters"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


# -----------------------
# Quick sanity test (do not execute in heavy environments)
# -----------------------
if __name__ == '__main__':
    print("=" * 70)
    print("Ultra-Compact MobileNetV3-Small Segmentation Model (v2)")
    print("=" * 70)

    # Create model
    model = UltraCompactSegmentationModel(num_classes=21, pretrained=False)

    # Count parameters
    params = count_parameters(model)
    print(f"\nTotal parameters: {params:,} ({params/1e6:.4f}M)")

    # Test forward pass
    print("\nTesting forward pass...")
    dummy_input = torch.randn(2, 3, 224, 224)

    with torch.no_grad():
        output = model(dummy_input)
        print(f"  Output shape: {output.shape}")

        # Test with feature extraction
        output, features = model(dummy_input, return_features=True)
        print(f"\n  Features for knowledge distillation:")
        for name, feat in features.items():
            print(f"    {name}: {feat.shape}")

    print("\n" + "=" * 70)
    print("Model ready. Remember to test on a validation subset and compare scores.")
    print("=" * 70)

C — Implementation notes & pitfalls (detailed)

Probing channel widths

I run a torch.zeros dummy through the truncated backbone inside __init__ to discover low_ch, mid_ch, and high_ch. That makes the code robust to torchvision/backbone updates. The dummy is small and only used at model construction — it doesn’t affect training.

Why SepConv instead of standard conv

Example param difference: standard 3×3 conv from C_in→C_out uses C_in * C_out * 3 * 3 parameters. Depthwise separable uses C_in * 3 * 3 + C_in * C_out — typically much smaller when C_out is similar to C_in. This lowers memory and CPU.

Why add dilation=18 branch

The three dilations (6,12,18) give good coverage for small/medium/large objects. With separable convs the cost is acceptable.

Decoder depth

I used two SepConv blocks in the decoder (instead of one regular 3×3). This improves mixing after concatenation with modest param cost.

Dropout

I reduced dropout to 0.3 in the classifier as recommended earlier. You can tune this.

Training details

Keep the teacher frozen and distill as you planned. Because channel widths are probed dynamically, your KD adapters (1×1 projections for feature KD) can be built using discovered channel sizes (low_ch, mid_ch, high_ch) if you add them later.

If you set pretrained=True the backbone parameters are loaded and included in count_parameters. If you want to freeze backbone at first training stages, do:

for p in model.features.parameters():
    p.requires_grad = False


then unfreeze for fine-tuning.

Device of dummy probe

The dummy is created on CPU; this is safe and fast. If you run into GPU-only environments where CPU tensors are not allowed for some reason (rare), move the dummy to the same device as the model before probing — but the current approach is standard.

D — What to run next (practical plan)

Replace your existing model file with this one.

Instantiate with pretrained=False for quick tests; then pretrained=True for final runs (if you want ImageNet pretrained features).

Print parameter count (the script prints it for you). Compare params_M to your original.

Train on a small subset for a few epochs and check validation mIoU.

If params are lower or similar and mIoU is up, compute your score formula.

If you still need budget, use the parameter savings to add: (a) a light low-level skip (1×1 projection) or (b) KD adapters (1×1) for feature-distillation.

If you want, I can now:

(1) produce a one-file training + KD loop that uses this model (including sample losses for response KD + feature cosine KD), or

(2) add a tight low-level skip implementation (1×1 projection) into this revised model and show the new parameter count so you can decide with exact numbers.

Which would you like next?