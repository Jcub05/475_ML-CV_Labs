
{"requestId":2,"time":1762624144714,"filePath":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","version":4,"statelessNextEditProviderId":"XtabProvider","nextEditRequest":{"id":"6d2bd882-6d37-4350-97dd-b10f4cf89428","documents":[{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.py","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"python","documentLinesBeforeEdit":["\"\"\"","Training Script for Compact Segmentation Models with Knowledge Distillation","ELEC 475 Lab 3","","Supports three training modes:","1. No Knowledge Distillation (baseline)","2. Response-based Knowledge Distillation","3. Feature-based Knowledge Distillation","","References:","- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)","- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)","\"\"\"","","import argparse","import os","import time","from pathlib import Path","import numpy as np","import matplotlib.pyplot as plt","","import torch","import torch.nn as nn","import torch.nn.functional as F","from torch.utils.data import DataLoader","from torchvision import transforms","from torchvision.datasets import VOCSegmentation","from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights","from tqdm import tqdm","","# Import custom models","from model_ultracompact import UltraCompactSegmentationModel","from model_standard import StandardSegmentationModel","","","class VOCNormalize:","    \"\"\"Custom normalization for VOC dataset\"\"\"","    def __init__(self, augment=True):","        self.augment = augment","        self.normalize = transforms.Normalize(","            mean=[0.485, 0.456, 0.406],","            std=[0.229, 0.224, 0.225]","        )","    ","    def __call__(self, img, target):","        # Convert PIL to tensor","        img = transforms.ToTensor()(img)","        ","        # Apply augmentation if training","        if self.augment and np.random.rand() > 0.5:","            # Random horizontal flip","            img = transforms.functional.hflip(img)","            target = transforms.functional.hflip(target)","        ","        # Normalize image","        img = self.normalize(img)","        ","        # Convert target to tensor","        target = torch.as_tensor(np.array(target), dtype=torch.int64)","        ","        return img, target","","","def collate_fn(batch):","    \"\"\"Custom collate function to handle variable-sized images\"\"\"","    images, targets = zip(*batch)","    return list(images), list(targets)","","","def calculate_miou(pred, target, num_classes=21):","    \"\"\"","    Calculate mean Intersection over Union (mIoU)","    ","    Args:","        pred: predicted segmentation mask (H, W)","        target: ground truth segmentation mask (H, W)","        num_classes: number of classes","    ","    Returns:","        mIoU value","    \"\"\"","    pred = pred.cpu().numpy()","    target = target.cpu().numpy()","    ","    ious = []","    for cls in range(num_classes):","        pred_mask = (pred == cls)","        target_mask = (target == cls)","        ","        intersection = np.logical_and(pred_mask, target_mask).sum()","        union = np.logical_or(pred_mask, target_mask).sum()","        ","        if union == 0:","            continue","        ","        iou = intersection / union","        ious.append(iou)","    ","    return np.mean(ious) if len(ious) > 0 else 0.0","","","def response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):","    \"\"\"","    Response-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        teacher_logits: teacher model output logits (B, C, H, W)","        targets: ground truth labels (B, H, W)","        temperature: softmax temperature for distillation","        alpha: weight for cross-entropy loss","        beta: weight for distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Distillation loss (KL divergence)","    student_soft = F.log_softmax(student_logits / temperature, dim=1)","    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)","    ","    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * kd_loss","    ","    return total_loss, ce_loss.item(), kd_loss.item()","","","def feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):","    \"\"\"","    Feature-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        student_features: dict of student intermediate features","        teacher_features: dict of teacher intermediate features","        targets: ground truth labels (B, H, W)","        alpha: weight for cross-entropy loss","        beta: weight for feature distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Feature matching loss (cosine similarity)","    feat_loss = 0","    num_levels = 0","    ","    for level in ['low', 'mid', 'high']:","        if level in student_features and level in teacher_features:","            s_feat = student_features[level]","            t_feat = teacher_features[level]","            ","            # Resize if needed (teacher features might be different size)","            if s_feat.shape != t_feat.shape:","                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)","                # Match channels if needed","                if s_feat.shape[1] != t_feat.shape[1]:","                    # Project teacher features to student channel size","                    continue  # Skip if channel mismatch (or add projection layer)","            ","            # Cosine similarity loss (1 - cosine_similarity)","            s_feat_norm = F.normalize(s_feat, p=2, dim=1)","            t_feat_norm = F.normalize(t_feat, p=2, dim=1)","            ","            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()","            feat_loss += (1 - cosine_sim)","            num_levels += 1","    ","    if num_levels > 0:","        feat_loss = feat_loss / num_levels","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * feat_loss","    ","    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0","","","def train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):","    \"\"\"","    Train for one epoch","    ","    Args:","        model: student model","        teacher_model: teacher model (None if kd_mode='none')","        dataloader: training data loader","        optimizer: optimizer","        device: device to run on","        kd_mode: 'none', 'response', or 'feature'","        args: training arguments","    ","    Returns:","        average loss, average ce_loss, average kd_loss","    \"\"\"","    model.train()","    if teacher_model is not None:","        teacher_model.eval()","    ","    total_loss = 0","    total_ce_loss = 0","    total_kd_loss = 0","    num_batches = 0","    ","    pbar = tqdm(dataloader, desc='Training')","    for images, targets in pbar:","        # Move to device and resize to fixed size for batching","        batch_imgs = []","        batch_targets = []","        ","        for img, target in zip(images, targets):","            img = img.to(device)","            target = target.to(device)","            ","            # Resize to fixed size","            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)","            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()","            ","            batch_imgs.append(img)","            batch_targets.append(target)","        ","        batch_imgs = torch.stack(batch_imgs)","        batch_targets = torch.stack(batch_targets)","        ","        optimizer.zero_grad()","        ","        # Forward pass based on KD mode","        if kd_mode == 'none':","            # No knowledge distillation","            output = model(batch_imgs)","            loss = F.cross_entropy(output, batch_targets, ignore_index=255)","            ce_loss_val = loss.item()","            kd_loss_val = 0.0","            ","        elif kd_mode == 'response':","            # Response-based KD","            output = model(batch_imgs)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","            ","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","            ","        elif kd_mode == 'feature':","            # Feature-based KD","            output, student_features = model(batch_imgs, return_features=True)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)","                # For now, use output-based distillation","                teacher_features = {}  # Would need to extract from teacher","            ","            # Fallback to response-based if teacher features not available","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","        ","        # Backward pass","        loss.backward()","        optimizer.step()","        ","        total_loss += loss.item()","        total_ce_loss += ce_loss_val","        total_kd_loss += kd_loss_val","        num_batches += 1","        ","        pbar.set_postfix({'loss': f'{loss.item():.4f}'})","    ","    avg_loss = total_loss / num_batches","    avg_ce_loss = total_ce_loss / num_batches","    avg_kd_loss = total_kd_loss / num_batches","    ","    return avg_loss, avg_ce_loss, avg_kd_loss","","","def validate(model, dataloader, device, num_classes=21):","    \"\"\"","    Validate the model","    ","    Args:","        model: model to validate","        dataloader: validation data loader","        device: device to run on","        num_classes: number of classes","    ","    Returns:","        mean_iou: mean IoU across all samples","        mean_loss: mean cross-entropy loss","    \"\"\"","    model.eval()","    all_ious = []","    all_losses = []","    criterion = nn.CrossEntropyLoss(ignore_index=255)","    ","    with torch.no_grad():","        for images, targets in tqdm(dataloader, desc='Validation'):","            for img, target in zip(images, targets):","                img = img.unsqueeze(0).to(device)","                target = target.to(device)","                ","                # Resize to fixed size","                h, w = target.shape","                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)","                ","                # Forward pass","                output = model(img)","                ","                # Calculate loss at this resolution","                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), ","                                               size=output.shape[-2:], ","                                               mode='nearest').squeeze().long()","                loss = criterion(output, target_resized.unsqueeze(0))","                all_losses.append(loss.item())","                ","                # Resize back to original size for IoU calculation","                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)","                ","                # Get predictions","                pred = output.squeeze(0).argmax(0)","                ","                # Calculate IoU","                iou = calculate_miou(pred, target, num_classes)","                all_ious.append(iou)","    ","    mean_iou = np.mean(all_ious)","    mean_loss = np.mean(all_losses)","    return mean_iou, mean_loss","","","def plot_training_curves(history, save_path):","    \"\"\"Plot and save training curves\"\"\"","    fig, axes = plt.subplots(1, 2, figsize=(12, 4))","    ","    # Loss plot - now includes both train and validation loss","    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)","    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')","    axes[0].set_xlabel('Epoch')","    axes[0].set_ylabel('Loss')","    axes[0].set_title('Training and Validation Loss')","    axes[0].legend()","    axes[0].grid(True)","    ","    # mIoU plot","    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)","    axes[1].set_xlabel('Epoch')","    axes[1].set_ylabel('mIoU')","    axes[1].set_title('Validation mIoU')","    axes[1].legend()","    axes[1].grid(True)","    ","    plt.tight_layout()","    plt.savefig(save_path, dpi=150)","    plt.close()","    print(f\"Training curves saved to {save_path}\")","","","def main(args):","    \"\"\"Main training function\"\"\"","    ","    print(\"=\" * 70)","    print(\"Training Compact Segmentation Model\")","    print(\"=\" * 70)","    print(f\"Model: {args.model}\")","    print(f\"KD Mode: {args.kd_mode}\")","    print(f\"Epochs: {args.epochs}\")","    print(f\"Batch Size: {args.batch_size}\")","    print(f\"Learning Rate: {args.lr}\")","    print(\"=\" * 70)","    ","    # Set device","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","    print(f\"\\nUsing device: {device}\")","    ","    # Create save directory","    save_dir = Path(args.save_dir)","    save_dir.mkdir(parents=True, exist_ok=True)","    ","    # Load student model","    print(\"\\nLoading student model...\")","    if args.model == 'ultracompact':","        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)","    elif args.model == 'standard':","        model = StandardSegmentationModel(num_classes=21, pretrained=True)","    else:","        raise ValueError(f\"Unknown model: {args.model}\")","    ","    model = model.to(device)","    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)","    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")","    ","    # Load teacher model if using KD","    teacher_model = None","    if args.kd_mode != 'none':","        print(\"\\nLoading teacher model (FCN-ResNet50)...\")","        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1","        teacher_model = fcn_resnet50(weights=weights)","        teacher_model = teacher_model.to(device)","        teacher_model.eval()","        for param in teacher_model.parameters():","            param.requires_grad = False","        print(\"Teacher model loaded and frozen\")","    ","    # Prepare datasets","    print(\"\\nPreparing datasets...\")","    ","    # Find dataset path (Google Colab compatible)","    # VOCSegmentation expects: root/VOCdevkit/VOC2012/","    dataset_roots = []","    ","    # Check for Google Colab structure (both with and without VOCdevkit wrapper)","    colab_train_path = Path('/content/data/VOC2012_train_val')","    if colab_train_path.exists():","        # Check if it has VOCdevkit structure","        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(colab_train_path))","        # Check if files are directly in the folder (no VOCdevkit wrapper)","        elif (colab_train_path / 'JPEGImages').exists():","            # Create a symlink or use parent structure","            dataset_roots.append(str(colab_train_path.parent))","    ","    # Check for local structure","    local_data_path = Path('./data')","    if local_data_path.exists():","        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(local_data_path))","        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():","            dataset_roots.append(str(local_data_path))","    ","    # Check standard paths","    dataset_roots.extend([","        '/content/data',  # Google Colab default","        './data',  # Local path","        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),","    ])","    ","    # Check dataset_path.txt","    dataset_path_file = Path('dataset_path.txt')","    if dataset_path_file.exists():","        with open(dataset_path_file, 'r') as f:","            saved_path = Path(f.read().strip())","            dataset_roots.insert(0, str(saved_path.parent.parent))","    ","    dataset_root = None","    for root in dataset_roots:","        try:","            # Try to create a temporary symlink structure if needed","            root_path = Path(root)","            ","            # Check if we need to create VOCdevkit/VOC2012 structure","            voc_path = root_path / 'VOC2012_train_val'","            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():","                # Files are directly in VOC2012_train_val, create temporary structure","                vocdevkit_path = voc_path / 'VOCdevkit'","                voc2012_path = vocdevkit_path / 'VOC2012'","                if not voc2012_path.exists():","                    print(f\"Creating VOCdevkit structure in {voc_path}\")","                    vocdevkit_path.mkdir(exist_ok=True)","                    voc2012_path.symlink_to(voc_path, target_is_directory=True)","            ","            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)","            dataset_root = root","            print(f\"✓ Successfully loaded dataset from: {root}\")","            break","        except Exception as e:","            continue","    ","    if dataset_root is None:","        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")","    ","    print(f\"Dataset found at: {dataset_root}\")","    ","    # Create datasets","    train_transform = VOCNormalize(augment=True)","    val_transform = VOCNormalize(augment=False)","    ","    train_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='train',","        download=False,","        transforms=train_transform","    )","    ","    val_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='val',","        download=False,","        transforms=val_transform","    )","    ","    print(f\"Training set size: {len(train_dataset)}\")","    print(f\"Validation set size: {len(val_dataset)}\")","    ","    # Create dataloaders","    train_loader = DataLoader(","        train_dataset,","        batch_size=args.batch_size,","        shuffle=True,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    val_loader = DataLoader(","        val_dataset,","        batch_size=4,","        shuffle=False,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    # Setup optimizer and scheduler","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)","    ","    # Training history","    history = {","        'train_loss': [],","        'train_ce_loss': [],","        'train_kd_loss': [],","        'val_loss': [],","        'val_miou': [],","        'epoch_times': []","    }","    ","    # Save hyperparameters","    hyperparameters = {","        'model': args.model,","        'kd_mode': args.kd_mode,","        'epochs': args.epochs,","        'batch_size': args.batch_size,","        'learning_rate': args.lr,","        'weight_decay': args.weight_decay,","        'temperature': args.temperature,","        'alpha': args.alpha,","        'beta': args.beta,","        'optimizer': 'Adam',","        'scheduler': 'CosineAnnealingLR',","        'num_parameters': sum(p.numel() for p in model.parameters()),","        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)","    }","    ","    best_miou = 0.0","    training_start_time = time.time()","    ","    # Training loop","    print(\"\\nStarting training...\")","    print(\"=\" * 70)","    print(\"\\nHyperparameters:\")","    for key, value in hyperparameters.items():","        print(f\"  {key}: {value}\")","    ","    for epoch in range(args.epochs):","        epoch_start_time = time.time()","        ","        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")","        print(\"-\" * 70)","        ","        # Train","        train_loss, train_ce_loss, train_kd_loss = train_epoch(","            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args","        )","        ","        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")","        ","        # Validate","        val_miou = validate(model, val_loader, device)","        print(f\"Validation mIoU: {val_miou:.4f}\")","        ","        # Record epoch time","        epoch_time = time.time() - epoch_start_time","        history['epoch_times'].append(epoch_time)","        print(f\"Epoch time: {epoch_time:.2f} seconds\")","        ","        # Update history","        history['train_loss'].append(train_loss)","        history['train_ce_loss'].append(train_ce_loss)","        history['train_kd_loss'].append(train_kd_loss)","        history['val_miou'].append(val_miou)","        ","        # Save best model","        if val_miou > best_miou:","            best_miou = val_miou","            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history","            }, save_path)","            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")","        ","        # Update scheduler","        scheduler.step()","        ","        # Save checkpoint every 10 epochs","        if (epoch + 1) % 10 == 0:","            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history,","                'hyperparameters': hyperparameters","            }, checkpoint_path)","            print(f\"Checkpoint saved: {checkpoint_path}\")","    ","    # Calculate total training time","    total_training_time = time.time() - training_start_time","    avg_epoch_time = np.mean(history['epoch_times'])","    ","    # Plot training curves","    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"","    plot_training_curves(history, plot_path)","    ","    # Save final training report","    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"","    with open(report_path, 'w') as f:","        f.write(\"=\" * 70 + \"\\n\")","        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")","        f.write(\"=\" * 70 + \"\\n\\n\")","        ","        f.write(\"HYPERPARAMETERS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        for key, value in hyperparameters.items():","            f.write(f\"  {key:25s}: {value}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"TRAINING STATISTICS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")","        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")","        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")","        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")","        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"FILES GENERATED:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")","        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")","        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")","        f.write(\"=\" * 70 + \"\\n\")","    ","    print(\"\\n\" + \"=\" * 70)","    print(\"Training completed!\")","    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")","    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")","    print(f\"Best validation mIoU: {best_miou:.4f}\")","    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")","    print(f\"Training report saved to: {report_path}\")","    ","    # Calculate final score","    score = 4 * best_miou / (1 + num_params / 1e6)","    print(f\"\\nFinal Score: {score:.3f}\")","    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")","    print(\"=\" * 70)","","","if __name__ == '__main__':","    parser = argparse.ArgumentParser(description='Train compact segmentation model')","    ","    # Model arguments","    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],","                        help='Model type to train')","    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],","                        help='Knowledge distillation mode')","    ","    # Training arguments","    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')","    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')","    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')","    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')","    ","    # KD arguments","    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')","    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')","    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')","    ","    # Other arguments","    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')","    ","    args = parser.parse_args()","    ","    main(args)",""],"recentEdit":[[582,583,["        val_miou, val_loss = validate(model, val_loader, device)"]],[583,584,["        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")"]],[594,594,["        history['val_loss'].append(val_loss)"]],[652,652,["        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")"]]],"documentBeforeEdits":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean_iou: mean IoU across all samples\r\n        mean_loss: mean cross-entropy loss\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    all_losses = []\r\n    criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot - now includes both train and validation loss\r\n    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training and Validation Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path, dpi=150)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","recentEdits":[[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]]],"lastSelectionInAfterEdit":[18491,18491]},{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"plaintext","documentLinesBeforeEdit":["# ELEC 475 Lab 3 - Training Commands","# All commands should be run from the Lab 3 directory","","# ============================================================================","# ULTRA-COMPACT MODEL (475K parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# STANDARD MODEL (3.4M parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# TRAINING OUTPUT FILES","# ============================================================================","# For each training run, the following files will be generated in ./checkpoints/:","#   - best_model_{model}_{kd_mode}.pth           (best model weights)","#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)","#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)","#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)","","# ============================================================================","# OPTIONAL: HYPERPARAMETER TUNING","# ============================================================================","# If you want to experiment with different hyperparameters:","","# Adjust learning rate","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005","","# Adjust KD temperature (higher = softer targets)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0","","# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3","","# Adjust batch size (if memory issues)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4","","# ============================================================================","# NOTES","# ============================================================================","# - Training on CPU will be slow. Consider using Google Colab with GPU.","# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU","# - Monitor validation mIoU - training should converge after 30-40 epochs","# - Best model is saved automatically when validation mIoU improves",""],"recentEdit":[[18,19,["# STANDARD MODEL (1 parameters)"]]],"documentBeforeEdits":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","recentEdits":[[[867,871,"1"]]],"lastSelectionInAfterEdit":[868,868]}],"activeDocumentIdx":1,"recording":[{"documentType":"workspaceRecording@1.0","kind":"header","repoRootUri":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","time":1762624143621,"uuid":"1ab12273-5d43-4ee2-a94e-294b5db859b2"},{"kind":"documentEncountered","id":4,"relativePath":"model_standard.py","time":1762545107282},{"kind":"setContent","id":4,"v":1,"content":"\"\"\"\r\nStandard MobileNetV3-Small Segmentation Model\r\nELEC 475 Lab 3 - Option 1\r\n\r\nParameters: ~3.41M\r\nExpected Score: 0.40-0.50\r\n\r\nReferences:\r\n- MobileNetV3: Searching for MobileNetV3 (Howard et al., 2019)\r\n- DeepLabV3+: Encoder-Decoder with Atrous Separable Convolution (Chen et al., 2018)\r\n- ASPP: Rethinking Atrous Convolution for Semantic Image Segmentation (Chen et al., 2017)\r\n\"\"\"\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\r\n\r\n\r\nclass ASPP(nn.Module):\r\n    \"\"\"\r\n    Atrous Spatial Pyramid Pooling (ASPP) module\r\n    Captures multi-scale context using parallel atrous convolutions\r\n    \"\"\"\r\n    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\r\n        super(ASPP, self).__init__()\r\n        \r\n        self.branches = nn.ModuleList()\r\n        \r\n        # 1x1 convolution branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Atrous convolution branches\r\n        for rate in rates:\r\n            self.branches.append(nn.Sequential(\r\n                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\r\n                nn.BatchNorm2d(out_channels),\r\n                nn.ReLU(inplace=True)\r\n            ))\r\n        \r\n        # Global average pooling branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.AdaptiveAvgPool2d(1),\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Combine all branches\r\n        num_branches = len(rates) + 2  # 1x1 + atrous branches + global pooling\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(out_channels * num_branches, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Dropout(0.5)\r\n        )\r\n    \r\n    def forward(self, x):\r\n        h, w = x.shape[2:]\r\n        branch_outputs = []\r\n        \r\n        for i, branch in enumerate(self.branches):\r\n            out = branch(x)\r\n            # Upsample global pooling branch to match spatial dimensions\r\n            if i == len(self.branches) - 1:\r\n                out = F.interpolate(out, size=(h, w), mode='bilinear', align_corners=False)\r\n            branch_outputs.append(out)\r\n        \r\n        # Concatenate all branches\r\n        x = torch.cat(branch_outputs, dim=1)\r\n        x = self.project(x)\r\n        \r\n        return x\r\n\r\n\r\nclass StandardSegmentationModel(nn.Module):\r\n    \"\"\"\r\n    Standard compact segmentation model using MobileNetV3-Small backbone\r\n    \r\n    Architecture:\r\n    - Full MobileNetV3-Small backbone (pretrained)\r\n    - 3-rate ASPP module (rates: 6, 12, 18)\r\n    - Two-stage decoder with skip connections\r\n    - Parameters: ~3.41M\r\n    \"\"\"\r\n    def __init__(self, num_classes=21, pretrained=True):\r\n        super(StandardSegmentationModel, self).__init__()\r\n        \r\n        # Load pretrained MobileNetV3-Small\r\n        if pretrained:\r\n            weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1\r\n            backbone = mobilenet_v3_small(weights=weights)\r\n        else:\r\n            backbone = mobilenet_v3_small(weights=None)\r\n        \r\n        # Extract feature layers from MobileNetV3-Small\r\n        self.features = backbone.features\r\n        \r\n        # Feature dimensions at tap points:\r\n        # Stage 1 (low):  16 channels, stride 4\r\n        # Stage 3 (mid):  24 channels, stride 8\r\n        # Stage 12 (high): 576 channels, stride 16\r\n        \r\n        # ASPP module for multi-scale context\r\n        self.aspp = ASPP(in_channels=576, out_channels=128, rates=[6, 12, 18])\r\n        \r\n        # Feature projection layers (keep dimensions)\r\n        self.mid_conv = nn.Sequential(\r\n            nn.Conv2d(24, 24, 1, bias=False),\r\n            nn.BatchNorm2d(24),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.low_conv = nn.Sequential(\r\n            nn.Conv2d(16, 16, 1, bias=False),\r\n            nn.BatchNorm2d(16),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Decoder with skip connections\r\n        self.decoder_conv1 = nn.Sequential(\r\n            nn.Conv2d(128 + 24, 128, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(128),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.decoder_conv2 = nn.Sequential(\r\n            nn.Conv2d(128 + 16, 64, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(64),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Classifier head\r\n        self.classifier = nn.Sequential(\r\n            nn.Dropout(0.5),\r\n            nn.Conv2d(64, num_classes, 1)\r\n        )\r\n        \r\n        self._init_weights()\r\n    \r\n    def _init_weights(self):\r\n        \"\"\"Initialize weights for new layers\"\"\"\r\n        for m in [self.aspp, self.mid_conv, self.low_conv, self.decoder_conv1, self.decoder_conv2, self.classifier]:\r\n            for module in m.modules():\r\n                if isinstance(module, nn.Conv2d):\r\n                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\r\n                    if module.bias is not None:\r\n                        nn.init.constant_(module.bias, 0)\r\n                elif isinstance(module, nn.BatchNorm2d):\r\n                    nn.init.constant_(module.weight, 1)\r\n                    nn.init.constant_(module.bias, 0)\r\n    \r\n    def forward(self, x, return_features=False):\r\n        \"\"\"\r\n        Forward pass\r\n        \r\n        Args:\r\n            x: input image tensor (B, 3, H, W)\r\n            return_features: if True, return intermediate features for knowledge distillation\r\n            \r\n        Returns:\r\n            output: segmentation logits (B, num_classes, H, W)\r\n            features (optional): dict of intermediate features for KD\r\n        \"\"\"\r\n        input_shape = x.shape[-2:]\r\n        \r\n        # Extract multi-scale features from backbone\r\n        low_feat = None   # stride 4\r\n        mid_feat = None   # stride 8\r\n        high_feat = None  # stride 16\r\n        \r\n        # Forward through MobileNetV3 stages\r\n        for i, layer in enumerate(self.features):\r\n            x = layer(x)\r\n            \r\n            # Capture features at specific stages\r\n            if i == 1:  # After first inverted residual block (stride 4)\r\n                low_feat = x\r\n            elif i == 3:  # After third inverted residual block (stride 8)\r\n                mid_feat = x\r\n        \r\n        # Final high-level features (stride 16)\r\n        high_feat = x\r\n        \r\n        # Process features\r\n        mid_feat = self.mid_conv(mid_feat)\r\n        low_feat = self.low_conv(low_feat)\r\n        \r\n        # ASPP for multi-scale context\r\n        x = self.aspp(high_feat)\r\n        \r\n        # Decoder with skip connections\r\n        # Upsample and concatenate with mid-level features\r\n        x = F.interpolate(x, size=mid_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, mid_feat], dim=1)\r\n        x = self.decoder_conv1(x)\r\n        \r\n        # Upsample and concatenate with low-level features\r\n        x = F.interpolate(x, size=low_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, low_feat], dim=1)\r\n        x = self.decoder_conv2(x)\r\n        \r\n        # Final upsampling to input resolution\r\n        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\r\n        \r\n        # Classifier\r\n        output = self.classifier(x)\r\n        \r\n        if return_features:\r\n            features = {\r\n                'low': low_feat,\r\n                'mid': mid_feat,\r\n                'high': high_feat\r\n            }\r\n            return output, features\r\n        \r\n        return output\r\n\r\n\r\ndef count_parameters(model):\r\n    \"\"\"Count the number of trainable parameters\"\"\"\r\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(\"=\" * 70)\r\n    print(\"Standard MobileNetV3-Small Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Create model\r\n    model = StandardSegmentationModel(num_classes=21, pretrained=False)\r\n    \r\n    # Count parameters\r\n    params = count_parameters(model)\r\n    print(f\"\\nTotal parameters: {params:,} ({params/1e6:.2f}M)\")\r\n    \r\n    # Test forward pass\r\n    print(\"\\nTesting forward pass...\")\r\n    dummy_input = torch.randn(2, 3, 224, 224)\r\n    \r\n    with torch.no_grad():\r\n        output = model(dummy_input)\r\n        print(f\"  Output shape: {output.shape}\")\r\n        \r\n        # Test with feature extraction\r\n        output, features = model(dummy_input, return_features=True)\r\n        print(f\"\\n  Features for knowledge distillation:\")\r\n        for name, feat in features.items():\r\n            print(f\"    {name}: {feat.shape}\")\r\n    \r\n    # Score estimation\r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Score Estimation: Score = 4 × mIoU / (1 + params_M)\")\r\n    print(\"=\" * 70)\r\n    \r\n    for miou in [0.40, 0.45, 0.50, 0.55]:\r\n        score = 4 * miou / (1 + params/1e6)\r\n        print(f\"  mIoU={miou:.2f} → Score={score:.3f}\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n","time":1762545107282},{"kind":"opened","id":4,"time":1762545107282},{"kind":"documentEncountered","id":5,"relativePath":"train.py","time":1762545107284},{"kind":"setContent","id":5,"v":1,"content":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean IoU\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Resize back to original size\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    return mean_iou\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot\r\n    axes[0].plot(history['train_loss'], label='Train Loss')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU')\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","time":1762622282025},{"kind":"opened","id":5,"time":1762622282025},{"kind":"documentEncountered","id":17,"relativePath":"train.txt","time":1762622441478},{"kind":"setContent","id":17,"v":1,"content":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","time":1762622441593},{"kind":"opened","id":17,"time":1762622441593},{"kind":"changed","id":4,"v":2,"edit":[[50,50," (Optimized)"],[94,94,"1."],[95,95,"6M (reduced from 3"],[99,99,")"],[119,120,"8"],[122,123,"1"],[124,125,"2"]],"time":1762623638975},{"kind":"changed","id":4,"v":3,"edit":[[2834,2834," (Optimized)"],[2919,2920,"Lightweight 2"],[2951,2955,""],[2956,2956," - reduced from 3 rates"],[2964,2972,"Simplifi"],[2973,2973,"d"],[2987,2991,"reduced"],[2993,2994,"ha"],[2997,3002,"l"],[3024,3024,"1."],[3025,3025,"6M (reduced from 3"],[3029,3029,")"],[3767,3767,"Lightweight "],[3779,3782,"(2"],[3783,3786,"ra"],[3787,3787,"es "],[3788,3789,"n"],[3790,3790,"tead of 3, redu"],[3791,3793,""],[3794,3794,"d output"],[3796,3797,"han"],[3798,3799,""],[3800,3802,"ls)"],[3859,3862,"64"],[3876,3880,""],[3931,3933,"r"],[3934,3936,""],[3937,3939,"uc"],[3940,3940,"d cha"],[3941,3944,""],[3945,3945,"el"],[4015,4017,"16"],[4063,4065,"16"],[4191,4193,"8"],[4239,4241,"8"],[4310,4310," Simplified"],[4311,4312,"d"],[4324,4328,"reduced"],[4330,4331,"ha"],[4334,4339,"l"],[4409,4412,"64"],[4415,4417,"16"],[4419,4422,"64"],[4479,4482,"64"],[4609,4612,"64"],[4615,4617,"8"],[4619,4621,"32"],[4678,4680,"32"],[4861,4863,"32"]],"time":1762623641194},{"kind":"changed","id":5,"v":2,"edit":[[10251,10251,"_iou:"],[10252,10252,"mean "],[10255,10255," across all samples"],[10261,10261,"    mean_loss: mean cross-entropy loss\r\n    "],[10307,10307,"all_losses = []"],[10313,10313,"criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    "],[10403,10403,"\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss"]],"time":1762623765832},{"kind":"changed","id":5,"v":3,"edit":[[11970,12883,""]],"time":1762623780348},{"kind":"changed","id":5,"v":4,"edit":[[12136,12136," - now includes both train and validation loss"],[12188,12188,"in Loss', l"],[12190,12190,"ewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val"],[12196,12196,", linewidth=2, linestyle='--'"],[12295,12295," and Validation"],[12434,12434,", linewidth=2"],[12645,12645,", dpi=150"]],"time":1762623791607},{"kind":"changed","id":5,"v":5,"edit":[[19126,19126,"loss': [],\r\n        'val_"]],"time":1762623803059},{"kind":"changed","id":5,"v":6,"edit":[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],"time":1762623816438},{"kind":"changed","id":5,"v":7,"edit":[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]],"time":1762623835010},{"kind":"selectionChanged","id":17,"selection":[[883,883]],"time":1762624017851},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624140244},{"kind":"selectionChanged","id":17,"selection":[[869,870]],"time":1762624140330},{"kind":"selectionChanged","id":17,"selection":[[868,870]],"time":1762624140356},{"kind":"selectionChanged","id":17,"selection":[[867,870]],"time":1762624140378},{"kind":"changed","id":17,"v":7,"edit":[[867,870,"1"]],"time":1762624143421},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624143443},{"kind":"changed","id":17,"v":9,"edit":[[868,869,""]],"time":1762624143557}]},"resultEdit":"   16  16 \n   17  17 # ============================================================================\n-  18     # STANDARD MODEL (1 parameters)\n+      18 # STANDARD MODEL (1.3M parameters)\n   19  19 # ============================================================================\n   20  20 \n   21  21 # Baseline (No Knowledge Distillation)","isCachedResult":true,"prompt":"System\n------\nYour role as an AI assistant is to help developers complete their code tasks by assisting in editing specific sections of code marked by the <|code_to_edit|> and <|/code_to_edit|> tags, while adhering to Microsoft's content policies and avoiding the creation of content that violates copyrights.\n\nYou have access to the following information to help you make informed suggestions:\n\n- recently_viewed_code_snippets: These are code snippets that the developer has recently looked at, which might provide context or examples relevant to the current task. They are listed from oldest to newest, with line numbers in the form #| to help you understand the edit diff history. It's possible these are entirely irrelevant to the developer's change.\n- current_file_content: The content of the file the developer is currently working on, providing the broader context of the code. Line numbers in the form #| are included to help you understand the edit diff history.\n- edit_diff_history: A record of changes made to the code, helping you understand the evolution of the code and the developer's intentions. These changes are listed from oldest to latest. It's possible a lot of old edit diff history is entirely irrelevant to the developer's change.\n- area_around_code_to_edit: The context showing the code surrounding the section to be edited.\n- cursor position marked as <|cursor|>: Indicates where the developer's cursor is currently located, which can be crucial for understanding what part of the code they are focusing on.\n\nYour task is to predict and complete the changes the developer would have made next in the <|code_to_edit|> section. The developer may have stopped in the middle of typing. Your goal is to keep the developer on the path that you think they're following. Some examples include further implementing a class, method, or variable, or improving the quality of the code. Make sure the developer doesn't get distracted and ensure your suggestion is relevant. Consider what changes need to be made next, if any. If you think changes should be made, ask yourself if this is truly what needs to happen. If you are confident about it, then proceed with the changes.\n\n# Steps\n\n1. **Review Context**: Analyze the context from the resources provided, such as recently viewed snippets, edit history, surrounding code, and cursor location.\n2. **Evaluate Current Code**: Determine if the current code within the tags requires any corrections or enhancements.\n3. **Suggest Edits**: If changes are required, ensure they align with the developer's patterns and improve code quality.\n4. **Maintain Consistency**: Ensure indentation and formatting follow the existing code style.\n\n# Output Format\n\n- Provide only the revised code within the tags. If no changes are necessary, simply return the original code from within the <|code_to_edit|> and <|/code_to_edit|> tags.\n- There are line numbers in the form #| in the code displayed to you above, but these are just for your reference. Please do not include the numbers of the form #| in your response.\n- Ensure that you do not output duplicate code that exists outside of these tags. The output should be the revised code that was between these tags and should not include the <|code_to_edit|> or <|/code_to_edit|> tags.\n\n```\n// Your revised code goes here\n```\n\n# Notes\n\n- Apologize with \"Sorry, I can't assist with that.\" for requests that may breach Microsoft content guidelines.\n- Avoid undoing or reverting the developer's last change unless there are obvious typos or errors.\n- Don't include the line numbers of the form #| in your response.\n==================\nUser\n------\n```\n<|recently_viewed_code_snippets|>\n<|recently_viewed_code_snippet|>\ncode_snippet_file_path: /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py (truncated)\n\"\"\"\nTraining Script for Compact Segmentation Models with Knowledge Distillation\nELEC 475 Lab 3\n\nSupports three training modes:\n1. No Knowledge Distillation (baseline)\n2. Response-based Knowledge Distillation\n3. Feature-based Knowledge Distillation\n\nReferences:\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\n\"\"\"\n\nimport argparse\nimport os\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import VOCSegmentation\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\nfrom tqdm import tqdm\n\n# Import custom models\nfrom model_ultracompact import UltraCompactSegmentationModel\nfrom model_standard import StandardSegmentationModel\n\n\nclass VOCNormalize:\n    \"\"\"Custom normalization for VOC dataset\"\"\"\n    def __init__(self, augment=True):\n        self.augment = augment\n        self.normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    \n    def __call__(self, img, target):\n        # Convert PIL to tensor\n        img = transforms.ToTensor()(img)\n        \n        # Apply augmentation if training\n        if self.augment and np.random.rand() > 0.5:\n            # Random horizontal flip\n            img = transforms.functional.hflip(img)\n            target = transforms.functional.hflip(target)\n        \n        # Normalize image\n        img = self.normalize(img)\n        \n        # Convert target to tensor\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n        \n        return img, target\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\n    images, targets = zip(*batch)\n    return list(images), list(targets)\n\n\ndef calculate_miou(pred, target, num_classes=21):\n    \"\"\"\n    Calculate mean Intersection over Union (mIoU)\n    \n    Args:\n        pred: predicted segmentation mask (H, W)\n        target: ground truth segmentation mask (H, W)\n        num_classes: number of classes\n    \n    Returns:\n        mIoU value\n    \"\"\"\n    pred = pred.cpu().numpy()\n    target = target.cpu().numpy()\n    \n    ious = []\n    for cls in range(num_classes):\n        pred_mask = (pred == cls)\n        target_mask = (target == cls)\n        \n        intersection = np.logical_and(pred_mask, target_mask).sum()\n        union = np.logical_or(pred_mask, target_mask).sum()\n        \n        if union == 0:\n            continue\n        \n        iou = intersection / union\n        ious.append(iou)\n    \n    return np.mean(ious) if len(ious) > 0 else 0.0\n\n\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\n    \"\"\"\n    Response-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        teacher_logits: teacher model output logits (B, C, H, W)\n        targets: ground truth labels (B, H, W)\n        temperature: softmax temperature for distillation\n        alpha: weight for cross-entropy loss\n        beta: weight for distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Distillation loss (KL divergence)\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\n    \n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * kd_loss\n    \n    return total_loss, ce_loss.item(), kd_loss.item()\n\n\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\n    \"\"\"\n    Feature-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        student_features: dict of student intermediate features\n        teacher_features: dict of teacher intermediate features\n        targets: ground truth labels (B, H, W)\n        alpha: weight for cross-entropy loss\n        beta: weight for feature distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Feature matching loss (cosine similarity)\n    feat_loss = 0\n    num_levels = 0\n    \n    for level in ['low', 'mid', 'high']:\n        if level in student_features and level in teacher_features:\n            s_feat = student_features[level]\n            t_feat = teacher_features[level]\n            \n            # Resize if needed (teacher features might be different size)\n            if s_feat.shape != t_feat.shape:\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n                # Match channels if needed\n                if s_feat.shape[1] != t_feat.shape[1]:\n                    # Project teacher features to student channel size\n                    continue  # Skip if channel mismatch (or add projection layer)\n            \n            # Cosine similarity loss (1 - cosine_similarity)\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\n            \n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\n            feat_loss += (1 - cosine_sim)\n            num_levels += 1\n    \n    if num_levels > 0:\n        feat_loss = feat_loss / num_levels\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * feat_loss\n    \n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\n\n\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\n    \"\"\"\n    Train for one epoch\n    \n    Args:\n        model: student model\n        teacher_model: teacher model (None if kd_mode='none')\n        dataloader: training data loader\n        optimizer: optimizer\n        device: device to run on\n        kd_mode: 'none', 'response', or 'feature'\n        args: training arguments\n    \n    Returns:\n        average loss, average ce_loss, average kd_loss\n    \"\"\"\n    model.train()\n    if teacher_model is not None:\n        teacher_model.eval()\n    \n    total_loss = 0\n    total_ce_loss = 0\n    total_kd_loss = 0\n    num_batches = 0\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for images, targets in pbar:\n        # Move to device and resize to fixed size for batching\n        batch_imgs = []\n        batch_targets = []\n        \n        for img, target in zip(images, targets):\n            img = img.to(device)\n            target = target.to(device)\n            \n            # Resize to fixed size\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\n            \n            batch_imgs.append(img)\n            batch_targets.append(target)\n        \n        batch_imgs = torch.stack(batch_imgs)\n<|/recently_viewed_code_snippet|>\n<|/recently_viewed_code_snippets|>\n\n<|current_file_content|>\ncurrent_file_path: train.txt\n# ELEC 475 Lab 3 - Training Commands\n# All commands should be run from the Lab 3 directory\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n# STANDARD MODEL (1<|cursor|> parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\n\n# ============================================================================\n# OPTIONAL: HYPERPARAMETER TUNING\n# ============================================================================\n# If you want to experiment with different hyperparameters:\n\n# Adjust learning rate\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\n\n# Adjust KD temperature (higher = softer targets)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\n\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\n\n# Adjust batch size (if memory issues)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\n\n# ============================================================================\n# NOTES\n# ============================================================================\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\n# - Monitor validation mIoU - training should converge after 30-40 epochs\n# - Best model is saved automatically when validation mIoU improves\n\n<|/current_file_content|>\n\n<|edit_diff_history|>\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -342,23 +342,0 @@\n-            for img, target in zip(images, targets):\n-                img = img.unsqueeze(0).to(device)\n-                target = target.to(device)\n-                \n-                # Resize to fixed size\n-                h, w = target.shape\n-                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\n-                \n-                # Forward pass\n-                output = model(img)\n-                \n-                # Resize back to original size\n-                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\n-                \n-                # Get predictions\n-                pred = output.squeeze(0).argmax(0)\n-                \n-                # Calculate IoU\n-                iou = calculate_miou(pred, target, num_classes)\n-                all_ious.append(iou)\n-    \n-    mean_iou = np.mean(all_ious)\n-    return mean_iou\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -348,1 +348,1 @@\n-    # Loss plot\n+    # Loss plot - now includes both train and validation loss\n@@ -349,1 +349,2 @@\n-    axes[0].plot(history['train_loss'], label='Train Loss')\n+    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n+    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\n@@ -352,1 +352,1 @@\n-    axes[0].set_title('Training Loss')\n+    axes[0].set_title('Training and Validation Loss')\n@@ -357,1 +357,1 @@\n-    axes[1].plot(history['val_miou'], label='Validation mIoU')\n+    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\n@@ -365,1 +365,1 @@\n-    plt.savefig(save_path)\n+    plt.savefig(save_path, dpi=150)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -535,1 +535,2 @@\n-        'val_miou': [],\n+        'val_loss': [],\n+        'val_miou': [],\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -581,1 +581,1 @@\n-        val_miou = validate(model, val_loader, device)\n+        val_miou, val_loss = validate(model, val_loader, device)\n@@ -582,1 +582,1 @@\n-        print(f\"Validation mIoU: {val_miou:.4f}\")\n+        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")\n@@ -592,1 +592,2 @@\n-        history['train_kd_loss'].append(train_kd_loss)\n+        history['train_kd_loss'].append(train_kd_loss)\n+        history['val_loss'].append(val_loss)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -652,1 +652,2 @@\n-        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n@@ -17,1 +17,1 @@\n-# STANDARD MODEL (3.4M parameters)\n+# STANDARD MODEL (1 parameters)\n\n<|/edit_diff_history|>\n\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n# STANDARD MODEL (1<|cursor|> parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n```\n\nThe developer was working on a section of code within the tags `code_to_edit` in the file located at `train.txt`. Using the given `recently_viewed_code_snippets`, `current_file_content`, `edit_diff_history`, `area_around_code_to_edit`, and the cursor position marked as `<|cursor|>`, please continue the developer's work. Update the `code_to_edit` section by predicting and completing the changes they would have made next. Provide the revised code that was between the `<|code_to_edit|>` and `<|/code_to_edit|>` tags with the following format, but do not include the tags themselves.\n```\n// Your revised code goes here\n```\n==================","error":"undefined","response":"```\n\n# ============================================================================\n# STANDARD MODEL (1.3M parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n```","responseResults":"- replaceRange:\n   start: 849\n   endExclusive: 880\n  newText: \"# STANDARD MODEL (1.3M parameters)\"\n","logs":["```\n{\n\t\"isAsyncCompletions\": true,\n\t\"isRevisedCacheStrategy\": true,\n\t\"isCacheTracksRejections\": true,\n\t\"isRecentlyShownCacheEnabled\": false,\n\t\"debounceUseCoreRequestTime\": false\n}\n```\n"]}
{"requestId":3,"time":1762624146236,"filePath":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","version":4,"statelessNextEditProviderId":"XtabProvider","nextEditRequest":{"id":"6d2bd882-6d37-4350-97dd-b10f4cf89428","documents":[{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.py","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"python","documentLinesBeforeEdit":["\"\"\"","Training Script for Compact Segmentation Models with Knowledge Distillation","ELEC 475 Lab 3","","Supports three training modes:","1. No Knowledge Distillation (baseline)","2. Response-based Knowledge Distillation","3. Feature-based Knowledge Distillation","","References:","- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)","- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)","\"\"\"","","import argparse","import os","import time","from pathlib import Path","import numpy as np","import matplotlib.pyplot as plt","","import torch","import torch.nn as nn","import torch.nn.functional as F","from torch.utils.data import DataLoader","from torchvision import transforms","from torchvision.datasets import VOCSegmentation","from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights","from tqdm import tqdm","","# Import custom models","from model_ultracompact import UltraCompactSegmentationModel","from model_standard import StandardSegmentationModel","","","class VOCNormalize:","    \"\"\"Custom normalization for VOC dataset\"\"\"","    def __init__(self, augment=True):","        self.augment = augment","        self.normalize = transforms.Normalize(","            mean=[0.485, 0.456, 0.406],","            std=[0.229, 0.224, 0.225]","        )","    ","    def __call__(self, img, target):","        # Convert PIL to tensor","        img = transforms.ToTensor()(img)","        ","        # Apply augmentation if training","        if self.augment and np.random.rand() > 0.5:","            # Random horizontal flip","            img = transforms.functional.hflip(img)","            target = transforms.functional.hflip(target)","        ","        # Normalize image","        img = self.normalize(img)","        ","        # Convert target to tensor","        target = torch.as_tensor(np.array(target), dtype=torch.int64)","        ","        return img, target","","","def collate_fn(batch):","    \"\"\"Custom collate function to handle variable-sized images\"\"\"","    images, targets = zip(*batch)","    return list(images), list(targets)","","","def calculate_miou(pred, target, num_classes=21):","    \"\"\"","    Calculate mean Intersection over Union (mIoU)","    ","    Args:","        pred: predicted segmentation mask (H, W)","        target: ground truth segmentation mask (H, W)","        num_classes: number of classes","    ","    Returns:","        mIoU value","    \"\"\"","    pred = pred.cpu().numpy()","    target = target.cpu().numpy()","    ","    ious = []","    for cls in range(num_classes):","        pred_mask = (pred == cls)","        target_mask = (target == cls)","        ","        intersection = np.logical_and(pred_mask, target_mask).sum()","        union = np.logical_or(pred_mask, target_mask).sum()","        ","        if union == 0:","            continue","        ","        iou = intersection / union","        ious.append(iou)","    ","    return np.mean(ious) if len(ious) > 0 else 0.0","","","def response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):","    \"\"\"","    Response-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        teacher_logits: teacher model output logits (B, C, H, W)","        targets: ground truth labels (B, H, W)","        temperature: softmax temperature for distillation","        alpha: weight for cross-entropy loss","        beta: weight for distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Distillation loss (KL divergence)","    student_soft = F.log_softmax(student_logits / temperature, dim=1)","    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)","    ","    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * kd_loss","    ","    return total_loss, ce_loss.item(), kd_loss.item()","","","def feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):","    \"\"\"","    Feature-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        student_features: dict of student intermediate features","        teacher_features: dict of teacher intermediate features","        targets: ground truth labels (B, H, W)","        alpha: weight for cross-entropy loss","        beta: weight for feature distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Feature matching loss (cosine similarity)","    feat_loss = 0","    num_levels = 0","    ","    for level in ['low', 'mid', 'high']:","        if level in student_features and level in teacher_features:","            s_feat = student_features[level]","            t_feat = teacher_features[level]","            ","            # Resize if needed (teacher features might be different size)","            if s_feat.shape != t_feat.shape:","                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)","                # Match channels if needed","                if s_feat.shape[1] != t_feat.shape[1]:","                    # Project teacher features to student channel size","                    continue  # Skip if channel mismatch (or add projection layer)","            ","            # Cosine similarity loss (1 - cosine_similarity)","            s_feat_norm = F.normalize(s_feat, p=2, dim=1)","            t_feat_norm = F.normalize(t_feat, p=2, dim=1)","            ","            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()","            feat_loss += (1 - cosine_sim)","            num_levels += 1","    ","    if num_levels > 0:","        feat_loss = feat_loss / num_levels","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * feat_loss","    ","    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0","","","def train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):","    \"\"\"","    Train for one epoch","    ","    Args:","        model: student model","        teacher_model: teacher model (None if kd_mode='none')","        dataloader: training data loader","        optimizer: optimizer","        device: device to run on","        kd_mode: 'none', 'response', or 'feature'","        args: training arguments","    ","    Returns:","        average loss, average ce_loss, average kd_loss","    \"\"\"","    model.train()","    if teacher_model is not None:","        teacher_model.eval()","    ","    total_loss = 0","    total_ce_loss = 0","    total_kd_loss = 0","    num_batches = 0","    ","    pbar = tqdm(dataloader, desc='Training')","    for images, targets in pbar:","        # Move to device and resize to fixed size for batching","        batch_imgs = []","        batch_targets = []","        ","        for img, target in zip(images, targets):","            img = img.to(device)","            target = target.to(device)","            ","            # Resize to fixed size","            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)","            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()","            ","            batch_imgs.append(img)","            batch_targets.append(target)","        ","        batch_imgs = torch.stack(batch_imgs)","        batch_targets = torch.stack(batch_targets)","        ","        optimizer.zero_grad()","        ","        # Forward pass based on KD mode","        if kd_mode == 'none':","            # No knowledge distillation","            output = model(batch_imgs)","            loss = F.cross_entropy(output, batch_targets, ignore_index=255)","            ce_loss_val = loss.item()","            kd_loss_val = 0.0","            ","        elif kd_mode == 'response':","            # Response-based KD","            output = model(batch_imgs)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","            ","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","            ","        elif kd_mode == 'feature':","            # Feature-based KD","            output, student_features = model(batch_imgs, return_features=True)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)","                # For now, use output-based distillation","                teacher_features = {}  # Would need to extract from teacher","            ","            # Fallback to response-based if teacher features not available","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","        ","        # Backward pass","        loss.backward()","        optimizer.step()","        ","        total_loss += loss.item()","        total_ce_loss += ce_loss_val","        total_kd_loss += kd_loss_val","        num_batches += 1","        ","        pbar.set_postfix({'loss': f'{loss.item():.4f}'})","    ","    avg_loss = total_loss / num_batches","    avg_ce_loss = total_ce_loss / num_batches","    avg_kd_loss = total_kd_loss / num_batches","    ","    return avg_loss, avg_ce_loss, avg_kd_loss","","","def validate(model, dataloader, device, num_classes=21):","    \"\"\"","    Validate the model","    ","    Args:","        model: model to validate","        dataloader: validation data loader","        device: device to run on","        num_classes: number of classes","    ","    Returns:","        mean_iou: mean IoU across all samples","        mean_loss: mean cross-entropy loss","    \"\"\"","    model.eval()","    all_ious = []","    all_losses = []","    criterion = nn.CrossEntropyLoss(ignore_index=255)","    ","    with torch.no_grad():","        for images, targets in tqdm(dataloader, desc='Validation'):","            for img, target in zip(images, targets):","                img = img.unsqueeze(0).to(device)","                target = target.to(device)","                ","                # Resize to fixed size","                h, w = target.shape","                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)","                ","                # Forward pass","                output = model(img)","                ","                # Calculate loss at this resolution","                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), ","                                               size=output.shape[-2:], ","                                               mode='nearest').squeeze().long()","                loss = criterion(output, target_resized.unsqueeze(0))","                all_losses.append(loss.item())","                ","                # Resize back to original size for IoU calculation","                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)","                ","                # Get predictions","                pred = output.squeeze(0).argmax(0)","                ","                # Calculate IoU","                iou = calculate_miou(pred, target, num_classes)","                all_ious.append(iou)","    ","    mean_iou = np.mean(all_ious)","    mean_loss = np.mean(all_losses)","    return mean_iou, mean_loss","","","def plot_training_curves(history, save_path):","    \"\"\"Plot and save training curves\"\"\"","    fig, axes = plt.subplots(1, 2, figsize=(12, 4))","    ","    # Loss plot - now includes both train and validation loss","    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)","    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')","    axes[0].set_xlabel('Epoch')","    axes[0].set_ylabel('Loss')","    axes[0].set_title('Training and Validation Loss')","    axes[0].legend()","    axes[0].grid(True)","    ","    # mIoU plot","    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)","    axes[1].set_xlabel('Epoch')","    axes[1].set_ylabel('mIoU')","    axes[1].set_title('Validation mIoU')","    axes[1].legend()","    axes[1].grid(True)","    ","    plt.tight_layout()","    plt.savefig(save_path, dpi=150)","    plt.close()","    print(f\"Training curves saved to {save_path}\")","","","def main(args):","    \"\"\"Main training function\"\"\"","    ","    print(\"=\" * 70)","    print(\"Training Compact Segmentation Model\")","    print(\"=\" * 70)","    print(f\"Model: {args.model}\")","    print(f\"KD Mode: {args.kd_mode}\")","    print(f\"Epochs: {args.epochs}\")","    print(f\"Batch Size: {args.batch_size}\")","    print(f\"Learning Rate: {args.lr}\")","    print(\"=\" * 70)","    ","    # Set device","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","    print(f\"\\nUsing device: {device}\")","    ","    # Create save directory","    save_dir = Path(args.save_dir)","    save_dir.mkdir(parents=True, exist_ok=True)","    ","    # Load student model","    print(\"\\nLoading student model...\")","    if args.model == 'ultracompact':","        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)","    elif args.model == 'standard':","        model = StandardSegmentationModel(num_classes=21, pretrained=True)","    else:","        raise ValueError(f\"Unknown model: {args.model}\")","    ","    model = model.to(device)","    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)","    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")","    ","    # Load teacher model if using KD","    teacher_model = None","    if args.kd_mode != 'none':","        print(\"\\nLoading teacher model (FCN-ResNet50)...\")","        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1","        teacher_model = fcn_resnet50(weights=weights)","        teacher_model = teacher_model.to(device)","        teacher_model.eval()","        for param in teacher_model.parameters():","            param.requires_grad = False","        print(\"Teacher model loaded and frozen\")","    ","    # Prepare datasets","    print(\"\\nPreparing datasets...\")","    ","    # Find dataset path (Google Colab compatible)","    # VOCSegmentation expects: root/VOCdevkit/VOC2012/","    dataset_roots = []","    ","    # Check for Google Colab structure (both with and without VOCdevkit wrapper)","    colab_train_path = Path('/content/data/VOC2012_train_val')","    if colab_train_path.exists():","        # Check if it has VOCdevkit structure","        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(colab_train_path))","        # Check if files are directly in the folder (no VOCdevkit wrapper)","        elif (colab_train_path / 'JPEGImages').exists():","            # Create a symlink or use parent structure","            dataset_roots.append(str(colab_train_path.parent))","    ","    # Check for local structure","    local_data_path = Path('./data')","    if local_data_path.exists():","        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(local_data_path))","        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():","            dataset_roots.append(str(local_data_path))","    ","    # Check standard paths","    dataset_roots.extend([","        '/content/data',  # Google Colab default","        './data',  # Local path","        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),","    ])","    ","    # Check dataset_path.txt","    dataset_path_file = Path('dataset_path.txt')","    if dataset_path_file.exists():","        with open(dataset_path_file, 'r') as f:","            saved_path = Path(f.read().strip())","            dataset_roots.insert(0, str(saved_path.parent.parent))","    ","    dataset_root = None","    for root in dataset_roots:","        try:","            # Try to create a temporary symlink structure if needed","            root_path = Path(root)","            ","            # Check if we need to create VOCdevkit/VOC2012 structure","            voc_path = root_path / 'VOC2012_train_val'","            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():","                # Files are directly in VOC2012_train_val, create temporary structure","                vocdevkit_path = voc_path / 'VOCdevkit'","                voc2012_path = vocdevkit_path / 'VOC2012'","                if not voc2012_path.exists():","                    print(f\"Creating VOCdevkit structure in {voc_path}\")","                    vocdevkit_path.mkdir(exist_ok=True)","                    voc2012_path.symlink_to(voc_path, target_is_directory=True)","            ","            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)","            dataset_root = root","            print(f\"✓ Successfully loaded dataset from: {root}\")","            break","        except Exception as e:","            continue","    ","    if dataset_root is None:","        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")","    ","    print(f\"Dataset found at: {dataset_root}\")","    ","    # Create datasets","    train_transform = VOCNormalize(augment=True)","    val_transform = VOCNormalize(augment=False)","    ","    train_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='train',","        download=False,","        transforms=train_transform","    )","    ","    val_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='val',","        download=False,","        transforms=val_transform","    )","    ","    print(f\"Training set size: {len(train_dataset)}\")","    print(f\"Validation set size: {len(val_dataset)}\")","    ","    # Create dataloaders","    train_loader = DataLoader(","        train_dataset,","        batch_size=args.batch_size,","        shuffle=True,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    val_loader = DataLoader(","        val_dataset,","        batch_size=4,","        shuffle=False,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    # Setup optimizer and scheduler","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)","    ","    # Training history","    history = {","        'train_loss': [],","        'train_ce_loss': [],","        'train_kd_loss': [],","        'val_loss': [],","        'val_miou': [],","        'epoch_times': []","    }","    ","    # Save hyperparameters","    hyperparameters = {","        'model': args.model,","        'kd_mode': args.kd_mode,","        'epochs': args.epochs,","        'batch_size': args.batch_size,","        'learning_rate': args.lr,","        'weight_decay': args.weight_decay,","        'temperature': args.temperature,","        'alpha': args.alpha,","        'beta': args.beta,","        'optimizer': 'Adam',","        'scheduler': 'CosineAnnealingLR',","        'num_parameters': sum(p.numel() for p in model.parameters()),","        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)","    }","    ","    best_miou = 0.0","    training_start_time = time.time()","    ","    # Training loop","    print(\"\\nStarting training...\")","    print(\"=\" * 70)","    print(\"\\nHyperparameters:\")","    for key, value in hyperparameters.items():","        print(f\"  {key}: {value}\")","    ","    for epoch in range(args.epochs):","        epoch_start_time = time.time()","        ","        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")","        print(\"-\" * 70)","        ","        # Train","        train_loss, train_ce_loss, train_kd_loss = train_epoch(","            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args","        )","        ","        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")","        ","        # Validate","        val_miou = validate(model, val_loader, device)","        print(f\"Validation mIoU: {val_miou:.4f}\")","        ","        # Record epoch time","        epoch_time = time.time() - epoch_start_time","        history['epoch_times'].append(epoch_time)","        print(f\"Epoch time: {epoch_time:.2f} seconds\")","        ","        # Update history","        history['train_loss'].append(train_loss)","        history['train_ce_loss'].append(train_ce_loss)","        history['train_kd_loss'].append(train_kd_loss)","        history['val_miou'].append(val_miou)","        ","        # Save best model","        if val_miou > best_miou:","            best_miou = val_miou","            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history","            }, save_path)","            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")","        ","        # Update scheduler","        scheduler.step()","        ","        # Save checkpoint every 10 epochs","        if (epoch + 1) % 10 == 0:","            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history,","                'hyperparameters': hyperparameters","            }, checkpoint_path)","            print(f\"Checkpoint saved: {checkpoint_path}\")","    ","    # Calculate total training time","    total_training_time = time.time() - training_start_time","    avg_epoch_time = np.mean(history['epoch_times'])","    ","    # Plot training curves","    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"","    plot_training_curves(history, plot_path)","    ","    # Save final training report","    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"","    with open(report_path, 'w') as f:","        f.write(\"=\" * 70 + \"\\n\")","        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")","        f.write(\"=\" * 70 + \"\\n\\n\")","        ","        f.write(\"HYPERPARAMETERS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        for key, value in hyperparameters.items():","            f.write(f\"  {key:25s}: {value}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"TRAINING STATISTICS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")","        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")","        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")","        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")","        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"FILES GENERATED:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")","        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")","        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")","        f.write(\"=\" * 70 + \"\\n\")","    ","    print(\"\\n\" + \"=\" * 70)","    print(\"Training completed!\")","    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")","    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")","    print(f\"Best validation mIoU: {best_miou:.4f}\")","    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")","    print(f\"Training report saved to: {report_path}\")","    ","    # Calculate final score","    score = 4 * best_miou / (1 + num_params / 1e6)","    print(f\"\\nFinal Score: {score:.3f}\")","    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")","    print(\"=\" * 70)","","","if __name__ == '__main__':","    parser = argparse.ArgumentParser(description='Train compact segmentation model')","    ","    # Model arguments","    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],","                        help='Model type to train')","    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],","                        help='Knowledge distillation mode')","    ","    # Training arguments","    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')","    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')","    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')","    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')","    ","    # KD arguments","    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')","    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')","    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')","    ","    # Other arguments","    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')","    ","    args = parser.parse_args()","    ","    main(args)",""],"recentEdit":[[582,583,["        val_miou, val_loss = validate(model, val_loader, device)"]],[583,584,["        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")"]],[594,594,["        history['val_loss'].append(val_loss)"]],[652,652,["        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")"]]],"documentBeforeEdits":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean_iou: mean IoU across all samples\r\n        mean_loss: mean cross-entropy loss\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    all_losses = []\r\n    criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot - now includes both train and validation loss\r\n    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training and Validation Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path, dpi=150)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","recentEdits":[[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]]],"lastSelectionInAfterEdit":[18491,18491]},{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"plaintext","documentLinesBeforeEdit":["# ELEC 475 Lab 3 - Training Commands","# All commands should be run from the Lab 3 directory","","# ============================================================================","# ULTRA-COMPACT MODEL (475K parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# STANDARD MODEL (3.4M parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# TRAINING OUTPUT FILES","# ============================================================================","# For each training run, the following files will be generated in ./checkpoints/:","#   - best_model_{model}_{kd_mode}.pth           (best model weights)","#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)","#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)","#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)","","# ============================================================================","# OPTIONAL: HYPERPARAMETER TUNING","# ============================================================================","# If you want to experiment with different hyperparameters:","","# Adjust learning rate","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005","","# Adjust KD temperature (higher = softer targets)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0","","# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3","","# Adjust batch size (if memory issues)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4","","# ============================================================================","# NOTES","# ============================================================================","# - Training on CPU will be slow. Consider using Google Colab with GPU.","# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU","# - Monitor validation mIoU - training should converge after 30-40 epochs","# - Best model is saved automatically when validation mIoU improves",""],"recentEdit":[[18,19,["# STANDARD MODEL (1 parameters)"]]],"documentBeforeEdits":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","recentEdits":[[[867,871,"1"]]],"lastSelectionInAfterEdit":[868,868]}],"activeDocumentIdx":1,"recording":[{"documentType":"workspaceRecording@1.0","kind":"header","repoRootUri":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","time":1762624143621,"uuid":"1ab12273-5d43-4ee2-a94e-294b5db859b2"},{"kind":"documentEncountered","id":4,"relativePath":"model_standard.py","time":1762545107282},{"kind":"setContent","id":4,"v":1,"content":"\"\"\"\r\nStandard MobileNetV3-Small Segmentation Model\r\nELEC 475 Lab 3 - Option 1\r\n\r\nParameters: ~3.41M\r\nExpected Score: 0.40-0.50\r\n\r\nReferences:\r\n- MobileNetV3: Searching for MobileNetV3 (Howard et al., 2019)\r\n- DeepLabV3+: Encoder-Decoder with Atrous Separable Convolution (Chen et al., 2018)\r\n- ASPP: Rethinking Atrous Convolution for Semantic Image Segmentation (Chen et al., 2017)\r\n\"\"\"\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\r\n\r\n\r\nclass ASPP(nn.Module):\r\n    \"\"\"\r\n    Atrous Spatial Pyramid Pooling (ASPP) module\r\n    Captures multi-scale context using parallel atrous convolutions\r\n    \"\"\"\r\n    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\r\n        super(ASPP, self).__init__()\r\n        \r\n        self.branches = nn.ModuleList()\r\n        \r\n        # 1x1 convolution branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Atrous convolution branches\r\n        for rate in rates:\r\n            self.branches.append(nn.Sequential(\r\n                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\r\n                nn.BatchNorm2d(out_channels),\r\n                nn.ReLU(inplace=True)\r\n            ))\r\n        \r\n        # Global average pooling branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.AdaptiveAvgPool2d(1),\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Combine all branches\r\n        num_branches = len(rates) + 2  # 1x1 + atrous branches + global pooling\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(out_channels * num_branches, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Dropout(0.5)\r\n        )\r\n    \r\n    def forward(self, x):\r\n        h, w = x.shape[2:]\r\n        branch_outputs = []\r\n        \r\n        for i, branch in enumerate(self.branches):\r\n            out = branch(x)\r\n            # Upsample global pooling branch to match spatial dimensions\r\n            if i == len(self.branches) - 1:\r\n                out = F.interpolate(out, size=(h, w), mode='bilinear', align_corners=False)\r\n            branch_outputs.append(out)\r\n        \r\n        # Concatenate all branches\r\n        x = torch.cat(branch_outputs, dim=1)\r\n        x = self.project(x)\r\n        \r\n        return x\r\n\r\n\r\nclass StandardSegmentationModel(nn.Module):\r\n    \"\"\"\r\n    Standard compact segmentation model using MobileNetV3-Small backbone\r\n    \r\n    Architecture:\r\n    - Full MobileNetV3-Small backbone (pretrained)\r\n    - 3-rate ASPP module (rates: 6, 12, 18)\r\n    - Two-stage decoder with skip connections\r\n    - Parameters: ~3.41M\r\n    \"\"\"\r\n    def __init__(self, num_classes=21, pretrained=True):\r\n        super(StandardSegmentationModel, self).__init__()\r\n        \r\n        # Load pretrained MobileNetV3-Small\r\n        if pretrained:\r\n            weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1\r\n            backbone = mobilenet_v3_small(weights=weights)\r\n        else:\r\n            backbone = mobilenet_v3_small(weights=None)\r\n        \r\n        # Extract feature layers from MobileNetV3-Small\r\n        self.features = backbone.features\r\n        \r\n        # Feature dimensions at tap points:\r\n        # Stage 1 (low):  16 channels, stride 4\r\n        # Stage 3 (mid):  24 channels, stride 8\r\n        # Stage 12 (high): 576 channels, stride 16\r\n        \r\n        # ASPP module for multi-scale context\r\n        self.aspp = ASPP(in_channels=576, out_channels=128, rates=[6, 12, 18])\r\n        \r\n        # Feature projection layers (keep dimensions)\r\n        self.mid_conv = nn.Sequential(\r\n            nn.Conv2d(24, 24, 1, bias=False),\r\n            nn.BatchNorm2d(24),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.low_conv = nn.Sequential(\r\n            nn.Conv2d(16, 16, 1, bias=False),\r\n            nn.BatchNorm2d(16),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Decoder with skip connections\r\n        self.decoder_conv1 = nn.Sequential(\r\n            nn.Conv2d(128 + 24, 128, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(128),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.decoder_conv2 = nn.Sequential(\r\n            nn.Conv2d(128 + 16, 64, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(64),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Classifier head\r\n        self.classifier = nn.Sequential(\r\n            nn.Dropout(0.5),\r\n            nn.Conv2d(64, num_classes, 1)\r\n        )\r\n        \r\n        self._init_weights()\r\n    \r\n    def _init_weights(self):\r\n        \"\"\"Initialize weights for new layers\"\"\"\r\n        for m in [self.aspp, self.mid_conv, self.low_conv, self.decoder_conv1, self.decoder_conv2, self.classifier]:\r\n            for module in m.modules():\r\n                if isinstance(module, nn.Conv2d):\r\n                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\r\n                    if module.bias is not None:\r\n                        nn.init.constant_(module.bias, 0)\r\n                elif isinstance(module, nn.BatchNorm2d):\r\n                    nn.init.constant_(module.weight, 1)\r\n                    nn.init.constant_(module.bias, 0)\r\n    \r\n    def forward(self, x, return_features=False):\r\n        \"\"\"\r\n        Forward pass\r\n        \r\n        Args:\r\n            x: input image tensor (B, 3, H, W)\r\n            return_features: if True, return intermediate features for knowledge distillation\r\n            \r\n        Returns:\r\n            output: segmentation logits (B, num_classes, H, W)\r\n            features (optional): dict of intermediate features for KD\r\n        \"\"\"\r\n        input_shape = x.shape[-2:]\r\n        \r\n        # Extract multi-scale features from backbone\r\n        low_feat = None   # stride 4\r\n        mid_feat = None   # stride 8\r\n        high_feat = None  # stride 16\r\n        \r\n        # Forward through MobileNetV3 stages\r\n        for i, layer in enumerate(self.features):\r\n            x = layer(x)\r\n            \r\n            # Capture features at specific stages\r\n            if i == 1:  # After first inverted residual block (stride 4)\r\n                low_feat = x\r\n            elif i == 3:  # After third inverted residual block (stride 8)\r\n                mid_feat = x\r\n        \r\n        # Final high-level features (stride 16)\r\n        high_feat = x\r\n        \r\n        # Process features\r\n        mid_feat = self.mid_conv(mid_feat)\r\n        low_feat = self.low_conv(low_feat)\r\n        \r\n        # ASPP for multi-scale context\r\n        x = self.aspp(high_feat)\r\n        \r\n        # Decoder with skip connections\r\n        # Upsample and concatenate with mid-level features\r\n        x = F.interpolate(x, size=mid_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, mid_feat], dim=1)\r\n        x = self.decoder_conv1(x)\r\n        \r\n        # Upsample and concatenate with low-level features\r\n        x = F.interpolate(x, size=low_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, low_feat], dim=1)\r\n        x = self.decoder_conv2(x)\r\n        \r\n        # Final upsampling to input resolution\r\n        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\r\n        \r\n        # Classifier\r\n        output = self.classifier(x)\r\n        \r\n        if return_features:\r\n            features = {\r\n                'low': low_feat,\r\n                'mid': mid_feat,\r\n                'high': high_feat\r\n            }\r\n            return output, features\r\n        \r\n        return output\r\n\r\n\r\ndef count_parameters(model):\r\n    \"\"\"Count the number of trainable parameters\"\"\"\r\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(\"=\" * 70)\r\n    print(\"Standard MobileNetV3-Small Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Create model\r\n    model = StandardSegmentationModel(num_classes=21, pretrained=False)\r\n    \r\n    # Count parameters\r\n    params = count_parameters(model)\r\n    print(f\"\\nTotal parameters: {params:,} ({params/1e6:.2f}M)\")\r\n    \r\n    # Test forward pass\r\n    print(\"\\nTesting forward pass...\")\r\n    dummy_input = torch.randn(2, 3, 224, 224)\r\n    \r\n    with torch.no_grad():\r\n        output = model(dummy_input)\r\n        print(f\"  Output shape: {output.shape}\")\r\n        \r\n        # Test with feature extraction\r\n        output, features = model(dummy_input, return_features=True)\r\n        print(f\"\\n  Features for knowledge distillation:\")\r\n        for name, feat in features.items():\r\n            print(f\"    {name}: {feat.shape}\")\r\n    \r\n    # Score estimation\r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Score Estimation: Score = 4 × mIoU / (1 + params_M)\")\r\n    print(\"=\" * 70)\r\n    \r\n    for miou in [0.40, 0.45, 0.50, 0.55]:\r\n        score = 4 * miou / (1 + params/1e6)\r\n        print(f\"  mIoU={miou:.2f} → Score={score:.3f}\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n","time":1762545107282},{"kind":"opened","id":4,"time":1762545107282},{"kind":"documentEncountered","id":5,"relativePath":"train.py","time":1762545107284},{"kind":"setContent","id":5,"v":1,"content":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean IoU\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Resize back to original size\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    return mean_iou\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot\r\n    axes[0].plot(history['train_loss'], label='Train Loss')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU')\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","time":1762622282025},{"kind":"opened","id":5,"time":1762622282025},{"kind":"documentEncountered","id":17,"relativePath":"train.txt","time":1762622441478},{"kind":"setContent","id":17,"v":1,"content":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","time":1762622441593},{"kind":"opened","id":17,"time":1762622441593},{"kind":"changed","id":4,"v":2,"edit":[[50,50," (Optimized)"],[94,94,"1."],[95,95,"6M (reduced from 3"],[99,99,")"],[119,120,"8"],[122,123,"1"],[124,125,"2"]],"time":1762623638975},{"kind":"changed","id":4,"v":3,"edit":[[2834,2834," (Optimized)"],[2919,2920,"Lightweight 2"],[2951,2955,""],[2956,2956," - reduced from 3 rates"],[2964,2972,"Simplifi"],[2973,2973,"d"],[2987,2991,"reduced"],[2993,2994,"ha"],[2997,3002,"l"],[3024,3024,"1."],[3025,3025,"6M (reduced from 3"],[3029,3029,")"],[3767,3767,"Lightweight "],[3779,3782,"(2"],[3783,3786,"ra"],[3787,3787,"es "],[3788,3789,"n"],[3790,3790,"tead of 3, redu"],[3791,3793,""],[3794,3794,"d output"],[3796,3797,"han"],[3798,3799,""],[3800,3802,"ls)"],[3859,3862,"64"],[3876,3880,""],[3931,3933,"r"],[3934,3936,""],[3937,3939,"uc"],[3940,3940,"d cha"],[3941,3944,""],[3945,3945,"el"],[4015,4017,"16"],[4063,4065,"16"],[4191,4193,"8"],[4239,4241,"8"],[4310,4310," Simplified"],[4311,4312,"d"],[4324,4328,"reduced"],[4330,4331,"ha"],[4334,4339,"l"],[4409,4412,"64"],[4415,4417,"16"],[4419,4422,"64"],[4479,4482,"64"],[4609,4612,"64"],[4615,4617,"8"],[4619,4621,"32"],[4678,4680,"32"],[4861,4863,"32"]],"time":1762623641194},{"kind":"changed","id":5,"v":2,"edit":[[10251,10251,"_iou:"],[10252,10252,"mean "],[10255,10255," across all samples"],[10261,10261,"    mean_loss: mean cross-entropy loss\r\n    "],[10307,10307,"all_losses = []"],[10313,10313,"criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    "],[10403,10403,"\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss"]],"time":1762623765832},{"kind":"changed","id":5,"v":3,"edit":[[11970,12883,""]],"time":1762623780348},{"kind":"changed","id":5,"v":4,"edit":[[12136,12136," - now includes both train and validation loss"],[12188,12188,"in Loss', l"],[12190,12190,"ewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val"],[12196,12196,", linewidth=2, linestyle='--'"],[12295,12295," and Validation"],[12434,12434,", linewidth=2"],[12645,12645,", dpi=150"]],"time":1762623791607},{"kind":"changed","id":5,"v":5,"edit":[[19126,19126,"loss': [],\r\n        'val_"]],"time":1762623803059},{"kind":"changed","id":5,"v":6,"edit":[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],"time":1762623816438},{"kind":"changed","id":5,"v":7,"edit":[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]],"time":1762623835010},{"kind":"selectionChanged","id":17,"selection":[[883,883]],"time":1762624017851},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624140244},{"kind":"selectionChanged","id":17,"selection":[[869,870]],"time":1762624140330},{"kind":"selectionChanged","id":17,"selection":[[868,870]],"time":1762624140356},{"kind":"selectionChanged","id":17,"selection":[[867,870]],"time":1762624140378},{"kind":"changed","id":17,"v":7,"edit":[[867,870,"1"]],"time":1762624143421},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624143443},{"kind":"changed","id":17,"v":9,"edit":[[868,869,""]],"time":1762624143557}]},"resultEdit":"   16  16 \n   17  17 # ============================================================================\n-  18     # STANDARD MODEL (1 parameters)\n+      18 # STANDARD MODEL (1.3M parameters)\n   19  19 # ============================================================================\n   20  20 \n   21  21 # Baseline (No Knowledge Distillation)","isCachedResult":true,"prompt":"System\n------\nYour role as an AI assistant is to help developers complete their code tasks by assisting in editing specific sections of code marked by the <|code_to_edit|> and <|/code_to_edit|> tags, while adhering to Microsoft's content policies and avoiding the creation of content that violates copyrights.\n\nYou have access to the following information to help you make informed suggestions:\n\n- recently_viewed_code_snippets: These are code snippets that the developer has recently looked at, which might provide context or examples relevant to the current task. They are listed from oldest to newest, with line numbers in the form #| to help you understand the edit diff history. It's possible these are entirely irrelevant to the developer's change.\n- current_file_content: The content of the file the developer is currently working on, providing the broader context of the code. Line numbers in the form #| are included to help you understand the edit diff history.\n- edit_diff_history: A record of changes made to the code, helping you understand the evolution of the code and the developer's intentions. These changes are listed from oldest to latest. It's possible a lot of old edit diff history is entirely irrelevant to the developer's change.\n- area_around_code_to_edit: The context showing the code surrounding the section to be edited.\n- cursor position marked as <|cursor|>: Indicates where the developer's cursor is currently located, which can be crucial for understanding what part of the code they are focusing on.\n\nYour task is to predict and complete the changes the developer would have made next in the <|code_to_edit|> section. The developer may have stopped in the middle of typing. Your goal is to keep the developer on the path that you think they're following. Some examples include further implementing a class, method, or variable, or improving the quality of the code. Make sure the developer doesn't get distracted and ensure your suggestion is relevant. Consider what changes need to be made next, if any. If you think changes should be made, ask yourself if this is truly what needs to happen. If you are confident about it, then proceed with the changes.\n\n# Steps\n\n1. **Review Context**: Analyze the context from the resources provided, such as recently viewed snippets, edit history, surrounding code, and cursor location.\n2. **Evaluate Current Code**: Determine if the current code within the tags requires any corrections or enhancements.\n3. **Suggest Edits**: If changes are required, ensure they align with the developer's patterns and improve code quality.\n4. **Maintain Consistency**: Ensure indentation and formatting follow the existing code style.\n\n# Output Format\n\n- Provide only the revised code within the tags. If no changes are necessary, simply return the original code from within the <|code_to_edit|> and <|/code_to_edit|> tags.\n- There are line numbers in the form #| in the code displayed to you above, but these are just for your reference. Please do not include the numbers of the form #| in your response.\n- Ensure that you do not output duplicate code that exists outside of these tags. The output should be the revised code that was between these tags and should not include the <|code_to_edit|> or <|/code_to_edit|> tags.\n\n```\n// Your revised code goes here\n```\n\n# Notes\n\n- Apologize with \"Sorry, I can't assist with that.\" for requests that may breach Microsoft content guidelines.\n- Avoid undoing or reverting the developer's last change unless there are obvious typos or errors.\n- Don't include the line numbers of the form #| in your response.\n==================\nUser\n------\n```\n<|recently_viewed_code_snippets|>\n<|recently_viewed_code_snippet|>\ncode_snippet_file_path: /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py (truncated)\n\"\"\"\nTraining Script for Compact Segmentation Models with Knowledge Distillation\nELEC 475 Lab 3\n\nSupports three training modes:\n1. No Knowledge Distillation (baseline)\n2. Response-based Knowledge Distillation\n3. Feature-based Knowledge Distillation\n\nReferences:\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\n\"\"\"\n\nimport argparse\nimport os\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import VOCSegmentation\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\nfrom tqdm import tqdm\n\n# Import custom models\nfrom model_ultracompact import UltraCompactSegmentationModel\nfrom model_standard import StandardSegmentationModel\n\n\nclass VOCNormalize:\n    \"\"\"Custom normalization for VOC dataset\"\"\"\n    def __init__(self, augment=True):\n        self.augment = augment\n        self.normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    \n    def __call__(self, img, target):\n        # Convert PIL to tensor\n        img = transforms.ToTensor()(img)\n        \n        # Apply augmentation if training\n        if self.augment and np.random.rand() > 0.5:\n            # Random horizontal flip\n            img = transforms.functional.hflip(img)\n            target = transforms.functional.hflip(target)\n        \n        # Normalize image\n        img = self.normalize(img)\n        \n        # Convert target to tensor\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n        \n        return img, target\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\n    images, targets = zip(*batch)\n    return list(images), list(targets)\n\n\ndef calculate_miou(pred, target, num_classes=21):\n    \"\"\"\n    Calculate mean Intersection over Union (mIoU)\n    \n    Args:\n        pred: predicted segmentation mask (H, W)\n        target: ground truth segmentation mask (H, W)\n        num_classes: number of classes\n    \n    Returns:\n        mIoU value\n    \"\"\"\n    pred = pred.cpu().numpy()\n    target = target.cpu().numpy()\n    \n    ious = []\n    for cls in range(num_classes):\n        pred_mask = (pred == cls)\n        target_mask = (target == cls)\n        \n        intersection = np.logical_and(pred_mask, target_mask).sum()\n        union = np.logical_or(pred_mask, target_mask).sum()\n        \n        if union == 0:\n            continue\n        \n        iou = intersection / union\n        ious.append(iou)\n    \n    return np.mean(ious) if len(ious) > 0 else 0.0\n\n\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\n    \"\"\"\n    Response-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        teacher_logits: teacher model output logits (B, C, H, W)\n        targets: ground truth labels (B, H, W)\n        temperature: softmax temperature for distillation\n        alpha: weight for cross-entropy loss\n        beta: weight for distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Distillation loss (KL divergence)\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\n    \n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * kd_loss\n    \n    return total_loss, ce_loss.item(), kd_loss.item()\n\n\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\n    \"\"\"\n    Feature-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        student_features: dict of student intermediate features\n        teacher_features: dict of teacher intermediate features\n        targets: ground truth labels (B, H, W)\n        alpha: weight for cross-entropy loss\n        beta: weight for feature distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Feature matching loss (cosine similarity)\n    feat_loss = 0\n    num_levels = 0\n    \n    for level in ['low', 'mid', 'high']:\n        if level in student_features and level in teacher_features:\n            s_feat = student_features[level]\n            t_feat = teacher_features[level]\n            \n            # Resize if needed (teacher features might be different size)\n            if s_feat.shape != t_feat.shape:\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n                # Match channels if needed\n                if s_feat.shape[1] != t_feat.shape[1]:\n                    # Project teacher features to student channel size\n                    continue  # Skip if channel mismatch (or add projection layer)\n            \n            # Cosine similarity loss (1 - cosine_similarity)\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\n            \n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\n            feat_loss += (1 - cosine_sim)\n            num_levels += 1\n    \n    if num_levels > 0:\n        feat_loss = feat_loss / num_levels\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * feat_loss\n    \n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\n\n\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\n    \"\"\"\n    Train for one epoch\n    \n    Args:\n        model: student model\n        teacher_model: teacher model (None if kd_mode='none')\n        dataloader: training data loader\n        optimizer: optimizer\n        device: device to run on\n        kd_mode: 'none', 'response', or 'feature'\n        args: training arguments\n    \n    Returns:\n        average loss, average ce_loss, average kd_loss\n    \"\"\"\n    model.train()\n    if teacher_model is not None:\n        teacher_model.eval()\n    \n    total_loss = 0\n    total_ce_loss = 0\n    total_kd_loss = 0\n    num_batches = 0\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for images, targets in pbar:\n        # Move to device and resize to fixed size for batching\n        batch_imgs = []\n        batch_targets = []\n        \n        for img, target in zip(images, targets):\n            img = img.to(device)\n            target = target.to(device)\n            \n            # Resize to fixed size\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\n            \n            batch_imgs.append(img)\n            batch_targets.append(target)\n        \n        batch_imgs = torch.stack(batch_imgs)\n<|/recently_viewed_code_snippet|>\n<|/recently_viewed_code_snippets|>\n\n<|current_file_content|>\ncurrent_file_path: train.txt\n# ELEC 475 Lab 3 - Training Commands\n# All commands should be run from the Lab 3 directory\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n# STANDARD MODEL (1<|cursor|> parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\n\n# ============================================================================\n# OPTIONAL: HYPERPARAMETER TUNING\n# ============================================================================\n# If you want to experiment with different hyperparameters:\n\n# Adjust learning rate\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\n\n# Adjust KD temperature (higher = softer targets)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\n\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\n\n# Adjust batch size (if memory issues)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\n\n# ============================================================================\n# NOTES\n# ============================================================================\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\n# - Monitor validation mIoU - training should converge after 30-40 epochs\n# - Best model is saved automatically when validation mIoU improves\n\n<|/current_file_content|>\n\n<|edit_diff_history|>\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -342,23 +342,0 @@\n-            for img, target in zip(images, targets):\n-                img = img.unsqueeze(0).to(device)\n-                target = target.to(device)\n-                \n-                # Resize to fixed size\n-                h, w = target.shape\n-                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\n-                \n-                # Forward pass\n-                output = model(img)\n-                \n-                # Resize back to original size\n-                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\n-                \n-                # Get predictions\n-                pred = output.squeeze(0).argmax(0)\n-                \n-                # Calculate IoU\n-                iou = calculate_miou(pred, target, num_classes)\n-                all_ious.append(iou)\n-    \n-    mean_iou = np.mean(all_ious)\n-    return mean_iou\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -348,1 +348,1 @@\n-    # Loss plot\n+    # Loss plot - now includes both train and validation loss\n@@ -349,1 +349,2 @@\n-    axes[0].plot(history['train_loss'], label='Train Loss')\n+    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n+    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\n@@ -352,1 +352,1 @@\n-    axes[0].set_title('Training Loss')\n+    axes[0].set_title('Training and Validation Loss')\n@@ -357,1 +357,1 @@\n-    axes[1].plot(history['val_miou'], label='Validation mIoU')\n+    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\n@@ -365,1 +365,1 @@\n-    plt.savefig(save_path)\n+    plt.savefig(save_path, dpi=150)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -535,1 +535,2 @@\n-        'val_miou': [],\n+        'val_loss': [],\n+        'val_miou': [],\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -581,1 +581,1 @@\n-        val_miou = validate(model, val_loader, device)\n+        val_miou, val_loss = validate(model, val_loader, device)\n@@ -582,1 +582,1 @@\n-        print(f\"Validation mIoU: {val_miou:.4f}\")\n+        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")\n@@ -592,1 +592,2 @@\n-        history['train_kd_loss'].append(train_kd_loss)\n+        history['train_kd_loss'].append(train_kd_loss)\n+        history['val_loss'].append(val_loss)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -652,1 +652,2 @@\n-        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n@@ -17,1 +17,1 @@\n-# STANDARD MODEL (3.4M parameters)\n+# STANDARD MODEL (1 parameters)\n\n<|/edit_diff_history|>\n\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n# STANDARD MODEL (1<|cursor|> parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n```\n\nThe developer was working on a section of code within the tags `code_to_edit` in the file located at `train.txt`. Using the given `recently_viewed_code_snippets`, `current_file_content`, `edit_diff_history`, `area_around_code_to_edit`, and the cursor position marked as `<|cursor|>`, please continue the developer's work. Update the `code_to_edit` section by predicting and completing the changes they would have made next. Provide the revised code that was between the `<|code_to_edit|>` and `<|/code_to_edit|>` tags with the following format, but do not include the tags themselves.\n```\n// Your revised code goes here\n```\n==================","error":"undefined","response":"```\n\n# ============================================================================\n# STANDARD MODEL (1.3M parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n```","responseResults":"- replaceRange:\n   start: 849\n   endExclusive: 880\n  newText: \"# STANDARD MODEL (1.3M parameters)\"\n","logs":["```\n{\n\t\"isAsyncCompletions\": true,\n\t\"isRevisedCacheStrategy\": true,\n\t\"isCacheTracksRejections\": true,\n\t\"isRecentlyShownCacheEnabled\": false,\n\t\"debounceUseCoreRequestTime\": false\n}\n```\n"]}
{"requestId":5,"time":1762624151170,"filePath":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","version":6,"statelessNextEditProviderId":"XtabProvider","nextEditRequest":{"id":"9fd4b880-9e9b-4903-b0b5-081b2290055a","documents":[{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.py","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"python","documentLinesBeforeEdit":["\"\"\"","Training Script for Compact Segmentation Models with Knowledge Distillation","ELEC 475 Lab 3","","Supports three training modes:","1. No Knowledge Distillation (baseline)","2. Response-based Knowledge Distillation","3. Feature-based Knowledge Distillation","","References:","- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)","- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)","\"\"\"","","import argparse","import os","import time","from pathlib import Path","import numpy as np","import matplotlib.pyplot as plt","","import torch","import torch.nn as nn","import torch.nn.functional as F","from torch.utils.data import DataLoader","from torchvision import transforms","from torchvision.datasets import VOCSegmentation","from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights","from tqdm import tqdm","","# Import custom models","from model_ultracompact import UltraCompactSegmentationModel","from model_standard import StandardSegmentationModel","","","class VOCNormalize:","    \"\"\"Custom normalization for VOC dataset\"\"\"","    def __init__(self, augment=True):","        self.augment = augment","        self.normalize = transforms.Normalize(","            mean=[0.485, 0.456, 0.406],","            std=[0.229, 0.224, 0.225]","        )","    ","    def __call__(self, img, target):","        # Convert PIL to tensor","        img = transforms.ToTensor()(img)","        ","        # Apply augmentation if training","        if self.augment and np.random.rand() > 0.5:","            # Random horizontal flip","            img = transforms.functional.hflip(img)","            target = transforms.functional.hflip(target)","        ","        # Normalize image","        img = self.normalize(img)","        ","        # Convert target to tensor","        target = torch.as_tensor(np.array(target), dtype=torch.int64)","        ","        return img, target","","","def collate_fn(batch):","    \"\"\"Custom collate function to handle variable-sized images\"\"\"","    images, targets = zip(*batch)","    return list(images), list(targets)","","","def calculate_miou(pred, target, num_classes=21):","    \"\"\"","    Calculate mean Intersection over Union (mIoU)","    ","    Args:","        pred: predicted segmentation mask (H, W)","        target: ground truth segmentation mask (H, W)","        num_classes: number of classes","    ","    Returns:","        mIoU value","    \"\"\"","    pred = pred.cpu().numpy()","    target = target.cpu().numpy()","    ","    ious = []","    for cls in range(num_classes):","        pred_mask = (pred == cls)","        target_mask = (target == cls)","        ","        intersection = np.logical_and(pred_mask, target_mask).sum()","        union = np.logical_or(pred_mask, target_mask).sum()","        ","        if union == 0:","            continue","        ","        iou = intersection / union","        ious.append(iou)","    ","    return np.mean(ious) if len(ious) > 0 else 0.0","","","def response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):","    \"\"\"","    Response-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        teacher_logits: teacher model output logits (B, C, H, W)","        targets: ground truth labels (B, H, W)","        temperature: softmax temperature for distillation","        alpha: weight for cross-entropy loss","        beta: weight for distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Distillation loss (KL divergence)","    student_soft = F.log_softmax(student_logits / temperature, dim=1)","    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)","    ","    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * kd_loss","    ","    return total_loss, ce_loss.item(), kd_loss.item()","","","def feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):","    \"\"\"","    Feature-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        student_features: dict of student intermediate features","        teacher_features: dict of teacher intermediate features","        targets: ground truth labels (B, H, W)","        alpha: weight for cross-entropy loss","        beta: weight for feature distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Feature matching loss (cosine similarity)","    feat_loss = 0","    num_levels = 0","    ","    for level in ['low', 'mid', 'high']:","        if level in student_features and level in teacher_features:","            s_feat = student_features[level]","            t_feat = teacher_features[level]","            ","            # Resize if needed (teacher features might be different size)","            if s_feat.shape != t_feat.shape:","                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)","                # Match channels if needed","                if s_feat.shape[1] != t_feat.shape[1]:","                    # Project teacher features to student channel size","                    continue  # Skip if channel mismatch (or add projection layer)","            ","            # Cosine similarity loss (1 - cosine_similarity)","            s_feat_norm = F.normalize(s_feat, p=2, dim=1)","            t_feat_norm = F.normalize(t_feat, p=2, dim=1)","            ","            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()","            feat_loss += (1 - cosine_sim)","            num_levels += 1","    ","    if num_levels > 0:","        feat_loss = feat_loss / num_levels","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * feat_loss","    ","    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0","","","def train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):","    \"\"\"","    Train for one epoch","    ","    Args:","        model: student model","        teacher_model: teacher model (None if kd_mode='none')","        dataloader: training data loader","        optimizer: optimizer","        device: device to run on","        kd_mode: 'none', 'response', or 'feature'","        args: training arguments","    ","    Returns:","        average loss, average ce_loss, average kd_loss","    \"\"\"","    model.train()","    if teacher_model is not None:","        teacher_model.eval()","    ","    total_loss = 0","    total_ce_loss = 0","    total_kd_loss = 0","    num_batches = 0","    ","    pbar = tqdm(dataloader, desc='Training')","    for images, targets in pbar:","        # Move to device and resize to fixed size for batching","        batch_imgs = []","        batch_targets = []","        ","        for img, target in zip(images, targets):","            img = img.to(device)","            target = target.to(device)","            ","            # Resize to fixed size","            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)","            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()","            ","            batch_imgs.append(img)","            batch_targets.append(target)","        ","        batch_imgs = torch.stack(batch_imgs)","        batch_targets = torch.stack(batch_targets)","        ","        optimizer.zero_grad()","        ","        # Forward pass based on KD mode","        if kd_mode == 'none':","            # No knowledge distillation","            output = model(batch_imgs)","            loss = F.cross_entropy(output, batch_targets, ignore_index=255)","            ce_loss_val = loss.item()","            kd_loss_val = 0.0","            ","        elif kd_mode == 'response':","            # Response-based KD","            output = model(batch_imgs)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","            ","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","            ","        elif kd_mode == 'feature':","            # Feature-based KD","            output, student_features = model(batch_imgs, return_features=True)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)","                # For now, use output-based distillation","                teacher_features = {}  # Would need to extract from teacher","            ","            # Fallback to response-based if teacher features not available","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","        ","        # Backward pass","        loss.backward()","        optimizer.step()","        ","        total_loss += loss.item()","        total_ce_loss += ce_loss_val","        total_kd_loss += kd_loss_val","        num_batches += 1","        ","        pbar.set_postfix({'loss': f'{loss.item():.4f}'})","    ","    avg_loss = total_loss / num_batches","    avg_ce_loss = total_ce_loss / num_batches","    avg_kd_loss = total_kd_loss / num_batches","    ","    return avg_loss, avg_ce_loss, avg_kd_loss","","","def validate(model, dataloader, device, num_classes=21):","    \"\"\"","    Validate the model","    ","    Args:","        model: model to validate","        dataloader: validation data loader","        device: device to run on","        num_classes: number of classes","    ","    Returns:","        mean_iou: mean IoU across all samples","        mean_loss: mean cross-entropy loss","    \"\"\"","    model.eval()","    all_ious = []","    all_losses = []","    criterion = nn.CrossEntropyLoss(ignore_index=255)","    ","    with torch.no_grad():","        for images, targets in tqdm(dataloader, desc='Validation'):","            for img, target in zip(images, targets):","                img = img.unsqueeze(0).to(device)","                target = target.to(device)","                ","                # Resize to fixed size","                h, w = target.shape","                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)","                ","                # Forward pass","                output = model(img)","                ","                # Calculate loss at this resolution","                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), ","                                               size=output.shape[-2:], ","                                               mode='nearest').squeeze().long()","                loss = criterion(output, target_resized.unsqueeze(0))","                all_losses.append(loss.item())","                ","                # Resize back to original size for IoU calculation","                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)","                ","                # Get predictions","                pred = output.squeeze(0).argmax(0)","                ","                # Calculate IoU","                iou = calculate_miou(pred, target, num_classes)","                all_ious.append(iou)","    ","    mean_iou = np.mean(all_ious)","    mean_loss = np.mean(all_losses)","    return mean_iou, mean_loss","","","def plot_training_curves(history, save_path):","    \"\"\"Plot and save training curves\"\"\"","    fig, axes = plt.subplots(1, 2, figsize=(12, 4))","    ","    # Loss plot - now includes both train and validation loss","    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)","    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')","    axes[0].set_xlabel('Epoch')","    axes[0].set_ylabel('Loss')","    axes[0].set_title('Training and Validation Loss')","    axes[0].legend()","    axes[0].grid(True)","    ","    # mIoU plot","    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)","    axes[1].set_xlabel('Epoch')","    axes[1].set_ylabel('mIoU')","    axes[1].set_title('Validation mIoU')","    axes[1].legend()","    axes[1].grid(True)","    ","    plt.tight_layout()","    plt.savefig(save_path, dpi=150)","    plt.close()","    print(f\"Training curves saved to {save_path}\")","","","def main(args):","    \"\"\"Main training function\"\"\"","    ","    print(\"=\" * 70)","    print(\"Training Compact Segmentation Model\")","    print(\"=\" * 70)","    print(f\"Model: {args.model}\")","    print(f\"KD Mode: {args.kd_mode}\")","    print(f\"Epochs: {args.epochs}\")","    print(f\"Batch Size: {args.batch_size}\")","    print(f\"Learning Rate: {args.lr}\")","    print(\"=\" * 70)","    ","    # Set device","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","    print(f\"\\nUsing device: {device}\")","    ","    # Create save directory","    save_dir = Path(args.save_dir)","    save_dir.mkdir(parents=True, exist_ok=True)","    ","    # Load student model","    print(\"\\nLoading student model...\")","    if args.model == 'ultracompact':","        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)","    elif args.model == 'standard':","        model = StandardSegmentationModel(num_classes=21, pretrained=True)","    else:","        raise ValueError(f\"Unknown model: {args.model}\")","    ","    model = model.to(device)","    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)","    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")","    ","    # Load teacher model if using KD","    teacher_model = None","    if args.kd_mode != 'none':","        print(\"\\nLoading teacher model (FCN-ResNet50)...\")","        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1","        teacher_model = fcn_resnet50(weights=weights)","        teacher_model = teacher_model.to(device)","        teacher_model.eval()","        for param in teacher_model.parameters():","            param.requires_grad = False","        print(\"Teacher model loaded and frozen\")","    ","    # Prepare datasets","    print(\"\\nPreparing datasets...\")","    ","    # Find dataset path (Google Colab compatible)","    # VOCSegmentation expects: root/VOCdevkit/VOC2012/","    dataset_roots = []","    ","    # Check for Google Colab structure (both with and without VOCdevkit wrapper)","    colab_train_path = Path('/content/data/VOC2012_train_val')","    if colab_train_path.exists():","        # Check if it has VOCdevkit structure","        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(colab_train_path))","        # Check if files are directly in the folder (no VOCdevkit wrapper)","        elif (colab_train_path / 'JPEGImages').exists():","            # Create a symlink or use parent structure","            dataset_roots.append(str(colab_train_path.parent))","    ","    # Check for local structure","    local_data_path = Path('./data')","    if local_data_path.exists():","        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(local_data_path))","        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():","            dataset_roots.append(str(local_data_path))","    ","    # Check standard paths","    dataset_roots.extend([","        '/content/data',  # Google Colab default","        './data',  # Local path","        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),","    ])","    ","    # Check dataset_path.txt","    dataset_path_file = Path('dataset_path.txt')","    if dataset_path_file.exists():","        with open(dataset_path_file, 'r') as f:","            saved_path = Path(f.read().strip())","            dataset_roots.insert(0, str(saved_path.parent.parent))","    ","    dataset_root = None","    for root in dataset_roots:","        try:","            # Try to create a temporary symlink structure if needed","            root_path = Path(root)","            ","            # Check if we need to create VOCdevkit/VOC2012 structure","            voc_path = root_path / 'VOC2012_train_val'","            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():","                # Files are directly in VOC2012_train_val, create temporary structure","                vocdevkit_path = voc_path / 'VOCdevkit'","                voc2012_path = vocdevkit_path / 'VOC2012'","                if not voc2012_path.exists():","                    print(f\"Creating VOCdevkit structure in {voc_path}\")","                    vocdevkit_path.mkdir(exist_ok=True)","                    voc2012_path.symlink_to(voc_path, target_is_directory=True)","            ","            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)","            dataset_root = root","            print(f\"✓ Successfully loaded dataset from: {root}\")","            break","        except Exception as e:","            continue","    ","    if dataset_root is None:","        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")","    ","    print(f\"Dataset found at: {dataset_root}\")","    ","    # Create datasets","    train_transform = VOCNormalize(augment=True)","    val_transform = VOCNormalize(augment=False)","    ","    train_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='train',","        download=False,","        transforms=train_transform","    )","    ","    val_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='val',","        download=False,","        transforms=val_transform","    )","    ","    print(f\"Training set size: {len(train_dataset)}\")","    print(f\"Validation set size: {len(val_dataset)}\")","    ","    # Create dataloaders","    train_loader = DataLoader(","        train_dataset,","        batch_size=args.batch_size,","        shuffle=True,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    val_loader = DataLoader(","        val_dataset,","        batch_size=4,","        shuffle=False,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    # Setup optimizer and scheduler","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)","    ","    # Training history","    history = {","        'train_loss': [],","        'train_ce_loss': [],","        'train_kd_loss': [],","        'val_loss': [],","        'val_miou': [],","        'epoch_times': []","    }","    ","    # Save hyperparameters","    hyperparameters = {","        'model': args.model,","        'kd_mode': args.kd_mode,","        'epochs': args.epochs,","        'batch_size': args.batch_size,","        'learning_rate': args.lr,","        'weight_decay': args.weight_decay,","        'temperature': args.temperature,","        'alpha': args.alpha,","        'beta': args.beta,","        'optimizer': 'Adam',","        'scheduler': 'CosineAnnealingLR',","        'num_parameters': sum(p.numel() for p in model.parameters()),","        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)","    }","    ","    best_miou = 0.0","    training_start_time = time.time()","    ","    # Training loop","    print(\"\\nStarting training...\")","    print(\"=\" * 70)","    print(\"\\nHyperparameters:\")","    for key, value in hyperparameters.items():","        print(f\"  {key}: {value}\")","    ","    for epoch in range(args.epochs):","        epoch_start_time = time.time()","        ","        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")","        print(\"-\" * 70)","        ","        # Train","        train_loss, train_ce_loss, train_kd_loss = train_epoch(","            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args","        )","        ","        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")","        ","        # Validate","        val_miou = validate(model, val_loader, device)","        print(f\"Validation mIoU: {val_miou:.4f}\")","        ","        # Record epoch time","        epoch_time = time.time() - epoch_start_time","        history['epoch_times'].append(epoch_time)","        print(f\"Epoch time: {epoch_time:.2f} seconds\")","        ","        # Update history","        history['train_loss'].append(train_loss)","        history['train_ce_loss'].append(train_ce_loss)","        history['train_kd_loss'].append(train_kd_loss)","        history['val_miou'].append(val_miou)","        ","        # Save best model","        if val_miou > best_miou:","            best_miou = val_miou","            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history","            }, save_path)","            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")","        ","        # Update scheduler","        scheduler.step()","        ","        # Save checkpoint every 10 epochs","        if (epoch + 1) % 10 == 0:","            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history,","                'hyperparameters': hyperparameters","            }, checkpoint_path)","            print(f\"Checkpoint saved: {checkpoint_path}\")","    ","    # Calculate total training time","    total_training_time = time.time() - training_start_time","    avg_epoch_time = np.mean(history['epoch_times'])","    ","    # Plot training curves","    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"","    plot_training_curves(history, plot_path)","    ","    # Save final training report","    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"","    with open(report_path, 'w') as f:","        f.write(\"=\" * 70 + \"\\n\")","        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")","        f.write(\"=\" * 70 + \"\\n\\n\")","        ","        f.write(\"HYPERPARAMETERS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        for key, value in hyperparameters.items():","            f.write(f\"  {key:25s}: {value}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"TRAINING STATISTICS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")","        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")","        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")","        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")","        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"FILES GENERATED:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")","        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")","        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")","        f.write(\"=\" * 70 + \"\\n\")","    ","    print(\"\\n\" + \"=\" * 70)","    print(\"Training completed!\")","    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")","    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")","    print(f\"Best validation mIoU: {best_miou:.4f}\")","    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")","    print(f\"Training report saved to: {report_path}\")","    ","    # Calculate final score","    score = 4 * best_miou / (1 + num_params / 1e6)","    print(f\"\\nFinal Score: {score:.3f}\")","    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")","    print(\"=\" * 70)","","","if __name__ == '__main__':","    parser = argparse.ArgumentParser(description='Train compact segmentation model')","    ","    # Model arguments","    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],","                        help='Model type to train')","    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],","                        help='Knowledge distillation mode')","    ","    # Training arguments","    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')","    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')","    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')","    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')","    ","    # KD arguments","    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')","    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')","    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')","    ","    # Other arguments","    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')","    ","    args = parser.parse_args()","    ","    main(args)",""],"recentEdit":[[582,583,["        val_miou, val_loss = validate(model, val_loader, device)"]],[583,584,["        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")"]],[594,594,["        history['val_loss'].append(val_loss)"]],[652,652,["        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")"]]],"documentBeforeEdits":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean_iou: mean IoU across all samples\r\n        mean_loss: mean cross-entropy loss\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    all_losses = []\r\n    criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot - now includes both train and validation loss\r\n    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training and Validation Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path, dpi=150)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","recentEdits":[[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]]],"lastSelectionInAfterEdit":[18491,18491]},{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"plaintext","documentLinesBeforeEdit":["# ELEC 475 Lab 3 - Training Commands","# All commands should be run from the Lab 3 directory","","# ============================================================================","# ULTRA-COMPACT MODEL (475K parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# STANDARD MODEL (3.4M parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# TRAINING OUTPUT FILES","# ============================================================================","# For each training run, the following files will be generated in ./checkpoints/:","#   - best_model_{model}_{kd_mode}.pth           (best model weights)","#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)","#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)","#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)","","# ============================================================================","# OPTIONAL: HYPERPARAMETER TUNING","# ============================================================================","# If you want to experiment with different hyperparameters:","","# Adjust learning rate","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005","","# Adjust KD temperature (higher = softer targets)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0","","# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3","","# Adjust batch size (if memory issues)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4","","# ============================================================================","# NOTES","# ============================================================================","# - Training on CPU will be slow. Consider using Google Colab with GPU.","# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU","# - Monitor validation mIoU - training should converge after 30-40 epochs","# - Best model is saved automatically when validation mIoU improves",""],"recentEdit":[[18,19,["# STANDARD MODEL (1parameters)"]]],"documentBeforeEdits":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","recentEdits":[[[867,872,"1"]]],"lastSelectionInAfterEdit":[868,868]}],"activeDocumentIdx":1,"recording":[{"documentType":"workspaceRecording@1.0","kind":"header","repoRootUri":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","time":1762624150653,"uuid":"97a2543a-1f09-451b-9866-1beef366a1c5"},{"kind":"documentEncountered","id":4,"relativePath":"model_standard.py","time":1762545107282},{"kind":"setContent","id":4,"v":1,"content":"\"\"\"\r\nStandard MobileNetV3-Small Segmentation Model\r\nELEC 475 Lab 3 - Option 1\r\n\r\nParameters: ~3.41M\r\nExpected Score: 0.40-0.50\r\n\r\nReferences:\r\n- MobileNetV3: Searching for MobileNetV3 (Howard et al., 2019)\r\n- DeepLabV3+: Encoder-Decoder with Atrous Separable Convolution (Chen et al., 2018)\r\n- ASPP: Rethinking Atrous Convolution for Semantic Image Segmentation (Chen et al., 2017)\r\n\"\"\"\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\r\n\r\n\r\nclass ASPP(nn.Module):\r\n    \"\"\"\r\n    Atrous Spatial Pyramid Pooling (ASPP) module\r\n    Captures multi-scale context using parallel atrous convolutions\r\n    \"\"\"\r\n    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\r\n        super(ASPP, self).__init__()\r\n        \r\n        self.branches = nn.ModuleList()\r\n        \r\n        # 1x1 convolution branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Atrous convolution branches\r\n        for rate in rates:\r\n            self.branches.append(nn.Sequential(\r\n                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\r\n                nn.BatchNorm2d(out_channels),\r\n                nn.ReLU(inplace=True)\r\n            ))\r\n        \r\n        # Global average pooling branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.AdaptiveAvgPool2d(1),\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Combine all branches\r\n        num_branches = len(rates) + 2  # 1x1 + atrous branches + global pooling\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(out_channels * num_branches, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Dropout(0.5)\r\n        )\r\n    \r\n    def forward(self, x):\r\n        h, w = x.shape[2:]\r\n        branch_outputs = []\r\n        \r\n        for i, branch in enumerate(self.branches):\r\n            out = branch(x)\r\n            # Upsample global pooling branch to match spatial dimensions\r\n            if i == len(self.branches) - 1:\r\n                out = F.interpolate(out, size=(h, w), mode='bilinear', align_corners=False)\r\n            branch_outputs.append(out)\r\n        \r\n        # Concatenate all branches\r\n        x = torch.cat(branch_outputs, dim=1)\r\n        x = self.project(x)\r\n        \r\n        return x\r\n\r\n\r\nclass StandardSegmentationModel(nn.Module):\r\n    \"\"\"\r\n    Standard compact segmentation model using MobileNetV3-Small backbone\r\n    \r\n    Architecture:\r\n    - Full MobileNetV3-Small backbone (pretrained)\r\n    - 3-rate ASPP module (rates: 6, 12, 18)\r\n    - Two-stage decoder with skip connections\r\n    - Parameters: ~3.41M\r\n    \"\"\"\r\n    def __init__(self, num_classes=21, pretrained=True):\r\n        super(StandardSegmentationModel, self).__init__()\r\n        \r\n        # Load pretrained MobileNetV3-Small\r\n        if pretrained:\r\n            weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1\r\n            backbone = mobilenet_v3_small(weights=weights)\r\n        else:\r\n            backbone = mobilenet_v3_small(weights=None)\r\n        \r\n        # Extract feature layers from MobileNetV3-Small\r\n        self.features = backbone.features\r\n        \r\n        # Feature dimensions at tap points:\r\n        # Stage 1 (low):  16 channels, stride 4\r\n        # Stage 3 (mid):  24 channels, stride 8\r\n        # Stage 12 (high): 576 channels, stride 16\r\n        \r\n        # ASPP module for multi-scale context\r\n        self.aspp = ASPP(in_channels=576, out_channels=128, rates=[6, 12, 18])\r\n        \r\n        # Feature projection layers (keep dimensions)\r\n        self.mid_conv = nn.Sequential(\r\n            nn.Conv2d(24, 24, 1, bias=False),\r\n            nn.BatchNorm2d(24),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.low_conv = nn.Sequential(\r\n            nn.Conv2d(16, 16, 1, bias=False),\r\n            nn.BatchNorm2d(16),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Decoder with skip connections\r\n        self.decoder_conv1 = nn.Sequential(\r\n            nn.Conv2d(128 + 24, 128, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(128),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.decoder_conv2 = nn.Sequential(\r\n            nn.Conv2d(128 + 16, 64, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(64),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Classifier head\r\n        self.classifier = nn.Sequential(\r\n            nn.Dropout(0.5),\r\n            nn.Conv2d(64, num_classes, 1)\r\n        )\r\n        \r\n        self._init_weights()\r\n    \r\n    def _init_weights(self):\r\n        \"\"\"Initialize weights for new layers\"\"\"\r\n        for m in [self.aspp, self.mid_conv, self.low_conv, self.decoder_conv1, self.decoder_conv2, self.classifier]:\r\n            for module in m.modules():\r\n                if isinstance(module, nn.Conv2d):\r\n                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\r\n                    if module.bias is not None:\r\n                        nn.init.constant_(module.bias, 0)\r\n                elif isinstance(module, nn.BatchNorm2d):\r\n                    nn.init.constant_(module.weight, 1)\r\n                    nn.init.constant_(module.bias, 0)\r\n    \r\n    def forward(self, x, return_features=False):\r\n        \"\"\"\r\n        Forward pass\r\n        \r\n        Args:\r\n            x: input image tensor (B, 3, H, W)\r\n            return_features: if True, return intermediate features for knowledge distillation\r\n            \r\n        Returns:\r\n            output: segmentation logits (B, num_classes, H, W)\r\n            features (optional): dict of intermediate features for KD\r\n        \"\"\"\r\n        input_shape = x.shape[-2:]\r\n        \r\n        # Extract multi-scale features from backbone\r\n        low_feat = None   # stride 4\r\n        mid_feat = None   # stride 8\r\n        high_feat = None  # stride 16\r\n        \r\n        # Forward through MobileNetV3 stages\r\n        for i, layer in enumerate(self.features):\r\n            x = layer(x)\r\n            \r\n            # Capture features at specific stages\r\n            if i == 1:  # After first inverted residual block (stride 4)\r\n                low_feat = x\r\n            elif i == 3:  # After third inverted residual block (stride 8)\r\n                mid_feat = x\r\n        \r\n        # Final high-level features (stride 16)\r\n        high_feat = x\r\n        \r\n        # Process features\r\n        mid_feat = self.mid_conv(mid_feat)\r\n        low_feat = self.low_conv(low_feat)\r\n        \r\n        # ASPP for multi-scale context\r\n        x = self.aspp(high_feat)\r\n        \r\n        # Decoder with skip connections\r\n        # Upsample and concatenate with mid-level features\r\n        x = F.interpolate(x, size=mid_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, mid_feat], dim=1)\r\n        x = self.decoder_conv1(x)\r\n        \r\n        # Upsample and concatenate with low-level features\r\n        x = F.interpolate(x, size=low_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, low_feat], dim=1)\r\n        x = self.decoder_conv2(x)\r\n        \r\n        # Final upsampling to input resolution\r\n        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\r\n        \r\n        # Classifier\r\n        output = self.classifier(x)\r\n        \r\n        if return_features:\r\n            features = {\r\n                'low': low_feat,\r\n                'mid': mid_feat,\r\n                'high': high_feat\r\n            }\r\n            return output, features\r\n        \r\n        return output\r\n\r\n\r\ndef count_parameters(model):\r\n    \"\"\"Count the number of trainable parameters\"\"\"\r\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(\"=\" * 70)\r\n    print(\"Standard MobileNetV3-Small Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Create model\r\n    model = StandardSegmentationModel(num_classes=21, pretrained=False)\r\n    \r\n    # Count parameters\r\n    params = count_parameters(model)\r\n    print(f\"\\nTotal parameters: {params:,} ({params/1e6:.2f}M)\")\r\n    \r\n    # Test forward pass\r\n    print(\"\\nTesting forward pass...\")\r\n    dummy_input = torch.randn(2, 3, 224, 224)\r\n    \r\n    with torch.no_grad():\r\n        output = model(dummy_input)\r\n        print(f\"  Output shape: {output.shape}\")\r\n        \r\n        # Test with feature extraction\r\n        output, features = model(dummy_input, return_features=True)\r\n        print(f\"\\n  Features for knowledge distillation:\")\r\n        for name, feat in features.items():\r\n            print(f\"    {name}: {feat.shape}\")\r\n    \r\n    # Score estimation\r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Score Estimation: Score = 4 × mIoU / (1 + params_M)\")\r\n    print(\"=\" * 70)\r\n    \r\n    for miou in [0.40, 0.45, 0.50, 0.55]:\r\n        score = 4 * miou / (1 + params/1e6)\r\n        print(f\"  mIoU={miou:.2f} → Score={score:.3f}\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n","time":1762545107282},{"kind":"opened","id":4,"time":1762545107282},{"kind":"documentEncountered","id":5,"relativePath":"train.py","time":1762545107284},{"kind":"setContent","id":5,"v":1,"content":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean IoU\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Resize back to original size\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    return mean_iou\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot\r\n    axes[0].plot(history['train_loss'], label='Train Loss')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU')\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","time":1762622282025},{"kind":"opened","id":5,"time":1762622282025},{"kind":"documentEncountered","id":17,"relativePath":"train.txt","time":1762622441478},{"kind":"setContent","id":17,"v":1,"content":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","time":1762622441593},{"kind":"opened","id":17,"time":1762622441593},{"kind":"changed","id":4,"v":2,"edit":[[50,50," (Optimized)"],[94,94,"1."],[95,95,"6M (reduced from 3"],[99,99,")"],[119,120,"8"],[122,123,"1"],[124,125,"2"]],"time":1762623638975},{"kind":"changed","id":4,"v":3,"edit":[[2834,2834," (Optimized)"],[2919,2920,"Lightweight 2"],[2951,2955,""],[2956,2956," - reduced from 3 rates"],[2964,2972,"Simplifi"],[2973,2973,"d"],[2987,2991,"reduced"],[2993,2994,"ha"],[2997,3002,"l"],[3024,3024,"1."],[3025,3025,"6M (reduced from 3"],[3029,3029,")"],[3767,3767,"Lightweight "],[3779,3782,"(2"],[3783,3786,"ra"],[3787,3787,"es "],[3788,3789,"n"],[3790,3790,"tead of 3, redu"],[3791,3793,""],[3794,3794,"d output"],[3796,3797,"han"],[3798,3799,""],[3800,3802,"ls)"],[3859,3862,"64"],[3876,3880,""],[3931,3933,"r"],[3934,3936,""],[3937,3939,"uc"],[3940,3940,"d cha"],[3941,3944,""],[3945,3945,"el"],[4015,4017,"16"],[4063,4065,"16"],[4191,4193,"8"],[4239,4241,"8"],[4310,4310," Simplified"],[4311,4312,"d"],[4324,4328,"reduced"],[4330,4331,"ha"],[4334,4339,"l"],[4409,4412,"64"],[4415,4417,"16"],[4419,4422,"64"],[4479,4482,"64"],[4609,4612,"64"],[4615,4617,"8"],[4619,4621,"32"],[4678,4680,"32"],[4861,4863,"32"]],"time":1762623641194},{"kind":"changed","id":5,"v":2,"edit":[[10251,10251,"_iou:"],[10252,10252,"mean "],[10255,10255," across all samples"],[10261,10261,"    mean_loss: mean cross-entropy loss\r\n    "],[10307,10307,"all_losses = []"],[10313,10313,"criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    "],[10403,10403,"\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss"]],"time":1762623765832},{"kind":"changed","id":5,"v":3,"edit":[[11970,12883,""]],"time":1762623780348},{"kind":"changed","id":5,"v":4,"edit":[[12136,12136," - now includes both train and validation loss"],[12188,12188,"in Loss', l"],[12190,12190,"ewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val"],[12196,12196,", linewidth=2, linestyle='--'"],[12295,12295," and Validation"],[12434,12434,", linewidth=2"],[12645,12645,", dpi=150"]],"time":1762623791607},{"kind":"changed","id":5,"v":5,"edit":[[19126,19126,"loss': [],\r\n        'val_"]],"time":1762623803059},{"kind":"changed","id":5,"v":6,"edit":[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],"time":1762623816438},{"kind":"changed","id":5,"v":7,"edit":[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]],"time":1762623835010},{"kind":"selectionChanged","id":17,"selection":[[883,883]],"time":1762624017851},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624140244},{"kind":"selectionChanged","id":17,"selection":[[869,870]],"time":1762624140330},{"kind":"selectionChanged","id":17,"selection":[[868,870]],"time":1762624140356},{"kind":"selectionChanged","id":17,"selection":[[867,870]],"time":1762624140378},{"kind":"changed","id":17,"v":7,"edit":[[867,870,"1"]],"time":1762624143421},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624143443},{"kind":"changed","id":17,"v":9,"edit":[[868,869,""]],"time":1762624143557},{"kind":"selectionChanged","id":17,"selection":[[849,849]],"time":1762624144669},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624146159},{"kind":"selectionChanged","id":17,"selection":[[869,869]],"time":1762624146967},{"kind":"changed","id":17,"v":13,"edit":[[868,869,""]],"time":1762624150477},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624150514}]},"resultEdit":"   16  16 \n   17  17 # ============================================================================\n-  18     # STANDARD MODEL (1arameters)\n+      18 # STANDARD MODEL (1.3M parameters)\n   19  19 # ============================================================================\n   20  20 \n   21  21 # Baseline (No Knowledge Distillation)","isCachedResult":true,"prompt":"System\n------\nYour role as an AI assistant is to help developers complete their code tasks by assisting in editing specific sections of code marked by the <|code_to_edit|> and <|/code_to_edit|> tags, while adhering to Microsoft's content policies and avoiding the creation of content that violates copyrights.\n\nYou have access to the following information to help you make informed suggestions:\n\n- recently_viewed_code_snippets: These are code snippets that the developer has recently looked at, which might provide context or examples relevant to the current task. They are listed from oldest to newest, with line numbers in the form #| to help you understand the edit diff history. It's possible these are entirely irrelevant to the developer's change.\n- current_file_content: The content of the file the developer is currently working on, providing the broader context of the code. Line numbers in the form #| are included to help you understand the edit diff history.\n- edit_diff_history: A record of changes made to the code, helping you understand the evolution of the code and the developer's intentions. These changes are listed from oldest to latest. It's possible a lot of old edit diff history is entirely irrelevant to the developer's change.\n- area_around_code_to_edit: The context showing the code surrounding the section to be edited.\n- cursor position marked as <|cursor|>: Indicates where the developer's cursor is currently located, which can be crucial for understanding what part of the code they are focusing on.\n\nYour task is to predict and complete the changes the developer would have made next in the <|code_to_edit|> section. The developer may have stopped in the middle of typing. Your goal is to keep the developer on the path that you think they're following. Some examples include further implementing a class, method, or variable, or improving the quality of the code. Make sure the developer doesn't get distracted and ensure your suggestion is relevant. Consider what changes need to be made next, if any. If you think changes should be made, ask yourself if this is truly what needs to happen. If you are confident about it, then proceed with the changes.\n\n# Steps\n\n1. **Review Context**: Analyze the context from the resources provided, such as recently viewed snippets, edit history, surrounding code, and cursor location.\n2. **Evaluate Current Code**: Determine if the current code within the tags requires any corrections or enhancements.\n3. **Suggest Edits**: If changes are required, ensure they align with the developer's patterns and improve code quality.\n4. **Maintain Consistency**: Ensure indentation and formatting follow the existing code style.\n\n# Output Format\n\n- Provide only the revised code within the tags. If no changes are necessary, simply return the original code from within the <|code_to_edit|> and <|/code_to_edit|> tags.\n- There are line numbers in the form #| in the code displayed to you above, but these are just for your reference. Please do not include the numbers of the form #| in your response.\n- Ensure that you do not output duplicate code that exists outside of these tags. The output should be the revised code that was between these tags and should not include the <|code_to_edit|> or <|/code_to_edit|> tags.\n\n```\n// Your revised code goes here\n```\n\n# Notes\n\n- Apologize with \"Sorry, I can't assist with that.\" for requests that may breach Microsoft content guidelines.\n- Avoid undoing or reverting the developer's last change unless there are obvious typos or errors.\n- Don't include the line numbers of the form #| in your response.\n==================\nUser\n------\n```\n<|recently_viewed_code_snippets|>\n<|recently_viewed_code_snippet|>\ncode_snippet_file_path: /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py (truncated)\n\"\"\"\nTraining Script for Compact Segmentation Models with Knowledge Distillation\nELEC 475 Lab 3\n\nSupports three training modes:\n1. No Knowledge Distillation (baseline)\n2. Response-based Knowledge Distillation\n3. Feature-based Knowledge Distillation\n\nReferences:\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\n\"\"\"\n\nimport argparse\nimport os\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import VOCSegmentation\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\nfrom tqdm import tqdm\n\n# Import custom models\nfrom model_ultracompact import UltraCompactSegmentationModel\nfrom model_standard import StandardSegmentationModel\n\n\nclass VOCNormalize:\n    \"\"\"Custom normalization for VOC dataset\"\"\"\n    def __init__(self, augment=True):\n        self.augment = augment\n        self.normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    \n    def __call__(self, img, target):\n        # Convert PIL to tensor\n        img = transforms.ToTensor()(img)\n        \n        # Apply augmentation if training\n        if self.augment and np.random.rand() > 0.5:\n            # Random horizontal flip\n            img = transforms.functional.hflip(img)\n            target = transforms.functional.hflip(target)\n        \n        # Normalize image\n        img = self.normalize(img)\n        \n        # Convert target to tensor\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n        \n        return img, target\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\n    images, targets = zip(*batch)\n    return list(images), list(targets)\n\n\ndef calculate_miou(pred, target, num_classes=21):\n    \"\"\"\n    Calculate mean Intersection over Union (mIoU)\n    \n    Args:\n        pred: predicted segmentation mask (H, W)\n        target: ground truth segmentation mask (H, W)\n        num_classes: number of classes\n    \n    Returns:\n        mIoU value\n    \"\"\"\n    pred = pred.cpu().numpy()\n    target = target.cpu().numpy()\n    \n    ious = []\n    for cls in range(num_classes):\n        pred_mask = (pred == cls)\n        target_mask = (target == cls)\n        \n        intersection = np.logical_and(pred_mask, target_mask).sum()\n        union = np.logical_or(pred_mask, target_mask).sum()\n        \n        if union == 0:\n            continue\n        \n        iou = intersection / union\n        ious.append(iou)\n    \n    return np.mean(ious) if len(ious) > 0 else 0.0\n\n\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\n    \"\"\"\n    Response-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        teacher_logits: teacher model output logits (B, C, H, W)\n        targets: ground truth labels (B, H, W)\n        temperature: softmax temperature for distillation\n        alpha: weight for cross-entropy loss\n        beta: weight for distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Distillation loss (KL divergence)\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\n    \n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * kd_loss\n    \n    return total_loss, ce_loss.item(), kd_loss.item()\n\n\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\n    \"\"\"\n    Feature-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        student_features: dict of student intermediate features\n        teacher_features: dict of teacher intermediate features\n        targets: ground truth labels (B, H, W)\n        alpha: weight for cross-entropy loss\n        beta: weight for feature distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Feature matching loss (cosine similarity)\n    feat_loss = 0\n    num_levels = 0\n    \n    for level in ['low', 'mid', 'high']:\n        if level in student_features and level in teacher_features:\n            s_feat = student_features[level]\n            t_feat = teacher_features[level]\n            \n            # Resize if needed (teacher features might be different size)\n            if s_feat.shape != t_feat.shape:\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n                # Match channels if needed\n                if s_feat.shape[1] != t_feat.shape[1]:\n                    # Project teacher features to student channel size\n                    continue  # Skip if channel mismatch (or add projection layer)\n            \n            # Cosine similarity loss (1 - cosine_similarity)\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\n            \n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\n            feat_loss += (1 - cosine_sim)\n            num_levels += 1\n    \n    if num_levels > 0:\n        feat_loss = feat_loss / num_levels\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * feat_loss\n    \n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\n\n\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\n    \"\"\"\n    Train for one epoch\n    \n    Args:\n        model: student model\n        teacher_model: teacher model (None if kd_mode='none')\n        dataloader: training data loader\n        optimizer: optimizer\n        device: device to run on\n        kd_mode: 'none', 'response', or 'feature'\n        args: training arguments\n    \n    Returns:\n        average loss, average ce_loss, average kd_loss\n    \"\"\"\n    model.train()\n    if teacher_model is not None:\n        teacher_model.eval()\n    \n    total_loss = 0\n    total_ce_loss = 0\n    total_kd_loss = 0\n    num_batches = 0\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for images, targets in pbar:\n        # Move to device and resize to fixed size for batching\n        batch_imgs = []\n        batch_targets = []\n        \n        for img, target in zip(images, targets):\n            img = img.to(device)\n            target = target.to(device)\n            \n            # Resize to fixed size\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\n            \n            batch_imgs.append(img)\n            batch_targets.append(target)\n        \n        batch_imgs = torch.stack(batch_imgs)\n<|/recently_viewed_code_snippet|>\n<|/recently_viewed_code_snippets|>\n\n<|current_file_content|>\ncurrent_file_path: train.txt\n# ELEC 475 Lab 3 - Training Commands\n# All commands should be run from the Lab 3 directory\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n# STANDARD MODEL (1<|cursor|>parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\n\n# ============================================================================\n# OPTIONAL: HYPERPARAMETER TUNING\n# ============================================================================\n# If you want to experiment with different hyperparameters:\n\n# Adjust learning rate\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\n\n# Adjust KD temperature (higher = softer targets)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\n\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\n\n# Adjust batch size (if memory issues)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\n\n# ============================================================================\n# NOTES\n# ============================================================================\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\n# - Monitor validation mIoU - training should converge after 30-40 epochs\n# - Best model is saved automatically when validation mIoU improves\n\n<|/current_file_content|>\n\n<|edit_diff_history|>\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -342,23 +342,0 @@\n-            for img, target in zip(images, targets):\n-                img = img.unsqueeze(0).to(device)\n-                target = target.to(device)\n-                \n-                # Resize to fixed size\n-                h, w = target.shape\n-                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\n-                \n-                # Forward pass\n-                output = model(img)\n-                \n-                # Resize back to original size\n-                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\n-                \n-                # Get predictions\n-                pred = output.squeeze(0).argmax(0)\n-                \n-                # Calculate IoU\n-                iou = calculate_miou(pred, target, num_classes)\n-                all_ious.append(iou)\n-    \n-    mean_iou = np.mean(all_ious)\n-    return mean_iou\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -348,1 +348,1 @@\n-    # Loss plot\n+    # Loss plot - now includes both train and validation loss\n@@ -349,1 +349,2 @@\n-    axes[0].plot(history['train_loss'], label='Train Loss')\n+    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n+    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\n@@ -352,1 +352,1 @@\n-    axes[0].set_title('Training Loss')\n+    axes[0].set_title('Training and Validation Loss')\n@@ -357,1 +357,1 @@\n-    axes[1].plot(history['val_miou'], label='Validation mIoU')\n+    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\n@@ -365,1 +365,1 @@\n-    plt.savefig(save_path)\n+    plt.savefig(save_path, dpi=150)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -535,1 +535,2 @@\n-        'val_miou': [],\n+        'val_loss': [],\n+        'val_miou': [],\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -581,1 +581,1 @@\n-        val_miou = validate(model, val_loader, device)\n+        val_miou, val_loss = validate(model, val_loader, device)\n@@ -582,1 +582,1 @@\n-        print(f\"Validation mIoU: {val_miou:.4f}\")\n+        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")\n@@ -592,1 +592,2 @@\n-        history['train_kd_loss'].append(train_kd_loss)\n+        history['train_kd_loss'].append(train_kd_loss)\n+        history['val_loss'].append(val_loss)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -652,1 +652,2 @@\n-        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n@@ -17,1 +17,1 @@\n-# STANDARD MODEL (3.4M parameters)\n+# STANDARD MODEL (1parameters)\n\n<|/edit_diff_history|>\n\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n# STANDARD MODEL (1<|cursor|>parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n```\n\nThe developer was working on a section of code within the tags `code_to_edit` in the file located at `train.txt`. Using the given `recently_viewed_code_snippets`, `current_file_content`, `edit_diff_history`, `area_around_code_to_edit`, and the cursor position marked as `<|cursor|>`, please continue the developer's work. Update the `code_to_edit` section by predicting and completing the changes they would have made next. Provide the revised code that was between the `<|code_to_edit|>` and `<|/code_to_edit|>` tags with the following format, but do not include the tags themselves.\n```\n// Your revised code goes here\n```\n==================","error":"undefined","response":"```\n\n# ============================================================================\n# STANDARD MODEL (1.3M parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n```","responseResults":"- replaceRange:\n   start: 867\n   endExclusive: 877\n  newText: 1.3M parameters\n","logs":["```\n{\n\t\"isAsyncCompletions\": true,\n\t\"isRevisedCacheStrategy\": true,\n\t\"isCacheTracksRejections\": true,\n\t\"isRecentlyShownCacheEnabled\": false,\n\t\"debounceUseCoreRequestTime\": false\n}\n```\n","First edit latency: 24 ms\n"]}
{"requestId":6,"time":1762624151779,"filePath":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","version":6,"statelessNextEditProviderId":"XtabProvider","nextEditRequest":{"id":"9fd4b880-9e9b-4903-b0b5-081b2290055a","documents":[{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.py","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"python","documentLinesBeforeEdit":["\"\"\"","Training Script for Compact Segmentation Models with Knowledge Distillation","ELEC 475 Lab 3","","Supports three training modes:","1. No Knowledge Distillation (baseline)","2. Response-based Knowledge Distillation","3. Feature-based Knowledge Distillation","","References:","- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)","- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)","\"\"\"","","import argparse","import os","import time","from pathlib import Path","import numpy as np","import matplotlib.pyplot as plt","","import torch","import torch.nn as nn","import torch.nn.functional as F","from torch.utils.data import DataLoader","from torchvision import transforms","from torchvision.datasets import VOCSegmentation","from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights","from tqdm import tqdm","","# Import custom models","from model_ultracompact import UltraCompactSegmentationModel","from model_standard import StandardSegmentationModel","","","class VOCNormalize:","    \"\"\"Custom normalization for VOC dataset\"\"\"","    def __init__(self, augment=True):","        self.augment = augment","        self.normalize = transforms.Normalize(","            mean=[0.485, 0.456, 0.406],","            std=[0.229, 0.224, 0.225]","        )","    ","    def __call__(self, img, target):","        # Convert PIL to tensor","        img = transforms.ToTensor()(img)","        ","        # Apply augmentation if training","        if self.augment and np.random.rand() > 0.5:","            # Random horizontal flip","            img = transforms.functional.hflip(img)","            target = transforms.functional.hflip(target)","        ","        # Normalize image","        img = self.normalize(img)","        ","        # Convert target to tensor","        target = torch.as_tensor(np.array(target), dtype=torch.int64)","        ","        return img, target","","","def collate_fn(batch):","    \"\"\"Custom collate function to handle variable-sized images\"\"\"","    images, targets = zip(*batch)","    return list(images), list(targets)","","","def calculate_miou(pred, target, num_classes=21):","    \"\"\"","    Calculate mean Intersection over Union (mIoU)","    ","    Args:","        pred: predicted segmentation mask (H, W)","        target: ground truth segmentation mask (H, W)","        num_classes: number of classes","    ","    Returns:","        mIoU value","    \"\"\"","    pred = pred.cpu().numpy()","    target = target.cpu().numpy()","    ","    ious = []","    for cls in range(num_classes):","        pred_mask = (pred == cls)","        target_mask = (target == cls)","        ","        intersection = np.logical_and(pred_mask, target_mask).sum()","        union = np.logical_or(pred_mask, target_mask).sum()","        ","        if union == 0:","            continue","        ","        iou = intersection / union","        ious.append(iou)","    ","    return np.mean(ious) if len(ious) > 0 else 0.0","","","def response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):","    \"\"\"","    Response-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        teacher_logits: teacher model output logits (B, C, H, W)","        targets: ground truth labels (B, H, W)","        temperature: softmax temperature for distillation","        alpha: weight for cross-entropy loss","        beta: weight for distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Distillation loss (KL divergence)","    student_soft = F.log_softmax(student_logits / temperature, dim=1)","    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)","    ","    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * kd_loss","    ","    return total_loss, ce_loss.item(), kd_loss.item()","","","def feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):","    \"\"\"","    Feature-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        student_features: dict of student intermediate features","        teacher_features: dict of teacher intermediate features","        targets: ground truth labels (B, H, W)","        alpha: weight for cross-entropy loss","        beta: weight for feature distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Feature matching loss (cosine similarity)","    feat_loss = 0","    num_levels = 0","    ","    for level in ['low', 'mid', 'high']:","        if level in student_features and level in teacher_features:","            s_feat = student_features[level]","            t_feat = teacher_features[level]","            ","            # Resize if needed (teacher features might be different size)","            if s_feat.shape != t_feat.shape:","                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)","                # Match channels if needed","                if s_feat.shape[1] != t_feat.shape[1]:","                    # Project teacher features to student channel size","                    continue  # Skip if channel mismatch (or add projection layer)","            ","            # Cosine similarity loss (1 - cosine_similarity)","            s_feat_norm = F.normalize(s_feat, p=2, dim=1)","            t_feat_norm = F.normalize(t_feat, p=2, dim=1)","            ","            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()","            feat_loss += (1 - cosine_sim)","            num_levels += 1","    ","    if num_levels > 0:","        feat_loss = feat_loss / num_levels","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * feat_loss","    ","    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0","","","def train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):","    \"\"\"","    Train for one epoch","    ","    Args:","        model: student model","        teacher_model: teacher model (None if kd_mode='none')","        dataloader: training data loader","        optimizer: optimizer","        device: device to run on","        kd_mode: 'none', 'response', or 'feature'","        args: training arguments","    ","    Returns:","        average loss, average ce_loss, average kd_loss","    \"\"\"","    model.train()","    if teacher_model is not None:","        teacher_model.eval()","    ","    total_loss = 0","    total_ce_loss = 0","    total_kd_loss = 0","    num_batches = 0","    ","    pbar = tqdm(dataloader, desc='Training')","    for images, targets in pbar:","        # Move to device and resize to fixed size for batching","        batch_imgs = []","        batch_targets = []","        ","        for img, target in zip(images, targets):","            img = img.to(device)","            target = target.to(device)","            ","            # Resize to fixed size","            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)","            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()","            ","            batch_imgs.append(img)","            batch_targets.append(target)","        ","        batch_imgs = torch.stack(batch_imgs)","        batch_targets = torch.stack(batch_targets)","        ","        optimizer.zero_grad()","        ","        # Forward pass based on KD mode","        if kd_mode == 'none':","            # No knowledge distillation","            output = model(batch_imgs)","            loss = F.cross_entropy(output, batch_targets, ignore_index=255)","            ce_loss_val = loss.item()","            kd_loss_val = 0.0","            ","        elif kd_mode == 'response':","            # Response-based KD","            output = model(batch_imgs)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","            ","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","            ","        elif kd_mode == 'feature':","            # Feature-based KD","            output, student_features = model(batch_imgs, return_features=True)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)","                # For now, use output-based distillation","                teacher_features = {}  # Would need to extract from teacher","            ","            # Fallback to response-based if teacher features not available","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","        ","        # Backward pass","        loss.backward()","        optimizer.step()","        ","        total_loss += loss.item()","        total_ce_loss += ce_loss_val","        total_kd_loss += kd_loss_val","        num_batches += 1","        ","        pbar.set_postfix({'loss': f'{loss.item():.4f}'})","    ","    avg_loss = total_loss / num_batches","    avg_ce_loss = total_ce_loss / num_batches","    avg_kd_loss = total_kd_loss / num_batches","    ","    return avg_loss, avg_ce_loss, avg_kd_loss","","","def validate(model, dataloader, device, num_classes=21):","    \"\"\"","    Validate the model","    ","    Args:","        model: model to validate","        dataloader: validation data loader","        device: device to run on","        num_classes: number of classes","    ","    Returns:","        mean_iou: mean IoU across all samples","        mean_loss: mean cross-entropy loss","    \"\"\"","    model.eval()","    all_ious = []","    all_losses = []","    criterion = nn.CrossEntropyLoss(ignore_index=255)","    ","    with torch.no_grad():","        for images, targets in tqdm(dataloader, desc='Validation'):","            for img, target in zip(images, targets):","                img = img.unsqueeze(0).to(device)","                target = target.to(device)","                ","                # Resize to fixed size","                h, w = target.shape","                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)","                ","                # Forward pass","                output = model(img)","                ","                # Calculate loss at this resolution","                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), ","                                               size=output.shape[-2:], ","                                               mode='nearest').squeeze().long()","                loss = criterion(output, target_resized.unsqueeze(0))","                all_losses.append(loss.item())","                ","                # Resize back to original size for IoU calculation","                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)","                ","                # Get predictions","                pred = output.squeeze(0).argmax(0)","                ","                # Calculate IoU","                iou = calculate_miou(pred, target, num_classes)","                all_ious.append(iou)","    ","    mean_iou = np.mean(all_ious)","    mean_loss = np.mean(all_losses)","    return mean_iou, mean_loss","","","def plot_training_curves(history, save_path):","    \"\"\"Plot and save training curves\"\"\"","    fig, axes = plt.subplots(1, 2, figsize=(12, 4))","    ","    # Loss plot - now includes both train and validation loss","    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)","    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')","    axes[0].set_xlabel('Epoch')","    axes[0].set_ylabel('Loss')","    axes[0].set_title('Training and Validation Loss')","    axes[0].legend()","    axes[0].grid(True)","    ","    # mIoU plot","    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)","    axes[1].set_xlabel('Epoch')","    axes[1].set_ylabel('mIoU')","    axes[1].set_title('Validation mIoU')","    axes[1].legend()","    axes[1].grid(True)","    ","    plt.tight_layout()","    plt.savefig(save_path, dpi=150)","    plt.close()","    print(f\"Training curves saved to {save_path}\")","","","def main(args):","    \"\"\"Main training function\"\"\"","    ","    print(\"=\" * 70)","    print(\"Training Compact Segmentation Model\")","    print(\"=\" * 70)","    print(f\"Model: {args.model}\")","    print(f\"KD Mode: {args.kd_mode}\")","    print(f\"Epochs: {args.epochs}\")","    print(f\"Batch Size: {args.batch_size}\")","    print(f\"Learning Rate: {args.lr}\")","    print(\"=\" * 70)","    ","    # Set device","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","    print(f\"\\nUsing device: {device}\")","    ","    # Create save directory","    save_dir = Path(args.save_dir)","    save_dir.mkdir(parents=True, exist_ok=True)","    ","    # Load student model","    print(\"\\nLoading student model...\")","    if args.model == 'ultracompact':","        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)","    elif args.model == 'standard':","        model = StandardSegmentationModel(num_classes=21, pretrained=True)","    else:","        raise ValueError(f\"Unknown model: {args.model}\")","    ","    model = model.to(device)","    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)","    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")","    ","    # Load teacher model if using KD","    teacher_model = None","    if args.kd_mode != 'none':","        print(\"\\nLoading teacher model (FCN-ResNet50)...\")","        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1","        teacher_model = fcn_resnet50(weights=weights)","        teacher_model = teacher_model.to(device)","        teacher_model.eval()","        for param in teacher_model.parameters():","            param.requires_grad = False","        print(\"Teacher model loaded and frozen\")","    ","    # Prepare datasets","    print(\"\\nPreparing datasets...\")","    ","    # Find dataset path (Google Colab compatible)","    # VOCSegmentation expects: root/VOCdevkit/VOC2012/","    dataset_roots = []","    ","    # Check for Google Colab structure (both with and without VOCdevkit wrapper)","    colab_train_path = Path('/content/data/VOC2012_train_val')","    if colab_train_path.exists():","        # Check if it has VOCdevkit structure","        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(colab_train_path))","        # Check if files are directly in the folder (no VOCdevkit wrapper)","        elif (colab_train_path / 'JPEGImages').exists():","            # Create a symlink or use parent structure","            dataset_roots.append(str(colab_train_path.parent))","    ","    # Check for local structure","    local_data_path = Path('./data')","    if local_data_path.exists():","        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(local_data_path))","        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():","            dataset_roots.append(str(local_data_path))","    ","    # Check standard paths","    dataset_roots.extend([","        '/content/data',  # Google Colab default","        './data',  # Local path","        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),","    ])","    ","    # Check dataset_path.txt","    dataset_path_file = Path('dataset_path.txt')","    if dataset_path_file.exists():","        with open(dataset_path_file, 'r') as f:","            saved_path = Path(f.read().strip())","            dataset_roots.insert(0, str(saved_path.parent.parent))","    ","    dataset_root = None","    for root in dataset_roots:","        try:","            # Try to create a temporary symlink structure if needed","            root_path = Path(root)","            ","            # Check if we need to create VOCdevkit/VOC2012 structure","            voc_path = root_path / 'VOC2012_train_val'","            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():","                # Files are directly in VOC2012_train_val, create temporary structure","                vocdevkit_path = voc_path / 'VOCdevkit'","                voc2012_path = vocdevkit_path / 'VOC2012'","                if not voc2012_path.exists():","                    print(f\"Creating VOCdevkit structure in {voc_path}\")","                    vocdevkit_path.mkdir(exist_ok=True)","                    voc2012_path.symlink_to(voc_path, target_is_directory=True)","            ","            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)","            dataset_root = root","            print(f\"✓ Successfully loaded dataset from: {root}\")","            break","        except Exception as e:","            continue","    ","    if dataset_root is None:","        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")","    ","    print(f\"Dataset found at: {dataset_root}\")","    ","    # Create datasets","    train_transform = VOCNormalize(augment=True)","    val_transform = VOCNormalize(augment=False)","    ","    train_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='train',","        download=False,","        transforms=train_transform","    )","    ","    val_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='val',","        download=False,","        transforms=val_transform","    )","    ","    print(f\"Training set size: {len(train_dataset)}\")","    print(f\"Validation set size: {len(val_dataset)}\")","    ","    # Create dataloaders","    train_loader = DataLoader(","        train_dataset,","        batch_size=args.batch_size,","        shuffle=True,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    val_loader = DataLoader(","        val_dataset,","        batch_size=4,","        shuffle=False,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    # Setup optimizer and scheduler","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)","    ","    # Training history","    history = {","        'train_loss': [],","        'train_ce_loss': [],","        'train_kd_loss': [],","        'val_loss': [],","        'val_miou': [],","        'epoch_times': []","    }","    ","    # Save hyperparameters","    hyperparameters = {","        'model': args.model,","        'kd_mode': args.kd_mode,","        'epochs': args.epochs,","        'batch_size': args.batch_size,","        'learning_rate': args.lr,","        'weight_decay': args.weight_decay,","        'temperature': args.temperature,","        'alpha': args.alpha,","        'beta': args.beta,","        'optimizer': 'Adam',","        'scheduler': 'CosineAnnealingLR',","        'num_parameters': sum(p.numel() for p in model.parameters()),","        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)","    }","    ","    best_miou = 0.0","    training_start_time = time.time()","    ","    # Training loop","    print(\"\\nStarting training...\")","    print(\"=\" * 70)","    print(\"\\nHyperparameters:\")","    for key, value in hyperparameters.items():","        print(f\"  {key}: {value}\")","    ","    for epoch in range(args.epochs):","        epoch_start_time = time.time()","        ","        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")","        print(\"-\" * 70)","        ","        # Train","        train_loss, train_ce_loss, train_kd_loss = train_epoch(","            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args","        )","        ","        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")","        ","        # Validate","        val_miou = validate(model, val_loader, device)","        print(f\"Validation mIoU: {val_miou:.4f}\")","        ","        # Record epoch time","        epoch_time = time.time() - epoch_start_time","        history['epoch_times'].append(epoch_time)","        print(f\"Epoch time: {epoch_time:.2f} seconds\")","        ","        # Update history","        history['train_loss'].append(train_loss)","        history['train_ce_loss'].append(train_ce_loss)","        history['train_kd_loss'].append(train_kd_loss)","        history['val_miou'].append(val_miou)","        ","        # Save best model","        if val_miou > best_miou:","            best_miou = val_miou","            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history","            }, save_path)","            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")","        ","        # Update scheduler","        scheduler.step()","        ","        # Save checkpoint every 10 epochs","        if (epoch + 1) % 10 == 0:","            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history,","                'hyperparameters': hyperparameters","            }, checkpoint_path)","            print(f\"Checkpoint saved: {checkpoint_path}\")","    ","    # Calculate total training time","    total_training_time = time.time() - training_start_time","    avg_epoch_time = np.mean(history['epoch_times'])","    ","    # Plot training curves","    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"","    plot_training_curves(history, plot_path)","    ","    # Save final training report","    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"","    with open(report_path, 'w') as f:","        f.write(\"=\" * 70 + \"\\n\")","        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")","        f.write(\"=\" * 70 + \"\\n\\n\")","        ","        f.write(\"HYPERPARAMETERS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        for key, value in hyperparameters.items():","            f.write(f\"  {key:25s}: {value}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"TRAINING STATISTICS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")","        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")","        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")","        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")","        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"FILES GENERATED:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")","        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")","        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")","        f.write(\"=\" * 70 + \"\\n\")","    ","    print(\"\\n\" + \"=\" * 70)","    print(\"Training completed!\")","    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")","    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")","    print(f\"Best validation mIoU: {best_miou:.4f}\")","    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")","    print(f\"Training report saved to: {report_path}\")","    ","    # Calculate final score","    score = 4 * best_miou / (1 + num_params / 1e6)","    print(f\"\\nFinal Score: {score:.3f}\")","    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")","    print(\"=\" * 70)","","","if __name__ == '__main__':","    parser = argparse.ArgumentParser(description='Train compact segmentation model')","    ","    # Model arguments","    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],","                        help='Model type to train')","    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],","                        help='Knowledge distillation mode')","    ","    # Training arguments","    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')","    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')","    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')","    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')","    ","    # KD arguments","    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')","    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')","    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')","    ","    # Other arguments","    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')","    ","    args = parser.parse_args()","    ","    main(args)",""],"recentEdit":[[582,583,["        val_miou, val_loss = validate(model, val_loader, device)"]],[583,584,["        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")"]],[594,594,["        history['val_loss'].append(val_loss)"]],[652,652,["        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")"]]],"documentBeforeEdits":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean_iou: mean IoU across all samples\r\n        mean_loss: mean cross-entropy loss\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    all_losses = []\r\n    criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot - now includes both train and validation loss\r\n    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training and Validation Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path, dpi=150)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","recentEdits":[[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]]],"lastSelectionInAfterEdit":[18491,18491]},{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"plaintext","documentLinesBeforeEdit":["# ELEC 475 Lab 3 - Training Commands","# All commands should be run from the Lab 3 directory","","# ============================================================================","# ULTRA-COMPACT MODEL (475K parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# STANDARD MODEL (3.4M parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# TRAINING OUTPUT FILES","# ============================================================================","# For each training run, the following files will be generated in ./checkpoints/:","#   - best_model_{model}_{kd_mode}.pth           (best model weights)","#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)","#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)","#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)","","# ============================================================================","# OPTIONAL: HYPERPARAMETER TUNING","# ============================================================================","# If you want to experiment with different hyperparameters:","","# Adjust learning rate","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005","","# Adjust KD temperature (higher = softer targets)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0","","# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3","","# Adjust batch size (if memory issues)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4","","# ============================================================================","# NOTES","# ============================================================================","# - Training on CPU will be slow. Consider using Google Colab with GPU.","# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU","# - Monitor validation mIoU - training should converge after 30-40 epochs","# - Best model is saved automatically when validation mIoU improves",""],"recentEdit":[[18,19,["# STANDARD MODEL (1parameters)"]]],"documentBeforeEdits":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","recentEdits":[[[867,872,"1"]]],"lastSelectionInAfterEdit":[868,868]}],"activeDocumentIdx":1,"recording":[{"documentType":"workspaceRecording@1.0","kind":"header","repoRootUri":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","time":1762624150653,"uuid":"97a2543a-1f09-451b-9866-1beef366a1c5"},{"kind":"documentEncountered","id":4,"relativePath":"model_standard.py","time":1762545107282},{"kind":"setContent","id":4,"v":1,"content":"\"\"\"\r\nStandard MobileNetV3-Small Segmentation Model\r\nELEC 475 Lab 3 - Option 1\r\n\r\nParameters: ~3.41M\r\nExpected Score: 0.40-0.50\r\n\r\nReferences:\r\n- MobileNetV3: Searching for MobileNetV3 (Howard et al., 2019)\r\n- DeepLabV3+: Encoder-Decoder with Atrous Separable Convolution (Chen et al., 2018)\r\n- ASPP: Rethinking Atrous Convolution for Semantic Image Segmentation (Chen et al., 2017)\r\n\"\"\"\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\r\n\r\n\r\nclass ASPP(nn.Module):\r\n    \"\"\"\r\n    Atrous Spatial Pyramid Pooling (ASPP) module\r\n    Captures multi-scale context using parallel atrous convolutions\r\n    \"\"\"\r\n    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\r\n        super(ASPP, self).__init__()\r\n        \r\n        self.branches = nn.ModuleList()\r\n        \r\n        # 1x1 convolution branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Atrous convolution branches\r\n        for rate in rates:\r\n            self.branches.append(nn.Sequential(\r\n                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\r\n                nn.BatchNorm2d(out_channels),\r\n                nn.ReLU(inplace=True)\r\n            ))\r\n        \r\n        # Global average pooling branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.AdaptiveAvgPool2d(1),\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Combine all branches\r\n        num_branches = len(rates) + 2  # 1x1 + atrous branches + global pooling\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(out_channels * num_branches, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Dropout(0.5)\r\n        )\r\n    \r\n    def forward(self, x):\r\n        h, w = x.shape[2:]\r\n        branch_outputs = []\r\n        \r\n        for i, branch in enumerate(self.branches):\r\n            out = branch(x)\r\n            # Upsample global pooling branch to match spatial dimensions\r\n            if i == len(self.branches) - 1:\r\n                out = F.interpolate(out, size=(h, w), mode='bilinear', align_corners=False)\r\n            branch_outputs.append(out)\r\n        \r\n        # Concatenate all branches\r\n        x = torch.cat(branch_outputs, dim=1)\r\n        x = self.project(x)\r\n        \r\n        return x\r\n\r\n\r\nclass StandardSegmentationModel(nn.Module):\r\n    \"\"\"\r\n    Standard compact segmentation model using MobileNetV3-Small backbone\r\n    \r\n    Architecture:\r\n    - Full MobileNetV3-Small backbone (pretrained)\r\n    - 3-rate ASPP module (rates: 6, 12, 18)\r\n    - Two-stage decoder with skip connections\r\n    - Parameters: ~3.41M\r\n    \"\"\"\r\n    def __init__(self, num_classes=21, pretrained=True):\r\n        super(StandardSegmentationModel, self).__init__()\r\n        \r\n        # Load pretrained MobileNetV3-Small\r\n        if pretrained:\r\n            weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1\r\n            backbone = mobilenet_v3_small(weights=weights)\r\n        else:\r\n            backbone = mobilenet_v3_small(weights=None)\r\n        \r\n        # Extract feature layers from MobileNetV3-Small\r\n        self.features = backbone.features\r\n        \r\n        # Feature dimensions at tap points:\r\n        # Stage 1 (low):  16 channels, stride 4\r\n        # Stage 3 (mid):  24 channels, stride 8\r\n        # Stage 12 (high): 576 channels, stride 16\r\n        \r\n        # ASPP module for multi-scale context\r\n        self.aspp = ASPP(in_channels=576, out_channels=128, rates=[6, 12, 18])\r\n        \r\n        # Feature projection layers (keep dimensions)\r\n        self.mid_conv = nn.Sequential(\r\n            nn.Conv2d(24, 24, 1, bias=False),\r\n            nn.BatchNorm2d(24),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.low_conv = nn.Sequential(\r\n            nn.Conv2d(16, 16, 1, bias=False),\r\n            nn.BatchNorm2d(16),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Decoder with skip connections\r\n        self.decoder_conv1 = nn.Sequential(\r\n            nn.Conv2d(128 + 24, 128, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(128),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.decoder_conv2 = nn.Sequential(\r\n            nn.Conv2d(128 + 16, 64, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(64),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Classifier head\r\n        self.classifier = nn.Sequential(\r\n            nn.Dropout(0.5),\r\n            nn.Conv2d(64, num_classes, 1)\r\n        )\r\n        \r\n        self._init_weights()\r\n    \r\n    def _init_weights(self):\r\n        \"\"\"Initialize weights for new layers\"\"\"\r\n        for m in [self.aspp, self.mid_conv, self.low_conv, self.decoder_conv1, self.decoder_conv2, self.classifier]:\r\n            for module in m.modules():\r\n                if isinstance(module, nn.Conv2d):\r\n                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\r\n                    if module.bias is not None:\r\n                        nn.init.constant_(module.bias, 0)\r\n                elif isinstance(module, nn.BatchNorm2d):\r\n                    nn.init.constant_(module.weight, 1)\r\n                    nn.init.constant_(module.bias, 0)\r\n    \r\n    def forward(self, x, return_features=False):\r\n        \"\"\"\r\n        Forward pass\r\n        \r\n        Args:\r\n            x: input image tensor (B, 3, H, W)\r\n            return_features: if True, return intermediate features for knowledge distillation\r\n            \r\n        Returns:\r\n            output: segmentation logits (B, num_classes, H, W)\r\n            features (optional): dict of intermediate features for KD\r\n        \"\"\"\r\n        input_shape = x.shape[-2:]\r\n        \r\n        # Extract multi-scale features from backbone\r\n        low_feat = None   # stride 4\r\n        mid_feat = None   # stride 8\r\n        high_feat = None  # stride 16\r\n        \r\n        # Forward through MobileNetV3 stages\r\n        for i, layer in enumerate(self.features):\r\n            x = layer(x)\r\n            \r\n            # Capture features at specific stages\r\n            if i == 1:  # After first inverted residual block (stride 4)\r\n                low_feat = x\r\n            elif i == 3:  # After third inverted residual block (stride 8)\r\n                mid_feat = x\r\n        \r\n        # Final high-level features (stride 16)\r\n        high_feat = x\r\n        \r\n        # Process features\r\n        mid_feat = self.mid_conv(mid_feat)\r\n        low_feat = self.low_conv(low_feat)\r\n        \r\n        # ASPP for multi-scale context\r\n        x = self.aspp(high_feat)\r\n        \r\n        # Decoder with skip connections\r\n        # Upsample and concatenate with mid-level features\r\n        x = F.interpolate(x, size=mid_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, mid_feat], dim=1)\r\n        x = self.decoder_conv1(x)\r\n        \r\n        # Upsample and concatenate with low-level features\r\n        x = F.interpolate(x, size=low_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, low_feat], dim=1)\r\n        x = self.decoder_conv2(x)\r\n        \r\n        # Final upsampling to input resolution\r\n        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\r\n        \r\n        # Classifier\r\n        output = self.classifier(x)\r\n        \r\n        if return_features:\r\n            features = {\r\n                'low': low_feat,\r\n                'mid': mid_feat,\r\n                'high': high_feat\r\n            }\r\n            return output, features\r\n        \r\n        return output\r\n\r\n\r\ndef count_parameters(model):\r\n    \"\"\"Count the number of trainable parameters\"\"\"\r\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(\"=\" * 70)\r\n    print(\"Standard MobileNetV3-Small Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Create model\r\n    model = StandardSegmentationModel(num_classes=21, pretrained=False)\r\n    \r\n    # Count parameters\r\n    params = count_parameters(model)\r\n    print(f\"\\nTotal parameters: {params:,} ({params/1e6:.2f}M)\")\r\n    \r\n    # Test forward pass\r\n    print(\"\\nTesting forward pass...\")\r\n    dummy_input = torch.randn(2, 3, 224, 224)\r\n    \r\n    with torch.no_grad():\r\n        output = model(dummy_input)\r\n        print(f\"  Output shape: {output.shape}\")\r\n        \r\n        # Test with feature extraction\r\n        output, features = model(dummy_input, return_features=True)\r\n        print(f\"\\n  Features for knowledge distillation:\")\r\n        for name, feat in features.items():\r\n            print(f\"    {name}: {feat.shape}\")\r\n    \r\n    # Score estimation\r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Score Estimation: Score = 4 × mIoU / (1 + params_M)\")\r\n    print(\"=\" * 70)\r\n    \r\n    for miou in [0.40, 0.45, 0.50, 0.55]:\r\n        score = 4 * miou / (1 + params/1e6)\r\n        print(f\"  mIoU={miou:.2f} → Score={score:.3f}\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n","time":1762545107282},{"kind":"opened","id":4,"time":1762545107282},{"kind":"documentEncountered","id":5,"relativePath":"train.py","time":1762545107284},{"kind":"setContent","id":5,"v":1,"content":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean IoU\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Resize back to original size\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    return mean_iou\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot\r\n    axes[0].plot(history['train_loss'], label='Train Loss')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU')\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","time":1762622282025},{"kind":"opened","id":5,"time":1762622282025},{"kind":"documentEncountered","id":17,"relativePath":"train.txt","time":1762622441478},{"kind":"setContent","id":17,"v":1,"content":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","time":1762622441593},{"kind":"opened","id":17,"time":1762622441593},{"kind":"changed","id":4,"v":2,"edit":[[50,50," (Optimized)"],[94,94,"1."],[95,95,"6M (reduced from 3"],[99,99,")"],[119,120,"8"],[122,123,"1"],[124,125,"2"]],"time":1762623638975},{"kind":"changed","id":4,"v":3,"edit":[[2834,2834," (Optimized)"],[2919,2920,"Lightweight 2"],[2951,2955,""],[2956,2956," - reduced from 3 rates"],[2964,2972,"Simplifi"],[2973,2973,"d"],[2987,2991,"reduced"],[2993,2994,"ha"],[2997,3002,"l"],[3024,3024,"1."],[3025,3025,"6M (reduced from 3"],[3029,3029,")"],[3767,3767,"Lightweight "],[3779,3782,"(2"],[3783,3786,"ra"],[3787,3787,"es "],[3788,3789,"n"],[3790,3790,"tead of 3, redu"],[3791,3793,""],[3794,3794,"d output"],[3796,3797,"han"],[3798,3799,""],[3800,3802,"ls)"],[3859,3862,"64"],[3876,3880,""],[3931,3933,"r"],[3934,3936,""],[3937,3939,"uc"],[3940,3940,"d cha"],[3941,3944,""],[3945,3945,"el"],[4015,4017,"16"],[4063,4065,"16"],[4191,4193,"8"],[4239,4241,"8"],[4310,4310," Simplified"],[4311,4312,"d"],[4324,4328,"reduced"],[4330,4331,"ha"],[4334,4339,"l"],[4409,4412,"64"],[4415,4417,"16"],[4419,4422,"64"],[4479,4482,"64"],[4609,4612,"64"],[4615,4617,"8"],[4619,4621,"32"],[4678,4680,"32"],[4861,4863,"32"]],"time":1762623641194},{"kind":"changed","id":5,"v":2,"edit":[[10251,10251,"_iou:"],[10252,10252,"mean "],[10255,10255," across all samples"],[10261,10261,"    mean_loss: mean cross-entropy loss\r\n    "],[10307,10307,"all_losses = []"],[10313,10313,"criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    "],[10403,10403,"\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss"]],"time":1762623765832},{"kind":"changed","id":5,"v":3,"edit":[[11970,12883,""]],"time":1762623780348},{"kind":"changed","id":5,"v":4,"edit":[[12136,12136," - now includes both train and validation loss"],[12188,12188,"in Loss', l"],[12190,12190,"ewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val"],[12196,12196,", linewidth=2, linestyle='--'"],[12295,12295," and Validation"],[12434,12434,", linewidth=2"],[12645,12645,", dpi=150"]],"time":1762623791607},{"kind":"changed","id":5,"v":5,"edit":[[19126,19126,"loss': [],\r\n        'val_"]],"time":1762623803059},{"kind":"changed","id":5,"v":6,"edit":[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],"time":1762623816438},{"kind":"changed","id":5,"v":7,"edit":[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]],"time":1762623835010},{"kind":"selectionChanged","id":17,"selection":[[883,883]],"time":1762624017851},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624140244},{"kind":"selectionChanged","id":17,"selection":[[869,870]],"time":1762624140330},{"kind":"selectionChanged","id":17,"selection":[[868,870]],"time":1762624140356},{"kind":"selectionChanged","id":17,"selection":[[867,870]],"time":1762624140378},{"kind":"changed","id":17,"v":7,"edit":[[867,870,"1"]],"time":1762624143421},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624143443},{"kind":"changed","id":17,"v":9,"edit":[[868,869,""]],"time":1762624143557},{"kind":"selectionChanged","id":17,"selection":[[849,849]],"time":1762624144669},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624146159},{"kind":"selectionChanged","id":17,"selection":[[869,869]],"time":1762624146967},{"kind":"changed","id":17,"v":13,"edit":[[868,869,""]],"time":1762624150477},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624150514}]},"resultEdit":"   16  16 \n   17  17 # ============================================================================\n-  18     # STANDARD MODEL (1arameters)\n+      18 # STANDARD MODEL (1.3M parameters)\n   19  19 # ============================================================================\n   20  20 \n   21  21 # Baseline (No Knowledge Distillation)","isCachedResult":true,"prompt":"System\n------\nYour role as an AI assistant is to help developers complete their code tasks by assisting in editing specific sections of code marked by the <|code_to_edit|> and <|/code_to_edit|> tags, while adhering to Microsoft's content policies and avoiding the creation of content that violates copyrights.\n\nYou have access to the following information to help you make informed suggestions:\n\n- recently_viewed_code_snippets: These are code snippets that the developer has recently looked at, which might provide context or examples relevant to the current task. They are listed from oldest to newest, with line numbers in the form #| to help you understand the edit diff history. It's possible these are entirely irrelevant to the developer's change.\n- current_file_content: The content of the file the developer is currently working on, providing the broader context of the code. Line numbers in the form #| are included to help you understand the edit diff history.\n- edit_diff_history: A record of changes made to the code, helping you understand the evolution of the code and the developer's intentions. These changes are listed from oldest to latest. It's possible a lot of old edit diff history is entirely irrelevant to the developer's change.\n- area_around_code_to_edit: The context showing the code surrounding the section to be edited.\n- cursor position marked as <|cursor|>: Indicates where the developer's cursor is currently located, which can be crucial for understanding what part of the code they are focusing on.\n\nYour task is to predict and complete the changes the developer would have made next in the <|code_to_edit|> section. The developer may have stopped in the middle of typing. Your goal is to keep the developer on the path that you think they're following. Some examples include further implementing a class, method, or variable, or improving the quality of the code. Make sure the developer doesn't get distracted and ensure your suggestion is relevant. Consider what changes need to be made next, if any. If you think changes should be made, ask yourself if this is truly what needs to happen. If you are confident about it, then proceed with the changes.\n\n# Steps\n\n1. **Review Context**: Analyze the context from the resources provided, such as recently viewed snippets, edit history, surrounding code, and cursor location.\n2. **Evaluate Current Code**: Determine if the current code within the tags requires any corrections or enhancements.\n3. **Suggest Edits**: If changes are required, ensure they align with the developer's patterns and improve code quality.\n4. **Maintain Consistency**: Ensure indentation and formatting follow the existing code style.\n\n# Output Format\n\n- Provide only the revised code within the tags. If no changes are necessary, simply return the original code from within the <|code_to_edit|> and <|/code_to_edit|> tags.\n- There are line numbers in the form #| in the code displayed to you above, but these are just for your reference. Please do not include the numbers of the form #| in your response.\n- Ensure that you do not output duplicate code that exists outside of these tags. The output should be the revised code that was between these tags and should not include the <|code_to_edit|> or <|/code_to_edit|> tags.\n\n```\n// Your revised code goes here\n```\n\n# Notes\n\n- Apologize with \"Sorry, I can't assist with that.\" for requests that may breach Microsoft content guidelines.\n- Avoid undoing or reverting the developer's last change unless there are obvious typos or errors.\n- Don't include the line numbers of the form #| in your response.\n==================\nUser\n------\n```\n<|recently_viewed_code_snippets|>\n<|recently_viewed_code_snippet|>\ncode_snippet_file_path: /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py (truncated)\n\"\"\"\nTraining Script for Compact Segmentation Models with Knowledge Distillation\nELEC 475 Lab 3\n\nSupports three training modes:\n1. No Knowledge Distillation (baseline)\n2. Response-based Knowledge Distillation\n3. Feature-based Knowledge Distillation\n\nReferences:\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\n\"\"\"\n\nimport argparse\nimport os\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import VOCSegmentation\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\nfrom tqdm import tqdm\n\n# Import custom models\nfrom model_ultracompact import UltraCompactSegmentationModel\nfrom model_standard import StandardSegmentationModel\n\n\nclass VOCNormalize:\n    \"\"\"Custom normalization for VOC dataset\"\"\"\n    def __init__(self, augment=True):\n        self.augment = augment\n        self.normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    \n    def __call__(self, img, target):\n        # Convert PIL to tensor\n        img = transforms.ToTensor()(img)\n        \n        # Apply augmentation if training\n        if self.augment and np.random.rand() > 0.5:\n            # Random horizontal flip\n            img = transforms.functional.hflip(img)\n            target = transforms.functional.hflip(target)\n        \n        # Normalize image\n        img = self.normalize(img)\n        \n        # Convert target to tensor\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n        \n        return img, target\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\n    images, targets = zip(*batch)\n    return list(images), list(targets)\n\n\ndef calculate_miou(pred, target, num_classes=21):\n    \"\"\"\n    Calculate mean Intersection over Union (mIoU)\n    \n    Args:\n        pred: predicted segmentation mask (H, W)\n        target: ground truth segmentation mask (H, W)\n        num_classes: number of classes\n    \n    Returns:\n        mIoU value\n    \"\"\"\n    pred = pred.cpu().numpy()\n    target = target.cpu().numpy()\n    \n    ious = []\n    for cls in range(num_classes):\n        pred_mask = (pred == cls)\n        target_mask = (target == cls)\n        \n        intersection = np.logical_and(pred_mask, target_mask).sum()\n        union = np.logical_or(pred_mask, target_mask).sum()\n        \n        if union == 0:\n            continue\n        \n        iou = intersection / union\n        ious.append(iou)\n    \n    return np.mean(ious) if len(ious) > 0 else 0.0\n\n\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\n    \"\"\"\n    Response-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        teacher_logits: teacher model output logits (B, C, H, W)\n        targets: ground truth labels (B, H, W)\n        temperature: softmax temperature for distillation\n        alpha: weight for cross-entropy loss\n        beta: weight for distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Distillation loss (KL divergence)\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\n    \n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * kd_loss\n    \n    return total_loss, ce_loss.item(), kd_loss.item()\n\n\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\n    \"\"\"\n    Feature-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        student_features: dict of student intermediate features\n        teacher_features: dict of teacher intermediate features\n        targets: ground truth labels (B, H, W)\n        alpha: weight for cross-entropy loss\n        beta: weight for feature distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Feature matching loss (cosine similarity)\n    feat_loss = 0\n    num_levels = 0\n    \n    for level in ['low', 'mid', 'high']:\n        if level in student_features and level in teacher_features:\n            s_feat = student_features[level]\n            t_feat = teacher_features[level]\n            \n            # Resize if needed (teacher features might be different size)\n            if s_feat.shape != t_feat.shape:\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n                # Match channels if needed\n                if s_feat.shape[1] != t_feat.shape[1]:\n                    # Project teacher features to student channel size\n                    continue  # Skip if channel mismatch (or add projection layer)\n            \n            # Cosine similarity loss (1 - cosine_similarity)\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\n            \n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\n            feat_loss += (1 - cosine_sim)\n            num_levels += 1\n    \n    if num_levels > 0:\n        feat_loss = feat_loss / num_levels\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * feat_loss\n    \n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\n\n\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\n    \"\"\"\n    Train for one epoch\n    \n    Args:\n        model: student model\n        teacher_model: teacher model (None if kd_mode='none')\n        dataloader: training data loader\n        optimizer: optimizer\n        device: device to run on\n        kd_mode: 'none', 'response', or 'feature'\n        args: training arguments\n    \n    Returns:\n        average loss, average ce_loss, average kd_loss\n    \"\"\"\n    model.train()\n    if teacher_model is not None:\n        teacher_model.eval()\n    \n    total_loss = 0\n    total_ce_loss = 0\n    total_kd_loss = 0\n    num_batches = 0\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for images, targets in pbar:\n        # Move to device and resize to fixed size for batching\n        batch_imgs = []\n        batch_targets = []\n        \n        for img, target in zip(images, targets):\n            img = img.to(device)\n            target = target.to(device)\n            \n            # Resize to fixed size\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\n            \n            batch_imgs.append(img)\n            batch_targets.append(target)\n        \n        batch_imgs = torch.stack(batch_imgs)\n<|/recently_viewed_code_snippet|>\n<|/recently_viewed_code_snippets|>\n\n<|current_file_content|>\ncurrent_file_path: train.txt\n# ELEC 475 Lab 3 - Training Commands\n# All commands should be run from the Lab 3 directory\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n# STANDARD MODEL (1<|cursor|>parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\n\n# ============================================================================\n# OPTIONAL: HYPERPARAMETER TUNING\n# ============================================================================\n# If you want to experiment with different hyperparameters:\n\n# Adjust learning rate\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\n\n# Adjust KD temperature (higher = softer targets)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\n\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\n\n# Adjust batch size (if memory issues)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\n\n# ============================================================================\n# NOTES\n# ============================================================================\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\n# - Monitor validation mIoU - training should converge after 30-40 epochs\n# - Best model is saved automatically when validation mIoU improves\n\n<|/current_file_content|>\n\n<|edit_diff_history|>\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -342,23 +342,0 @@\n-            for img, target in zip(images, targets):\n-                img = img.unsqueeze(0).to(device)\n-                target = target.to(device)\n-                \n-                # Resize to fixed size\n-                h, w = target.shape\n-                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\n-                \n-                # Forward pass\n-                output = model(img)\n-                \n-                # Resize back to original size\n-                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\n-                \n-                # Get predictions\n-                pred = output.squeeze(0).argmax(0)\n-                \n-                # Calculate IoU\n-                iou = calculate_miou(pred, target, num_classes)\n-                all_ious.append(iou)\n-    \n-    mean_iou = np.mean(all_ious)\n-    return mean_iou\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -348,1 +348,1 @@\n-    # Loss plot\n+    # Loss plot - now includes both train and validation loss\n@@ -349,1 +349,2 @@\n-    axes[0].plot(history['train_loss'], label='Train Loss')\n+    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n+    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\n@@ -352,1 +352,1 @@\n-    axes[0].set_title('Training Loss')\n+    axes[0].set_title('Training and Validation Loss')\n@@ -357,1 +357,1 @@\n-    axes[1].plot(history['val_miou'], label='Validation mIoU')\n+    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\n@@ -365,1 +365,1 @@\n-    plt.savefig(save_path)\n+    plt.savefig(save_path, dpi=150)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -535,1 +535,2 @@\n-        'val_miou': [],\n+        'val_loss': [],\n+        'val_miou': [],\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -581,1 +581,1 @@\n-        val_miou = validate(model, val_loader, device)\n+        val_miou, val_loss = validate(model, val_loader, device)\n@@ -582,1 +582,1 @@\n-        print(f\"Validation mIoU: {val_miou:.4f}\")\n+        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")\n@@ -592,1 +592,2 @@\n-        history['train_kd_loss'].append(train_kd_loss)\n+        history['train_kd_loss'].append(train_kd_loss)\n+        history['val_loss'].append(val_loss)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -652,1 +652,2 @@\n-        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n@@ -17,1 +17,1 @@\n-# STANDARD MODEL (3.4M parameters)\n+# STANDARD MODEL (1parameters)\n\n<|/edit_diff_history|>\n\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n# STANDARD MODEL (1<|cursor|>parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n```\n\nThe developer was working on a section of code within the tags `code_to_edit` in the file located at `train.txt`. Using the given `recently_viewed_code_snippets`, `current_file_content`, `edit_diff_history`, `area_around_code_to_edit`, and the cursor position marked as `<|cursor|>`, please continue the developer's work. Update the `code_to_edit` section by predicting and completing the changes they would have made next. Provide the revised code that was between the `<|code_to_edit|>` and `<|/code_to_edit|>` tags with the following format, but do not include the tags themselves.\n```\n// Your revised code goes here\n```\n==================","error":"undefined","response":"```\n\n# ============================================================================\n# STANDARD MODEL (1.3M parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n```","logs":["```\n{\n\t\"isAsyncCompletions\": true,\n\t\"isRevisedCacheStrategy\": true,\n\t\"isCacheTracksRejections\": true,\n\t\"isRecentlyShownCacheEnabled\": false,\n\t\"debounceUseCoreRequestTime\": false\n}\n```\n"]}
{"requestId":7,"time":1762624152329,"filePath":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","version":7,"statelessNextEditProviderId":"XtabProvider","nextEditRequest":{"id":"d2461b87-001a-4b73-8e62-afe19cd44b94","documents":[{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.py","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"python","documentLinesBeforeEdit":["\"\"\"","Training Script for Compact Segmentation Models with Knowledge Distillation","ELEC 475 Lab 3","","Supports three training modes:","1. No Knowledge Distillation (baseline)","2. Response-based Knowledge Distillation","3. Feature-based Knowledge Distillation","","References:","- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)","- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)","\"\"\"","","import argparse","import os","import time","from pathlib import Path","import numpy as np","import matplotlib.pyplot as plt","","import torch","import torch.nn as nn","import torch.nn.functional as F","from torch.utils.data import DataLoader","from torchvision import transforms","from torchvision.datasets import VOCSegmentation","from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights","from tqdm import tqdm","","# Import custom models","from model_ultracompact import UltraCompactSegmentationModel","from model_standard import StandardSegmentationModel","","","class VOCNormalize:","    \"\"\"Custom normalization for VOC dataset\"\"\"","    def __init__(self, augment=True):","        self.augment = augment","        self.normalize = transforms.Normalize(","            mean=[0.485, 0.456, 0.406],","            std=[0.229, 0.224, 0.225]","        )","    ","    def __call__(self, img, target):","        # Convert PIL to tensor","        img = transforms.ToTensor()(img)","        ","        # Apply augmentation if training","        if self.augment and np.random.rand() > 0.5:","            # Random horizontal flip","            img = transforms.functional.hflip(img)","            target = transforms.functional.hflip(target)","        ","        # Normalize image","        img = self.normalize(img)","        ","        # Convert target to tensor","        target = torch.as_tensor(np.array(target), dtype=torch.int64)","        ","        return img, target","","","def collate_fn(batch):","    \"\"\"Custom collate function to handle variable-sized images\"\"\"","    images, targets = zip(*batch)","    return list(images), list(targets)","","","def calculate_miou(pred, target, num_classes=21):","    \"\"\"","    Calculate mean Intersection over Union (mIoU)","    ","    Args:","        pred: predicted segmentation mask (H, W)","        target: ground truth segmentation mask (H, W)","        num_classes: number of classes","    ","    Returns:","        mIoU value","    \"\"\"","    pred = pred.cpu().numpy()","    target = target.cpu().numpy()","    ","    ious = []","    for cls in range(num_classes):","        pred_mask = (pred == cls)","        target_mask = (target == cls)","        ","        intersection = np.logical_and(pred_mask, target_mask).sum()","        union = np.logical_or(pred_mask, target_mask).sum()","        ","        if union == 0:","            continue","        ","        iou = intersection / union","        ious.append(iou)","    ","    return np.mean(ious) if len(ious) > 0 else 0.0","","","def response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):","    \"\"\"","    Response-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        teacher_logits: teacher model output logits (B, C, H, W)","        targets: ground truth labels (B, H, W)","        temperature: softmax temperature for distillation","        alpha: weight for cross-entropy loss","        beta: weight for distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Distillation loss (KL divergence)","    student_soft = F.log_softmax(student_logits / temperature, dim=1)","    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)","    ","    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * kd_loss","    ","    return total_loss, ce_loss.item(), kd_loss.item()","","","def feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):","    \"\"\"","    Feature-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        student_features: dict of student intermediate features","        teacher_features: dict of teacher intermediate features","        targets: ground truth labels (B, H, W)","        alpha: weight for cross-entropy loss","        beta: weight for feature distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Feature matching loss (cosine similarity)","    feat_loss = 0","    num_levels = 0","    ","    for level in ['low', 'mid', 'high']:","        if level in student_features and level in teacher_features:","            s_feat = student_features[level]","            t_feat = teacher_features[level]","            ","            # Resize if needed (teacher features might be different size)","            if s_feat.shape != t_feat.shape:","                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)","                # Match channels if needed","                if s_feat.shape[1] != t_feat.shape[1]:","                    # Project teacher features to student channel size","                    continue  # Skip if channel mismatch (or add projection layer)","            ","            # Cosine similarity loss (1 - cosine_similarity)","            s_feat_norm = F.normalize(s_feat, p=2, dim=1)","            t_feat_norm = F.normalize(t_feat, p=2, dim=1)","            ","            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()","            feat_loss += (1 - cosine_sim)","            num_levels += 1","    ","    if num_levels > 0:","        feat_loss = feat_loss / num_levels","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * feat_loss","    ","    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0","","","def train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):","    \"\"\"","    Train for one epoch","    ","    Args:","        model: student model","        teacher_model: teacher model (None if kd_mode='none')","        dataloader: training data loader","        optimizer: optimizer","        device: device to run on","        kd_mode: 'none', 'response', or 'feature'","        args: training arguments","    ","    Returns:","        average loss, average ce_loss, average kd_loss","    \"\"\"","    model.train()","    if teacher_model is not None:","        teacher_model.eval()","    ","    total_loss = 0","    total_ce_loss = 0","    total_kd_loss = 0","    num_batches = 0","    ","    pbar = tqdm(dataloader, desc='Training')","    for images, targets in pbar:","        # Move to device and resize to fixed size for batching","        batch_imgs = []","        batch_targets = []","        ","        for img, target in zip(images, targets):","            img = img.to(device)","            target = target.to(device)","            ","            # Resize to fixed size","            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)","            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()","            ","            batch_imgs.append(img)","            batch_targets.append(target)","        ","        batch_imgs = torch.stack(batch_imgs)","        batch_targets = torch.stack(batch_targets)","        ","        optimizer.zero_grad()","        ","        # Forward pass based on KD mode","        if kd_mode == 'none':","            # No knowledge distillation","            output = model(batch_imgs)","            loss = F.cross_entropy(output, batch_targets, ignore_index=255)","            ce_loss_val = loss.item()","            kd_loss_val = 0.0","            ","        elif kd_mode == 'response':","            # Response-based KD","            output = model(batch_imgs)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","            ","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","            ","        elif kd_mode == 'feature':","            # Feature-based KD","            output, student_features = model(batch_imgs, return_features=True)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)","                # For now, use output-based distillation","                teacher_features = {}  # Would need to extract from teacher","            ","            # Fallback to response-based if teacher features not available","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","        ","        # Backward pass","        loss.backward()","        optimizer.step()","        ","        total_loss += loss.item()","        total_ce_loss += ce_loss_val","        total_kd_loss += kd_loss_val","        num_batches += 1","        ","        pbar.set_postfix({'loss': f'{loss.item():.4f}'})","    ","    avg_loss = total_loss / num_batches","    avg_ce_loss = total_ce_loss / num_batches","    avg_kd_loss = total_kd_loss / num_batches","    ","    return avg_loss, avg_ce_loss, avg_kd_loss","","","def validate(model, dataloader, device, num_classes=21):","    \"\"\"","    Validate the model","    ","    Args:","        model: model to validate","        dataloader: validation data loader","        device: device to run on","        num_classes: number of classes","    ","    Returns:","        mean_iou: mean IoU across all samples","        mean_loss: mean cross-entropy loss","    \"\"\"","    model.eval()","    all_ious = []","    all_losses = []","    criterion = nn.CrossEntropyLoss(ignore_index=255)","    ","    with torch.no_grad():","        for images, targets in tqdm(dataloader, desc='Validation'):","            for img, target in zip(images, targets):","                img = img.unsqueeze(0).to(device)","                target = target.to(device)","                ","                # Resize to fixed size","                h, w = target.shape","                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)","                ","                # Forward pass","                output = model(img)","                ","                # Calculate loss at this resolution","                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), ","                                               size=output.shape[-2:], ","                                               mode='nearest').squeeze().long()","                loss = criterion(output, target_resized.unsqueeze(0))","                all_losses.append(loss.item())","                ","                # Resize back to original size for IoU calculation","                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)","                ","                # Get predictions","                pred = output.squeeze(0).argmax(0)","                ","                # Calculate IoU","                iou = calculate_miou(pred, target, num_classes)","                all_ious.append(iou)","    ","    mean_iou = np.mean(all_ious)","    mean_loss = np.mean(all_losses)","    return mean_iou, mean_loss","","","def plot_training_curves(history, save_path):","    \"\"\"Plot and save training curves\"\"\"","    fig, axes = plt.subplots(1, 2, figsize=(12, 4))","    ","    # Loss plot - now includes both train and validation loss","    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)","    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')","    axes[0].set_xlabel('Epoch')","    axes[0].set_ylabel('Loss')","    axes[0].set_title('Training and Validation Loss')","    axes[0].legend()","    axes[0].grid(True)","    ","    # mIoU plot","    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)","    axes[1].set_xlabel('Epoch')","    axes[1].set_ylabel('mIoU')","    axes[1].set_title('Validation mIoU')","    axes[1].legend()","    axes[1].grid(True)","    ","    plt.tight_layout()","    plt.savefig(save_path, dpi=150)","    plt.close()","    print(f\"Training curves saved to {save_path}\")","","","def main(args):","    \"\"\"Main training function\"\"\"","    ","    print(\"=\" * 70)","    print(\"Training Compact Segmentation Model\")","    print(\"=\" * 70)","    print(f\"Model: {args.model}\")","    print(f\"KD Mode: {args.kd_mode}\")","    print(f\"Epochs: {args.epochs}\")","    print(f\"Batch Size: {args.batch_size}\")","    print(f\"Learning Rate: {args.lr}\")","    print(\"=\" * 70)","    ","    # Set device","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","    print(f\"\\nUsing device: {device}\")","    ","    # Create save directory","    save_dir = Path(args.save_dir)","    save_dir.mkdir(parents=True, exist_ok=True)","    ","    # Load student model","    print(\"\\nLoading student model...\")","    if args.model == 'ultracompact':","        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)","    elif args.model == 'standard':","        model = StandardSegmentationModel(num_classes=21, pretrained=True)","    else:","        raise ValueError(f\"Unknown model: {args.model}\")","    ","    model = model.to(device)","    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)","    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")","    ","    # Load teacher model if using KD","    teacher_model = None","    if args.kd_mode != 'none':","        print(\"\\nLoading teacher model (FCN-ResNet50)...\")","        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1","        teacher_model = fcn_resnet50(weights=weights)","        teacher_model = teacher_model.to(device)","        teacher_model.eval()","        for param in teacher_model.parameters():","            param.requires_grad = False","        print(\"Teacher model loaded and frozen\")","    ","    # Prepare datasets","    print(\"\\nPreparing datasets...\")","    ","    # Find dataset path (Google Colab compatible)","    # VOCSegmentation expects: root/VOCdevkit/VOC2012/","    dataset_roots = []","    ","    # Check for Google Colab structure (both with and without VOCdevkit wrapper)","    colab_train_path = Path('/content/data/VOC2012_train_val')","    if colab_train_path.exists():","        # Check if it has VOCdevkit structure","        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(colab_train_path))","        # Check if files are directly in the folder (no VOCdevkit wrapper)","        elif (colab_train_path / 'JPEGImages').exists():","            # Create a symlink or use parent structure","            dataset_roots.append(str(colab_train_path.parent))","    ","    # Check for local structure","    local_data_path = Path('./data')","    if local_data_path.exists():","        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(local_data_path))","        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():","            dataset_roots.append(str(local_data_path))","    ","    # Check standard paths","    dataset_roots.extend([","        '/content/data',  # Google Colab default","        './data',  # Local path","        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),","    ])","    ","    # Check dataset_path.txt","    dataset_path_file = Path('dataset_path.txt')","    if dataset_path_file.exists():","        with open(dataset_path_file, 'r') as f:","            saved_path = Path(f.read().strip())","            dataset_roots.insert(0, str(saved_path.parent.parent))","    ","    dataset_root = None","    for root in dataset_roots:","        try:","            # Try to create a temporary symlink structure if needed","            root_path = Path(root)","            ","            # Check if we need to create VOCdevkit/VOC2012 structure","            voc_path = root_path / 'VOC2012_train_val'","            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():","                # Files are directly in VOC2012_train_val, create temporary structure","                vocdevkit_path = voc_path / 'VOCdevkit'","                voc2012_path = vocdevkit_path / 'VOC2012'","                if not voc2012_path.exists():","                    print(f\"Creating VOCdevkit structure in {voc_path}\")","                    vocdevkit_path.mkdir(exist_ok=True)","                    voc2012_path.symlink_to(voc_path, target_is_directory=True)","            ","            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)","            dataset_root = root","            print(f\"✓ Successfully loaded dataset from: {root}\")","            break","        except Exception as e:","            continue","    ","    if dataset_root is None:","        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")","    ","    print(f\"Dataset found at: {dataset_root}\")","    ","    # Create datasets","    train_transform = VOCNormalize(augment=True)","    val_transform = VOCNormalize(augment=False)","    ","    train_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='train',","        download=False,","        transforms=train_transform","    )","    ","    val_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='val',","        download=False,","        transforms=val_transform","    )","    ","    print(f\"Training set size: {len(train_dataset)}\")","    print(f\"Validation set size: {len(val_dataset)}\")","    ","    # Create dataloaders","    train_loader = DataLoader(","        train_dataset,","        batch_size=args.batch_size,","        shuffle=True,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    val_loader = DataLoader(","        val_dataset,","        batch_size=4,","        shuffle=False,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    # Setup optimizer and scheduler","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)","    ","    # Training history","    history = {","        'train_loss': [],","        'train_ce_loss': [],","        'train_kd_loss': [],","        'val_loss': [],","        'val_miou': [],","        'epoch_times': []","    }","    ","    # Save hyperparameters","    hyperparameters = {","        'model': args.model,","        'kd_mode': args.kd_mode,","        'epochs': args.epochs,","        'batch_size': args.batch_size,","        'learning_rate': args.lr,","        'weight_decay': args.weight_decay,","        'temperature': args.temperature,","        'alpha': args.alpha,","        'beta': args.beta,","        'optimizer': 'Adam',","        'scheduler': 'CosineAnnealingLR',","        'num_parameters': sum(p.numel() for p in model.parameters()),","        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)","    }","    ","    best_miou = 0.0","    training_start_time = time.time()","    ","    # Training loop","    print(\"\\nStarting training...\")","    print(\"=\" * 70)","    print(\"\\nHyperparameters:\")","    for key, value in hyperparameters.items():","        print(f\"  {key}: {value}\")","    ","    for epoch in range(args.epochs):","        epoch_start_time = time.time()","        ","        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")","        print(\"-\" * 70)","        ","        # Train","        train_loss, train_ce_loss, train_kd_loss = train_epoch(","            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args","        )","        ","        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")","        ","        # Validate","        val_miou = validate(model, val_loader, device)","        print(f\"Validation mIoU: {val_miou:.4f}\")","        ","        # Record epoch time","        epoch_time = time.time() - epoch_start_time","        history['epoch_times'].append(epoch_time)","        print(f\"Epoch time: {epoch_time:.2f} seconds\")","        ","        # Update history","        history['train_loss'].append(train_loss)","        history['train_ce_loss'].append(train_ce_loss)","        history['train_kd_loss'].append(train_kd_loss)","        history['val_miou'].append(val_miou)","        ","        # Save best model","        if val_miou > best_miou:","            best_miou = val_miou","            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history","            }, save_path)","            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")","        ","        # Update scheduler","        scheduler.step()","        ","        # Save checkpoint every 10 epochs","        if (epoch + 1) % 10 == 0:","            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history,","                'hyperparameters': hyperparameters","            }, checkpoint_path)","            print(f\"Checkpoint saved: {checkpoint_path}\")","    ","    # Calculate total training time","    total_training_time = time.time() - training_start_time","    avg_epoch_time = np.mean(history['epoch_times'])","    ","    # Plot training curves","    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"","    plot_training_curves(history, plot_path)","    ","    # Save final training report","    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"","    with open(report_path, 'w') as f:","        f.write(\"=\" * 70 + \"\\n\")","        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")","        f.write(\"=\" * 70 + \"\\n\\n\")","        ","        f.write(\"HYPERPARAMETERS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        for key, value in hyperparameters.items():","            f.write(f\"  {key:25s}: {value}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"TRAINING STATISTICS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")","        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")","        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")","        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")","        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"FILES GENERATED:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")","        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")","        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")","        f.write(\"=\" * 70 + \"\\n\")","    ","    print(\"\\n\" + \"=\" * 70)","    print(\"Training completed!\")","    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")","    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")","    print(f\"Best validation mIoU: {best_miou:.4f}\")","    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")","    print(f\"Training report saved to: {report_path}\")","    ","    # Calculate final score","    score = 4 * best_miou / (1 + num_params / 1e6)","    print(f\"\\nFinal Score: {score:.3f}\")","    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")","    print(\"=\" * 70)","","","if __name__ == '__main__':","    parser = argparse.ArgumentParser(description='Train compact segmentation model')","    ","    # Model arguments","    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],","                        help='Model type to train')","    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],","                        help='Knowledge distillation mode')","    ","    # Training arguments","    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')","    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')","    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')","    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')","    ","    # KD arguments","    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')","    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')","    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')","    ","    # Other arguments","    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')","    ","    args = parser.parse_args()","    ","    main(args)",""],"recentEdit":[[582,583,["        val_miou, val_loss = validate(model, val_loader, device)"]],[583,584,["        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")"]],[594,594,["        history['val_loss'].append(val_loss)"]],[652,652,["        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")"]]],"documentBeforeEdits":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean_iou: mean IoU across all samples\r\n        mean_loss: mean cross-entropy loss\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    all_losses = []\r\n    criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot - now includes both train and validation loss\r\n    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training and Validation Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path, dpi=150)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","recentEdits":[[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]]],"lastSelectionInAfterEdit":[18491,18491]},{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"plaintext","documentLinesBeforeEdit":["# ELEC 475 Lab 3 - Training Commands","# All commands should be run from the Lab 3 directory","","# ============================================================================","# ULTRA-COMPACT MODEL (475K parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# STANDARD MODEL (3.4M parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# TRAINING OUTPUT FILES","# ============================================================================","# For each training run, the following files will be generated in ./checkpoints/:","#   - best_model_{model}_{kd_mode}.pth           (best model weights)","#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)","#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)","#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)","","# ============================================================================","# OPTIONAL: HYPERPARAMETER TUNING","# ============================================================================","# If you want to experiment with different hyperparameters:","","# Adjust learning rate","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005","","# Adjust KD temperature (higher = softer targets)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0","","# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3","","# Adjust batch size (if memory issues)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4","","# ============================================================================","# NOTES","# ============================================================================","# - Training on CPU will be slow. Consider using Google Colab with GPU.","# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU","# - Monitor validation mIoU - training should converge after 30-40 epochs","# - Best model is saved automatically when validation mIoU improves",""],"recentEdit":[[18,19,[" # STANDARD MODEL (1arameters)"]]],"documentBeforeEdits":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","recentEdits":[[[867,873,"1"]],[[849,849," "]]],"lastSelectionInAfterEdit":[850,850]}],"activeDocumentIdx":1,"recording":[{"documentType":"workspaceRecording@1.0","kind":"header","repoRootUri":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","time":1762624152333,"uuid":"c90b2c0a-008b-47db-b3af-2285c45f081a"},{"kind":"documentEncountered","id":4,"relativePath":"model_standard.py","time":1762545107282},{"kind":"setContent","id":4,"v":1,"content":"\"\"\"\r\nStandard MobileNetV3-Small Segmentation Model\r\nELEC 475 Lab 3 - Option 1\r\n\r\nParameters: ~3.41M\r\nExpected Score: 0.40-0.50\r\n\r\nReferences:\r\n- MobileNetV3: Searching for MobileNetV3 (Howard et al., 2019)\r\n- DeepLabV3+: Encoder-Decoder with Atrous Separable Convolution (Chen et al., 2018)\r\n- ASPP: Rethinking Atrous Convolution for Semantic Image Segmentation (Chen et al., 2017)\r\n\"\"\"\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\r\n\r\n\r\nclass ASPP(nn.Module):\r\n    \"\"\"\r\n    Atrous Spatial Pyramid Pooling (ASPP) module\r\n    Captures multi-scale context using parallel atrous convolutions\r\n    \"\"\"\r\n    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\r\n        super(ASPP, self).__init__()\r\n        \r\n        self.branches = nn.ModuleList()\r\n        \r\n        # 1x1 convolution branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Atrous convolution branches\r\n        for rate in rates:\r\n            self.branches.append(nn.Sequential(\r\n                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\r\n                nn.BatchNorm2d(out_channels),\r\n                nn.ReLU(inplace=True)\r\n            ))\r\n        \r\n        # Global average pooling branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.AdaptiveAvgPool2d(1),\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Combine all branches\r\n        num_branches = len(rates) + 2  # 1x1 + atrous branches + global pooling\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(out_channels * num_branches, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Dropout(0.5)\r\n        )\r\n    \r\n    def forward(self, x):\r\n        h, w = x.shape[2:]\r\n        branch_outputs = []\r\n        \r\n        for i, branch in enumerate(self.branches):\r\n            out = branch(x)\r\n            # Upsample global pooling branch to match spatial dimensions\r\n            if i == len(self.branches) - 1:\r\n                out = F.interpolate(out, size=(h, w), mode='bilinear', align_corners=False)\r\n            branch_outputs.append(out)\r\n        \r\n        # Concatenate all branches\r\n        x = torch.cat(branch_outputs, dim=1)\r\n        x = self.project(x)\r\n        \r\n        return x\r\n\r\n\r\nclass StandardSegmentationModel(nn.Module):\r\n    \"\"\"\r\n    Standard compact segmentation model using MobileNetV3-Small backbone\r\n    \r\n    Architecture:\r\n    - Full MobileNetV3-Small backbone (pretrained)\r\n    - 3-rate ASPP module (rates: 6, 12, 18)\r\n    - Two-stage decoder with skip connections\r\n    - Parameters: ~3.41M\r\n    \"\"\"\r\n    def __init__(self, num_classes=21, pretrained=True):\r\n        super(StandardSegmentationModel, self).__init__()\r\n        \r\n        # Load pretrained MobileNetV3-Small\r\n        if pretrained:\r\n            weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1\r\n            backbone = mobilenet_v3_small(weights=weights)\r\n        else:\r\n            backbone = mobilenet_v3_small(weights=None)\r\n        \r\n        # Extract feature layers from MobileNetV3-Small\r\n        self.features = backbone.features\r\n        \r\n        # Feature dimensions at tap points:\r\n        # Stage 1 (low):  16 channels, stride 4\r\n        # Stage 3 (mid):  24 channels, stride 8\r\n        # Stage 12 (high): 576 channels, stride 16\r\n        \r\n        # ASPP module for multi-scale context\r\n        self.aspp = ASPP(in_channels=576, out_channels=128, rates=[6, 12, 18])\r\n        \r\n        # Feature projection layers (keep dimensions)\r\n        self.mid_conv = nn.Sequential(\r\n            nn.Conv2d(24, 24, 1, bias=False),\r\n            nn.BatchNorm2d(24),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.low_conv = nn.Sequential(\r\n            nn.Conv2d(16, 16, 1, bias=False),\r\n            nn.BatchNorm2d(16),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Decoder with skip connections\r\n        self.decoder_conv1 = nn.Sequential(\r\n            nn.Conv2d(128 + 24, 128, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(128),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.decoder_conv2 = nn.Sequential(\r\n            nn.Conv2d(128 + 16, 64, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(64),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Classifier head\r\n        self.classifier = nn.Sequential(\r\n            nn.Dropout(0.5),\r\n            nn.Conv2d(64, num_classes, 1)\r\n        )\r\n        \r\n        self._init_weights()\r\n    \r\n    def _init_weights(self):\r\n        \"\"\"Initialize weights for new layers\"\"\"\r\n        for m in [self.aspp, self.mid_conv, self.low_conv, self.decoder_conv1, self.decoder_conv2, self.classifier]:\r\n            for module in m.modules():\r\n                if isinstance(module, nn.Conv2d):\r\n                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\r\n                    if module.bias is not None:\r\n                        nn.init.constant_(module.bias, 0)\r\n                elif isinstance(module, nn.BatchNorm2d):\r\n                    nn.init.constant_(module.weight, 1)\r\n                    nn.init.constant_(module.bias, 0)\r\n    \r\n    def forward(self, x, return_features=False):\r\n        \"\"\"\r\n        Forward pass\r\n        \r\n        Args:\r\n            x: input image tensor (B, 3, H, W)\r\n            return_features: if True, return intermediate features for knowledge distillation\r\n            \r\n        Returns:\r\n            output: segmentation logits (B, num_classes, H, W)\r\n            features (optional): dict of intermediate features for KD\r\n        \"\"\"\r\n        input_shape = x.shape[-2:]\r\n        \r\n        # Extract multi-scale features from backbone\r\n        low_feat = None   # stride 4\r\n        mid_feat = None   # stride 8\r\n        high_feat = None  # stride 16\r\n        \r\n        # Forward through MobileNetV3 stages\r\n        for i, layer in enumerate(self.features):\r\n            x = layer(x)\r\n            \r\n            # Capture features at specific stages\r\n            if i == 1:  # After first inverted residual block (stride 4)\r\n                low_feat = x\r\n            elif i == 3:  # After third inverted residual block (stride 8)\r\n                mid_feat = x\r\n        \r\n        # Final high-level features (stride 16)\r\n        high_feat = x\r\n        \r\n        # Process features\r\n        mid_feat = self.mid_conv(mid_feat)\r\n        low_feat = self.low_conv(low_feat)\r\n        \r\n        # ASPP for multi-scale context\r\n        x = self.aspp(high_feat)\r\n        \r\n        # Decoder with skip connections\r\n        # Upsample and concatenate with mid-level features\r\n        x = F.interpolate(x, size=mid_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, mid_feat], dim=1)\r\n        x = self.decoder_conv1(x)\r\n        \r\n        # Upsample and concatenate with low-level features\r\n        x = F.interpolate(x, size=low_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, low_feat], dim=1)\r\n        x = self.decoder_conv2(x)\r\n        \r\n        # Final upsampling to input resolution\r\n        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\r\n        \r\n        # Classifier\r\n        output = self.classifier(x)\r\n        \r\n        if return_features:\r\n            features = {\r\n                'low': low_feat,\r\n                'mid': mid_feat,\r\n                'high': high_feat\r\n            }\r\n            return output, features\r\n        \r\n        return output\r\n\r\n\r\ndef count_parameters(model):\r\n    \"\"\"Count the number of trainable parameters\"\"\"\r\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(\"=\" * 70)\r\n    print(\"Standard MobileNetV3-Small Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Create model\r\n    model = StandardSegmentationModel(num_classes=21, pretrained=False)\r\n    \r\n    # Count parameters\r\n    params = count_parameters(model)\r\n    print(f\"\\nTotal parameters: {params:,} ({params/1e6:.2f}M)\")\r\n    \r\n    # Test forward pass\r\n    print(\"\\nTesting forward pass...\")\r\n    dummy_input = torch.randn(2, 3, 224, 224)\r\n    \r\n    with torch.no_grad():\r\n        output = model(dummy_input)\r\n        print(f\"  Output shape: {output.shape}\")\r\n        \r\n        # Test with feature extraction\r\n        output, features = model(dummy_input, return_features=True)\r\n        print(f\"\\n  Features for knowledge distillation:\")\r\n        for name, feat in features.items():\r\n            print(f\"    {name}: {feat.shape}\")\r\n    \r\n    # Score estimation\r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Score Estimation: Score = 4 × mIoU / (1 + params_M)\")\r\n    print(\"=\" * 70)\r\n    \r\n    for miou in [0.40, 0.45, 0.50, 0.55]:\r\n        score = 4 * miou / (1 + params/1e6)\r\n        print(f\"  mIoU={miou:.2f} → Score={score:.3f}\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n","time":1762545107282},{"kind":"opened","id":4,"time":1762545107282},{"kind":"documentEncountered","id":5,"relativePath":"train.py","time":1762545107284},{"kind":"setContent","id":5,"v":1,"content":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean IoU\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Resize back to original size\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    return mean_iou\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot\r\n    axes[0].plot(history['train_loss'], label='Train Loss')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU')\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","time":1762622282025},{"kind":"opened","id":5,"time":1762622282025},{"kind":"documentEncountered","id":17,"relativePath":"train.txt","time":1762622441478},{"kind":"setContent","id":17,"v":1,"content":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","time":1762622441593},{"kind":"opened","id":17,"time":1762622441593},{"kind":"changed","id":4,"v":2,"edit":[[50,50," (Optimized)"],[94,94,"1."],[95,95,"6M (reduced from 3"],[99,99,")"],[119,120,"8"],[122,123,"1"],[124,125,"2"]],"time":1762623638975},{"kind":"changed","id":4,"v":3,"edit":[[2834,2834," (Optimized)"],[2919,2920,"Lightweight 2"],[2951,2955,""],[2956,2956," - reduced from 3 rates"],[2964,2972,"Simplifi"],[2973,2973,"d"],[2987,2991,"reduced"],[2993,2994,"ha"],[2997,3002,"l"],[3024,3024,"1."],[3025,3025,"6M (reduced from 3"],[3029,3029,")"],[3767,3767,"Lightweight "],[3779,3782,"(2"],[3783,3786,"ra"],[3787,3787,"es "],[3788,3789,"n"],[3790,3790,"tead of 3, redu"],[3791,3793,""],[3794,3794,"d output"],[3796,3797,"han"],[3798,3799,""],[3800,3802,"ls)"],[3859,3862,"64"],[3876,3880,""],[3931,3933,"r"],[3934,3936,""],[3937,3939,"uc"],[3940,3940,"d cha"],[3941,3944,""],[3945,3945,"el"],[4015,4017,"16"],[4063,4065,"16"],[4191,4193,"8"],[4239,4241,"8"],[4310,4310," Simplified"],[4311,4312,"d"],[4324,4328,"reduced"],[4330,4331,"ha"],[4334,4339,"l"],[4409,4412,"64"],[4415,4417,"16"],[4419,4422,"64"],[4479,4482,"64"],[4609,4612,"64"],[4615,4617,"8"],[4619,4621,"32"],[4678,4680,"32"],[4861,4863,"32"]],"time":1762623641194},{"kind":"changed","id":5,"v":2,"edit":[[10251,10251,"_iou:"],[10252,10252,"mean "],[10255,10255," across all samples"],[10261,10261,"    mean_loss: mean cross-entropy loss\r\n    "],[10307,10307,"all_losses = []"],[10313,10313,"criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    "],[10403,10403,"\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss"]],"time":1762623765832},{"kind":"changed","id":5,"v":3,"edit":[[11970,12883,""]],"time":1762623780348},{"kind":"changed","id":5,"v":4,"edit":[[12136,12136," - now includes both train and validation loss"],[12188,12188,"in Loss', l"],[12190,12190,"ewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val"],[12196,12196,", linewidth=2, linestyle='--'"],[12295,12295," and Validation"],[12434,12434,", linewidth=2"],[12645,12645,", dpi=150"]],"time":1762623791607},{"kind":"changed","id":5,"v":5,"edit":[[19126,19126,"loss': [],\r\n        'val_"]],"time":1762623803059},{"kind":"changed","id":5,"v":6,"edit":[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],"time":1762623816438},{"kind":"changed","id":5,"v":7,"edit":[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]],"time":1762623835010},{"kind":"selectionChanged","id":17,"selection":[[883,883]],"time":1762624017851},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624140244},{"kind":"selectionChanged","id":17,"selection":[[869,870]],"time":1762624140330},{"kind":"selectionChanged","id":17,"selection":[[868,870]],"time":1762624140356},{"kind":"selectionChanged","id":17,"selection":[[867,870]],"time":1762624140378},{"kind":"changed","id":17,"v":7,"edit":[[867,870,"1"]],"time":1762624143421},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624143443},{"kind":"changed","id":17,"v":9,"edit":[[868,869,""]],"time":1762624143557},{"kind":"selectionChanged","id":17,"selection":[[849,849]],"time":1762624144669},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624146159},{"kind":"selectionChanged","id":17,"selection":[[869,869]],"time":1762624146967},{"kind":"changed","id":17,"v":13,"edit":[[868,869,""]],"time":1762624150477},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624150514},{"kind":"changed","id":17,"v":15,"edit":[[868,869,""]],"time":1762624151107},{"kind":"selectionChanged","id":17,"selection":[[849,849]],"time":1762624151684},{"kind":"changed","id":17,"v":17,"edit":[[849,849," "]],"time":1762624152194},{"kind":"selectionChanged","id":17,"selection":[[850,850]],"time":1762624152207}]},"resultEdit":"   16  16 \n   17  17 # ============================================================================\n-  18      # STANDARD MODEL (1arameters)\n+      18 # STANDARD MODEL (1.3M parameters)\n   19  19 # ============================================================================\n   20  20 \n   21  21 # Baseline (No Knowledge Distillation)","isCachedResult":false,"prompt":"System\n------\nYour role as an AI assistant is to help developers complete their code tasks by assisting in editing specific sections of code marked by the <|code_to_edit|> and <|/code_to_edit|> tags, while adhering to Microsoft's content policies and avoiding the creation of content that violates copyrights.\n\nYou have access to the following information to help you make informed suggestions:\n\n- recently_viewed_code_snippets: These are code snippets that the developer has recently looked at, which might provide context or examples relevant to the current task. They are listed from oldest to newest, with line numbers in the form #| to help you understand the edit diff history. It's possible these are entirely irrelevant to the developer's change.\n- current_file_content: The content of the file the developer is currently working on, providing the broader context of the code. Line numbers in the form #| are included to help you understand the edit diff history.\n- edit_diff_history: A record of changes made to the code, helping you understand the evolution of the code and the developer's intentions. These changes are listed from oldest to latest. It's possible a lot of old edit diff history is entirely irrelevant to the developer's change.\n- area_around_code_to_edit: The context showing the code surrounding the section to be edited.\n- cursor position marked as <|cursor|>: Indicates where the developer's cursor is currently located, which can be crucial for understanding what part of the code they are focusing on.\n\nYour task is to predict and complete the changes the developer would have made next in the <|code_to_edit|> section. The developer may have stopped in the middle of typing. Your goal is to keep the developer on the path that you think they're following. Some examples include further implementing a class, method, or variable, or improving the quality of the code. Make sure the developer doesn't get distracted and ensure your suggestion is relevant. Consider what changes need to be made next, if any. If you think changes should be made, ask yourself if this is truly what needs to happen. If you are confident about it, then proceed with the changes.\n\n# Steps\n\n1. **Review Context**: Analyze the context from the resources provided, such as recently viewed snippets, edit history, surrounding code, and cursor location.\n2. **Evaluate Current Code**: Determine if the current code within the tags requires any corrections or enhancements.\n3. **Suggest Edits**: If changes are required, ensure they align with the developer's patterns and improve code quality.\n4. **Maintain Consistency**: Ensure indentation and formatting follow the existing code style.\n\n# Output Format\n\n- Provide only the revised code within the tags. If no changes are necessary, simply return the original code from within the <|code_to_edit|> and <|/code_to_edit|> tags.\n- There are line numbers in the form #| in the code displayed to you above, but these are just for your reference. Please do not include the numbers of the form #| in your response.\n- Ensure that you do not output duplicate code that exists outside of these tags. The output should be the revised code that was between these tags and should not include the <|code_to_edit|> or <|/code_to_edit|> tags.\n\n```\n// Your revised code goes here\n```\n\n# Notes\n\n- Apologize with \"Sorry, I can't assist with that.\" for requests that may breach Microsoft content guidelines.\n- Avoid undoing or reverting the developer's last change unless there are obvious typos or errors.\n- Don't include the line numbers of the form #| in your response.\n==================\nUser\n------\n```\n<|recently_viewed_code_snippets|>\n<|recently_viewed_code_snippet|>\ncode_snippet_file_path: /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py (truncated)\n\"\"\"\nTraining Script for Compact Segmentation Models with Knowledge Distillation\nELEC 475 Lab 3\n\nSupports three training modes:\n1. No Knowledge Distillation (baseline)\n2. Response-based Knowledge Distillation\n3. Feature-based Knowledge Distillation\n\nReferences:\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\n\"\"\"\n\nimport argparse\nimport os\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import VOCSegmentation\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\nfrom tqdm import tqdm\n\n# Import custom models\nfrom model_ultracompact import UltraCompactSegmentationModel\nfrom model_standard import StandardSegmentationModel\n\n\nclass VOCNormalize:\n    \"\"\"Custom normalization for VOC dataset\"\"\"\n    def __init__(self, augment=True):\n        self.augment = augment\n        self.normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    \n    def __call__(self, img, target):\n        # Convert PIL to tensor\n        img = transforms.ToTensor()(img)\n        \n        # Apply augmentation if training\n        if self.augment and np.random.rand() > 0.5:\n            # Random horizontal flip\n            img = transforms.functional.hflip(img)\n            target = transforms.functional.hflip(target)\n        \n        # Normalize image\n        img = self.normalize(img)\n        \n        # Convert target to tensor\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n        \n        return img, target\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\n    images, targets = zip(*batch)\n    return list(images), list(targets)\n\n\ndef calculate_miou(pred, target, num_classes=21):\n    \"\"\"\n    Calculate mean Intersection over Union (mIoU)\n    \n    Args:\n        pred: predicted segmentation mask (H, W)\n        target: ground truth segmentation mask (H, W)\n        num_classes: number of classes\n    \n    Returns:\n        mIoU value\n    \"\"\"\n    pred = pred.cpu().numpy()\n    target = target.cpu().numpy()\n    \n    ious = []\n    for cls in range(num_classes):\n        pred_mask = (pred == cls)\n        target_mask = (target == cls)\n        \n        intersection = np.logical_and(pred_mask, target_mask).sum()\n        union = np.logical_or(pred_mask, target_mask).sum()\n        \n        if union == 0:\n            continue\n        \n        iou = intersection / union\n        ious.append(iou)\n    \n    return np.mean(ious) if len(ious) > 0 else 0.0\n\n\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\n    \"\"\"\n    Response-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        teacher_logits: teacher model output logits (B, C, H, W)\n        targets: ground truth labels (B, H, W)\n        temperature: softmax temperature for distillation\n        alpha: weight for cross-entropy loss\n        beta: weight for distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Distillation loss (KL divergence)\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\n    \n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * kd_loss\n    \n    return total_loss, ce_loss.item(), kd_loss.item()\n\n\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\n    \"\"\"\n    Feature-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        student_features: dict of student intermediate features\n        teacher_features: dict of teacher intermediate features\n        targets: ground truth labels (B, H, W)\n        alpha: weight for cross-entropy loss\n        beta: weight for feature distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Feature matching loss (cosine similarity)\n    feat_loss = 0\n    num_levels = 0\n    \n    for level in ['low', 'mid', 'high']:\n        if level in student_features and level in teacher_features:\n            s_feat = student_features[level]\n            t_feat = teacher_features[level]\n            \n            # Resize if needed (teacher features might be different size)\n            if s_feat.shape != t_feat.shape:\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n                # Match channels if needed\n                if s_feat.shape[1] != t_feat.shape[1]:\n                    # Project teacher features to student channel size\n                    continue  # Skip if channel mismatch (or add projection layer)\n            \n            # Cosine similarity loss (1 - cosine_similarity)\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\n            \n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\n            feat_loss += (1 - cosine_sim)\n            num_levels += 1\n    \n    if num_levels > 0:\n        feat_loss = feat_loss / num_levels\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * feat_loss\n    \n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\n\n\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\n    \"\"\"\n    Train for one epoch\n    \n    Args:\n        model: student model\n        teacher_model: teacher model (None if kd_mode='none')\n        dataloader: training data loader\n        optimizer: optimizer\n        device: device to run on\n        kd_mode: 'none', 'response', or 'feature'\n        args: training arguments\n    \n    Returns:\n        average loss, average ce_loss, average kd_loss\n    \"\"\"\n    model.train()\n    if teacher_model is not None:\n        teacher_model.eval()\n    \n    total_loss = 0\n    total_ce_loss = 0\n    total_kd_loss = 0\n    num_batches = 0\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for images, targets in pbar:\n        # Move to device and resize to fixed size for batching\n        batch_imgs = []\n        batch_targets = []\n        \n        for img, target in zip(images, targets):\n            img = img.to(device)\n            target = target.to(device)\n            \n            # Resize to fixed size\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\n            \n            batch_imgs.append(img)\n            batch_targets.append(target)\n        \n        batch_imgs = torch.stack(batch_imgs)\n<|/recently_viewed_code_snippet|>\n<|/recently_viewed_code_snippets|>\n\n<|current_file_content|>\ncurrent_file_path: train.txt\n# ELEC 475 Lab 3 - Training Commands\n# All commands should be run from the Lab 3 directory\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n <|cursor|># STANDARD MODEL (1arameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\n\n# ============================================================================\n# OPTIONAL: HYPERPARAMETER TUNING\n# ============================================================================\n# If you want to experiment with different hyperparameters:\n\n# Adjust learning rate\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\n\n# Adjust KD temperature (higher = softer targets)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\n\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\n\n# Adjust batch size (if memory issues)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\n\n# ============================================================================\n# NOTES\n# ============================================================================\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\n# - Monitor validation mIoU - training should converge after 30-40 epochs\n# - Best model is saved automatically when validation mIoU improves\n\n<|/current_file_content|>\n\n<|edit_diff_history|>\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -342,23 +342,0 @@\n-            for img, target in zip(images, targets):\n-                img = img.unsqueeze(0).to(device)\n-                target = target.to(device)\n-                \n-                # Resize to fixed size\n-                h, w = target.shape\n-                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\n-                \n-                # Forward pass\n-                output = model(img)\n-                \n-                # Resize back to original size\n-                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\n-                \n-                # Get predictions\n-                pred = output.squeeze(0).argmax(0)\n-                \n-                # Calculate IoU\n-                iou = calculate_miou(pred, target, num_classes)\n-                all_ious.append(iou)\n-    \n-    mean_iou = np.mean(all_ious)\n-    return mean_iou\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -348,1 +348,1 @@\n-    # Loss plot\n+    # Loss plot - now includes both train and validation loss\n@@ -349,1 +349,2 @@\n-    axes[0].plot(history['train_loss'], label='Train Loss')\n+    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n+    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\n@@ -352,1 +352,1 @@\n-    axes[0].set_title('Training Loss')\n+    axes[0].set_title('Training and Validation Loss')\n@@ -357,1 +357,1 @@\n-    axes[1].plot(history['val_miou'], label='Validation mIoU')\n+    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\n@@ -365,1 +365,1 @@\n-    plt.savefig(save_path)\n+    plt.savefig(save_path, dpi=150)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -535,1 +535,2 @@\n-        'val_miou': [],\n+        'val_loss': [],\n+        'val_miou': [],\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -581,1 +581,1 @@\n-        val_miou = validate(model, val_loader, device)\n+        val_miou, val_loss = validate(model, val_loader, device)\n@@ -582,1 +582,1 @@\n-        print(f\"Validation mIoU: {val_miou:.4f}\")\n+        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")\n@@ -592,1 +592,2 @@\n-        history['train_kd_loss'].append(train_kd_loss)\n+        history['train_kd_loss'].append(train_kd_loss)\n+        history['val_loss'].append(val_loss)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -652,1 +652,2 @@\n-        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n@@ -17,1 +17,1 @@\n-# STANDARD MODEL (3.4M parameters)\n+ # STANDARD MODEL (1arameters)\n\n<|/edit_diff_history|>\n\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n <|cursor|># STANDARD MODEL (1arameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n```\n\nThe developer was working on a section of code within the tags `code_to_edit` in the file located at `train.txt`. Using the given `recently_viewed_code_snippets`, `current_file_content`, `edit_diff_history`, `area_around_code_to_edit`, and the cursor position marked as `<|cursor|>`, please continue the developer's work. Update the `code_to_edit` section by predicting and completing the changes they would have made next. Provide the revised code that was between the `<|code_to_edit|>` and `<|/code_to_edit|>` tags with the following format, but do not include the tags themselves.\n```\n// Your revised code goes here\n```\n==================","error":"undefined","response":"```\n\n# ============================================================================\n# STANDARD MODEL (1.3M parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n```","responseResults":"- replaceRange:\n   start: 849\n   endExclusive: 879\n  newText: \"# STANDARD MODEL (1.3M parameters)\"\n","providerStartTime":1762624152335,"providerEndTime":1762624152548,"fetchStartTime":1762624152545,"fetchEndTime":1762624153155,"logs":["```\n{\n\t\"isAsyncCompletions\": true,\n\t\"isRevisedCacheStrategy\": true,\n\t\"isCacheTracksRejections\": true,\n\t\"isRecentlyShownCacheEnabled\": false,\n\t\"debounceUseCoreRequestTime\": false\n}\n```\n","TTFT 219.57509998977184 ms\n","Line 0 emitted with latency 233.16459999978542 ms\n","Line 1 emitted with latency 233.19539999961853 ms\n","Line 2 emitted with latency 296.133499994874 ms\n","Line 3 emitted with latency 296.8477999866009 ms\n","pushing edit #0:\\n[18,19)->[\"# STANDARD MODEL (1.3M parameters)\"]\n","First edit latency: 514 ms\n","Line 4 emitted with latency 389.0992999970913 ms\n","Line 5 emitted with latency 389.1343999952078 ms\n","Line 6 emitted with latency 483.14559999108315 ms\n","Line 7 emitted with latency 605.542999997735 ms\n","Line 8 emitted with latency 605.5742999911308 ms\n","Line 9 emitted with latency 609.7521999925375 ms\n"]}
{"requestId":13,"time":1762624165799,"filePath":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","version":13,"statelessNextEditProviderId":"XtabProvider","nextEditRequest":{"id":"f6fd6dbc-1bce-48ca-a1c9-82b4e7e68029","documents":[{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.py","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"python","documentLinesBeforeEdit":["\"\"\"","Training Script for Compact Segmentation Models with Knowledge Distillation","ELEC 475 Lab 3","","Supports three training modes:","1. No Knowledge Distillation (baseline)","2. Response-based Knowledge Distillation","3. Feature-based Knowledge Distillation","","References:","- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)","- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)","\"\"\"","","import argparse","import os","import time","from pathlib import Path","import numpy as np","import matplotlib.pyplot as plt","","import torch","import torch.nn as nn","import torch.nn.functional as F","from torch.utils.data import DataLoader","from torchvision import transforms","from torchvision.datasets import VOCSegmentation","from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights","from tqdm import tqdm","","# Import custom models","from model_ultracompact import UltraCompactSegmentationModel","from model_standard import StandardSegmentationModel","","","class VOCNormalize:","    \"\"\"Custom normalization for VOC dataset\"\"\"","    def __init__(self, augment=True):","        self.augment = augment","        self.normalize = transforms.Normalize(","            mean=[0.485, 0.456, 0.406],","            std=[0.229, 0.224, 0.225]","        )","    ","    def __call__(self, img, target):","        # Convert PIL to tensor","        img = transforms.ToTensor()(img)","        ","        # Apply augmentation if training","        if self.augment and np.random.rand() > 0.5:","            # Random horizontal flip","            img = transforms.functional.hflip(img)","            target = transforms.functional.hflip(target)","        ","        # Normalize image","        img = self.normalize(img)","        ","        # Convert target to tensor","        target = torch.as_tensor(np.array(target), dtype=torch.int64)","        ","        return img, target","","","def collate_fn(batch):","    \"\"\"Custom collate function to handle variable-sized images\"\"\"","    images, targets = zip(*batch)","    return list(images), list(targets)","","","def calculate_miou(pred, target, num_classes=21):","    \"\"\"","    Calculate mean Intersection over Union (mIoU)","    ","    Args:","        pred: predicted segmentation mask (H, W)","        target: ground truth segmentation mask (H, W)","        num_classes: number of classes","    ","    Returns:","        mIoU value","    \"\"\"","    pred = pred.cpu().numpy()","    target = target.cpu().numpy()","    ","    ious = []","    for cls in range(num_classes):","        pred_mask = (pred == cls)","        target_mask = (target == cls)","        ","        intersection = np.logical_and(pred_mask, target_mask).sum()","        union = np.logical_or(pred_mask, target_mask).sum()","        ","        if union == 0:","            continue","        ","        iou = intersection / union","        ious.append(iou)","    ","    return np.mean(ious) if len(ious) > 0 else 0.0","","","def response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):","    \"\"\"","    Response-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        teacher_logits: teacher model output logits (B, C, H, W)","        targets: ground truth labels (B, H, W)","        temperature: softmax temperature for distillation","        alpha: weight for cross-entropy loss","        beta: weight for distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Distillation loss (KL divergence)","    student_soft = F.log_softmax(student_logits / temperature, dim=1)","    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)","    ","    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * kd_loss","    ","    return total_loss, ce_loss.item(), kd_loss.item()","","","def feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):","    \"\"\"","    Feature-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        student_features: dict of student intermediate features","        teacher_features: dict of teacher intermediate features","        targets: ground truth labels (B, H, W)","        alpha: weight for cross-entropy loss","        beta: weight for feature distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Feature matching loss (cosine similarity)","    feat_loss = 0","    num_levels = 0","    ","    for level in ['low', 'mid', 'high']:","        if level in student_features and level in teacher_features:","            s_feat = student_features[level]","            t_feat = teacher_features[level]","            ","            # Resize if needed (teacher features might be different size)","            if s_feat.shape != t_feat.shape:","                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)","                # Match channels if needed","                if s_feat.shape[1] != t_feat.shape[1]:","                    # Project teacher features to student channel size","                    continue  # Skip if channel mismatch (or add projection layer)","            ","            # Cosine similarity loss (1 - cosine_similarity)","            s_feat_norm = F.normalize(s_feat, p=2, dim=1)","            t_feat_norm = F.normalize(t_feat, p=2, dim=1)","            ","            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()","            feat_loss += (1 - cosine_sim)","            num_levels += 1","    ","    if num_levels > 0:","        feat_loss = feat_loss / num_levels","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * feat_loss","    ","    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0","","","def train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):","    \"\"\"","    Train for one epoch","    ","    Args:","        model: student model","        teacher_model: teacher model (None if kd_mode='none')","        dataloader: training data loader","        optimizer: optimizer","        device: device to run on","        kd_mode: 'none', 'response', or 'feature'","        args: training arguments","    ","    Returns:","        average loss, average ce_loss, average kd_loss","    \"\"\"","    model.train()","    if teacher_model is not None:","        teacher_model.eval()","    ","    total_loss = 0","    total_ce_loss = 0","    total_kd_loss = 0","    num_batches = 0","    ","    pbar = tqdm(dataloader, desc='Training')","    for images, targets in pbar:","        # Move to device and resize to fixed size for batching","        batch_imgs = []","        batch_targets = []","        ","        for img, target in zip(images, targets):","            img = img.to(device)","            target = target.to(device)","            ","            # Resize to fixed size","            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)","            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()","            ","            batch_imgs.append(img)","            batch_targets.append(target)","        ","        batch_imgs = torch.stack(batch_imgs)","        batch_targets = torch.stack(batch_targets)","        ","        optimizer.zero_grad()","        ","        # Forward pass based on KD mode","        if kd_mode == 'none':","            # No knowledge distillation","            output = model(batch_imgs)","            loss = F.cross_entropy(output, batch_targets, ignore_index=255)","            ce_loss_val = loss.item()","            kd_loss_val = 0.0","            ","        elif kd_mode == 'response':","            # Response-based KD","            output = model(batch_imgs)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","            ","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","            ","        elif kd_mode == 'feature':","            # Feature-based KD","            output, student_features = model(batch_imgs, return_features=True)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)","                # For now, use output-based distillation","                teacher_features = {}  # Would need to extract from teacher","            ","            # Fallback to response-based if teacher features not available","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","        ","        # Backward pass","        loss.backward()","        optimizer.step()","        ","        total_loss += loss.item()","        total_ce_loss += ce_loss_val","        total_kd_loss += kd_loss_val","        num_batches += 1","        ","        pbar.set_postfix({'loss': f'{loss.item():.4f}'})","    ","    avg_loss = total_loss / num_batches","    avg_ce_loss = total_ce_loss / num_batches","    avg_kd_loss = total_kd_loss / num_batches","    ","    return avg_loss, avg_ce_loss, avg_kd_loss","","","def validate(model, dataloader, device, num_classes=21):","    \"\"\"","    Validate the model","    ","    Args:","        model: model to validate","        dataloader: validation data loader","        device: device to run on","        num_classes: number of classes","    ","    Returns:","        mean_iou: mean IoU across all samples","        mean_loss: mean cross-entropy loss","    \"\"\"","    model.eval()","    all_ious = []","    all_losses = []","    criterion = nn.CrossEntropyLoss(ignore_index=255)","    ","    with torch.no_grad():","        for images, targets in tqdm(dataloader, desc='Validation'):","            for img, target in zip(images, targets):","                img = img.unsqueeze(0).to(device)","                target = target.to(device)","                ","                # Resize to fixed size","                h, w = target.shape","                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)","                ","                # Forward pass","                output = model(img)","                ","                # Calculate loss at this resolution","                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), ","                                               size=output.shape[-2:], ","                                               mode='nearest').squeeze().long()","                loss = criterion(output, target_resized.unsqueeze(0))","                all_losses.append(loss.item())","                ","                # Resize back to original size for IoU calculation","                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)","                ","                # Get predictions","                pred = output.squeeze(0).argmax(0)","                ","                # Calculate IoU","                iou = calculate_miou(pred, target, num_classes)","                all_ious.append(iou)","    ","    mean_iou = np.mean(all_ious)","    mean_loss = np.mean(all_losses)","    return mean_iou, mean_loss","","","def plot_training_curves(history, save_path):","    \"\"\"Plot and save training curves\"\"\"","    fig, axes = plt.subplots(1, 2, figsize=(12, 4))","    ","    # Loss plot - now includes both train and validation loss","    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)","    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')","    axes[0].set_xlabel('Epoch')","    axes[0].set_ylabel('Loss')","    axes[0].set_title('Training and Validation Loss')","    axes[0].legend()","    axes[0].grid(True)","    ","    # mIoU plot","    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)","    axes[1].set_xlabel('Epoch')","    axes[1].set_ylabel('mIoU')","    axes[1].set_title('Validation mIoU')","    axes[1].legend()","    axes[1].grid(True)","    ","    plt.tight_layout()","    plt.savefig(save_path, dpi=150)","    plt.close()","    print(f\"Training curves saved to {save_path}\")","","","def main(args):","    \"\"\"Main training function\"\"\"","    ","    print(\"=\" * 70)","    print(\"Training Compact Segmentation Model\")","    print(\"=\" * 70)","    print(f\"Model: {args.model}\")","    print(f\"KD Mode: {args.kd_mode}\")","    print(f\"Epochs: {args.epochs}\")","    print(f\"Batch Size: {args.batch_size}\")","    print(f\"Learning Rate: {args.lr}\")","    print(\"=\" * 70)","    ","    # Set device","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","    print(f\"\\nUsing device: {device}\")","    ","    # Create save directory","    save_dir = Path(args.save_dir)","    save_dir.mkdir(parents=True, exist_ok=True)","    ","    # Load student model","    print(\"\\nLoading student model...\")","    if args.model == 'ultracompact':","        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)","    elif args.model == 'standard':","        model = StandardSegmentationModel(num_classes=21, pretrained=True)","    else:","        raise ValueError(f\"Unknown model: {args.model}\")","    ","    model = model.to(device)","    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)","    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")","    ","    # Load teacher model if using KD","    teacher_model = None","    if args.kd_mode != 'none':","        print(\"\\nLoading teacher model (FCN-ResNet50)...\")","        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1","        teacher_model = fcn_resnet50(weights=weights)","        teacher_model = teacher_model.to(device)","        teacher_model.eval()","        for param in teacher_model.parameters():","            param.requires_grad = False","        print(\"Teacher model loaded and frozen\")","    ","    # Prepare datasets","    print(\"\\nPreparing datasets...\")","    ","    # Find dataset path (Google Colab compatible)","    # VOCSegmentation expects: root/VOCdevkit/VOC2012/","    dataset_roots = []","    ","    # Check for Google Colab structure (both with and without VOCdevkit wrapper)","    colab_train_path = Path('/content/data/VOC2012_train_val')","    if colab_train_path.exists():","        # Check if it has VOCdevkit structure","        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(colab_train_path))","        # Check if files are directly in the folder (no VOCdevkit wrapper)","        elif (colab_train_path / 'JPEGImages').exists():","            # Create a symlink or use parent structure","            dataset_roots.append(str(colab_train_path.parent))","    ","    # Check for local structure","    local_data_path = Path('./data')","    if local_data_path.exists():","        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(local_data_path))","        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():","            dataset_roots.append(str(local_data_path))","    ","    # Check standard paths","    dataset_roots.extend([","        '/content/data',  # Google Colab default","        './data',  # Local path","        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),","    ])","    ","    # Check dataset_path.txt","    dataset_path_file = Path('dataset_path.txt')","    if dataset_path_file.exists():","        with open(dataset_path_file, 'r') as f:","            saved_path = Path(f.read().strip())","            dataset_roots.insert(0, str(saved_path.parent.parent))","    ","    dataset_root = None","    for root in dataset_roots:","        try:","            # Try to create a temporary symlink structure if needed","            root_path = Path(root)","            ","            # Check if we need to create VOCdevkit/VOC2012 structure","            voc_path = root_path / 'VOC2012_train_val'","            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():","                # Files are directly in VOC2012_train_val, create temporary structure","                vocdevkit_path = voc_path / 'VOCdevkit'","                voc2012_path = vocdevkit_path / 'VOC2012'","                if not voc2012_path.exists():","                    print(f\"Creating VOCdevkit structure in {voc_path}\")","                    vocdevkit_path.mkdir(exist_ok=True)","                    voc2012_path.symlink_to(voc_path, target_is_directory=True)","            ","            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)","            dataset_root = root","            print(f\"✓ Successfully loaded dataset from: {root}\")","            break","        except Exception as e:","            continue","    ","    if dataset_root is None:","        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")","    ","    print(f\"Dataset found at: {dataset_root}\")","    ","    # Create datasets","    train_transform = VOCNormalize(augment=True)","    val_transform = VOCNormalize(augment=False)","    ","    train_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='train',","        download=False,","        transforms=train_transform","    )","    ","    val_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='val',","        download=False,","        transforms=val_transform","    )","    ","    print(f\"Training set size: {len(train_dataset)}\")","    print(f\"Validation set size: {len(val_dataset)}\")","    ","    # Create dataloaders","    train_loader = DataLoader(","        train_dataset,","        batch_size=args.batch_size,","        shuffle=True,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    val_loader = DataLoader(","        val_dataset,","        batch_size=4,","        shuffle=False,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    # Setup optimizer and scheduler","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)","    ","    # Training history","    history = {","        'train_loss': [],","        'train_ce_loss': [],","        'train_kd_loss': [],","        'val_loss': [],","        'val_miou': [],","        'epoch_times': []","    }","    ","    # Save hyperparameters","    hyperparameters = {","        'model': args.model,","        'kd_mode': args.kd_mode,","        'epochs': args.epochs,","        'batch_size': args.batch_size,","        'learning_rate': args.lr,","        'weight_decay': args.weight_decay,","        'temperature': args.temperature,","        'alpha': args.alpha,","        'beta': args.beta,","        'optimizer': 'Adam',","        'scheduler': 'CosineAnnealingLR',","        'num_parameters': sum(p.numel() for p in model.parameters()),","        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)","    }","    ","    best_miou = 0.0","    training_start_time = time.time()","    ","    # Training loop","    print(\"\\nStarting training...\")","    print(\"=\" * 70)","    print(\"\\nHyperparameters:\")","    for key, value in hyperparameters.items():","        print(f\"  {key}: {value}\")","    ","    for epoch in range(args.epochs):","        epoch_start_time = time.time()","        ","        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")","        print(\"-\" * 70)","        ","        # Train","        train_loss, train_ce_loss, train_kd_loss = train_epoch(","            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args","        )","        ","        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")","        ","        # Validate","        val_miou = validate(model, val_loader, device)","        print(f\"Validation mIoU: {val_miou:.4f}\")","        ","        # Record epoch time","        epoch_time = time.time() - epoch_start_time","        history['epoch_times'].append(epoch_time)","        print(f\"Epoch time: {epoch_time:.2f} seconds\")","        ","        # Update history","        history['train_loss'].append(train_loss)","        history['train_ce_loss'].append(train_ce_loss)","        history['train_kd_loss'].append(train_kd_loss)","        history['val_miou'].append(val_miou)","        ","        # Save best model","        if val_miou > best_miou:","            best_miou = val_miou","            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history","            }, save_path)","            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")","        ","        # Update scheduler","        scheduler.step()","        ","        # Save checkpoint every 10 epochs","        if (epoch + 1) % 10 == 0:","            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history,","                'hyperparameters': hyperparameters","            }, checkpoint_path)","            print(f\"Checkpoint saved: {checkpoint_path}\")","    ","    # Calculate total training time","    total_training_time = time.time() - training_start_time","    avg_epoch_time = np.mean(history['epoch_times'])","    ","    # Plot training curves","    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"","    plot_training_curves(history, plot_path)","    ","    # Save final training report","    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"","    with open(report_path, 'w') as f:","        f.write(\"=\" * 70 + \"\\n\")","        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")","        f.write(\"=\" * 70 + \"\\n\\n\")","        ","        f.write(\"HYPERPARAMETERS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        for key, value in hyperparameters.items():","            f.write(f\"  {key:25s}: {value}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"TRAINING STATISTICS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")","        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")","        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")","        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")","        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"FILES GENERATED:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")","        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")","        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")","        f.write(\"=\" * 70 + \"\\n\")","    ","    print(\"\\n\" + \"=\" * 70)","    print(\"Training completed!\")","    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")","    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")","    print(f\"Best validation mIoU: {best_miou:.4f}\")","    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")","    print(f\"Training report saved to: {report_path}\")","    ","    # Calculate final score","    score = 4 * best_miou / (1 + num_params / 1e6)","    print(f\"\\nFinal Score: {score:.3f}\")","    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")","    print(\"=\" * 70)","","","if __name__ == '__main__':","    parser = argparse.ArgumentParser(description='Train compact segmentation model')","    ","    # Model arguments","    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],","                        help='Model type to train')","    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],","                        help='Knowledge distillation mode')","    ","    # Training arguments","    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')","    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')","    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')","    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')","    ","    # KD arguments","    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')","    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')","    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')","    ","    # Other arguments","    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')","    ","    args = parser.parse_args()","    ","    main(args)",""],"recentEdit":[[582,583,["        val_miou, val_loss = validate(model, val_loader, device)"]],[583,584,["        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")"]],[594,594,["        history['val_loss'].append(val_loss)"]],[652,652,["        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")"]]],"documentBeforeEdits":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean_iou: mean IoU across all samples\r\n        mean_loss: mean cross-entropy loss\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    all_losses = []\r\n    criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot - now includes both train and validation loss\r\n    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training and Validation Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path, dpi=150)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","recentEdits":[[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]]],"lastSelectionInAfterEdit":[18491,18491]},{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"plaintext","documentLinesBeforeEdit":["# ELEC 475 Lab 3 - Training Commands","# All commands should be run from the Lab 3 directory","","# ============================================================================","# ULTRA-COMPACT MODEL (475K parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# STANDARD MODEL (3.4M parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# TRAINING OUTPUT FILES","# ============================================================================","# For each training run, the following files will be generated in ./checkpoints/:","#   - best_model_{model}_{kd_mode}.pth           (best model weights)","#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)","#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)","#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)","","# ============================================================================","# OPTIONAL: HYPERPARAMETER TUNING","# ============================================================================","# If you want to experiment with different hyperparameters:","","# Adjust learning rate","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005","","# Adjust KD temperature (higher = softer targets)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0","","# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3","","# Adjust batch size (if memory issues)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4","","# ============================================================================","# NOTES","# ============================================================================","# - Training on CPU will be slow. Consider using Google Colab with GPU.","# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU","# - Monitor validation mIoU - training should converge after 30-40 epochs","# - Best model is saved automatically when validation mIoU improves",""],"recentEdit":[[18,19,["# STANDARD MODEL (1.75arameters)"]]],"documentBeforeEdits":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","recentEdits":[[[867,873,"1"]],[[849,849," "]],[[849,879,"# STANDARD MODEL (1arameters)"]],[[868,868,".75"]]],"lastSelectionInAfterEdit":[871,871]}],"activeDocumentIdx":1,"recording":[{"documentType":"workspaceRecording@1.0","kind":"header","repoRootUri":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","time":1762624165825,"uuid":"888250d7-117b-4c62-a2c9-81f68c72a05a"},{"kind":"documentEncountered","id":4,"relativePath":"model_standard.py","time":1762545107282},{"kind":"setContent","id":4,"v":1,"content":"\"\"\"\r\nStandard MobileNetV3-Small Segmentation Model\r\nELEC 475 Lab 3 - Option 1\r\n\r\nParameters: ~3.41M\r\nExpected Score: 0.40-0.50\r\n\r\nReferences:\r\n- MobileNetV3: Searching for MobileNetV3 (Howard et al., 2019)\r\n- DeepLabV3+: Encoder-Decoder with Atrous Separable Convolution (Chen et al., 2018)\r\n- ASPP: Rethinking Atrous Convolution for Semantic Image Segmentation (Chen et al., 2017)\r\n\"\"\"\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\r\n\r\n\r\nclass ASPP(nn.Module):\r\n    \"\"\"\r\n    Atrous Spatial Pyramid Pooling (ASPP) module\r\n    Captures multi-scale context using parallel atrous convolutions\r\n    \"\"\"\r\n    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\r\n        super(ASPP, self).__init__()\r\n        \r\n        self.branches = nn.ModuleList()\r\n        \r\n        # 1x1 convolution branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Atrous convolution branches\r\n        for rate in rates:\r\n            self.branches.append(nn.Sequential(\r\n                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\r\n                nn.BatchNorm2d(out_channels),\r\n                nn.ReLU(inplace=True)\r\n            ))\r\n        \r\n        # Global average pooling branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.AdaptiveAvgPool2d(1),\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Combine all branches\r\n        num_branches = len(rates) + 2  # 1x1 + atrous branches + global pooling\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(out_channels * num_branches, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Dropout(0.5)\r\n        )\r\n    \r\n    def forward(self, x):\r\n        h, w = x.shape[2:]\r\n        branch_outputs = []\r\n        \r\n        for i, branch in enumerate(self.branches):\r\n            out = branch(x)\r\n            # Upsample global pooling branch to match spatial dimensions\r\n            if i == len(self.branches) - 1:\r\n                out = F.interpolate(out, size=(h, w), mode='bilinear', align_corners=False)\r\n            branch_outputs.append(out)\r\n        \r\n        # Concatenate all branches\r\n        x = torch.cat(branch_outputs, dim=1)\r\n        x = self.project(x)\r\n        \r\n        return x\r\n\r\n\r\nclass StandardSegmentationModel(nn.Module):\r\n    \"\"\"\r\n    Standard compact segmentation model using MobileNetV3-Small backbone\r\n    \r\n    Architecture:\r\n    - Full MobileNetV3-Small backbone (pretrained)\r\n    - 3-rate ASPP module (rates: 6, 12, 18)\r\n    - Two-stage decoder with skip connections\r\n    - Parameters: ~3.41M\r\n    \"\"\"\r\n    def __init__(self, num_classes=21, pretrained=True):\r\n        super(StandardSegmentationModel, self).__init__()\r\n        \r\n        # Load pretrained MobileNetV3-Small\r\n        if pretrained:\r\n            weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1\r\n            backbone = mobilenet_v3_small(weights=weights)\r\n        else:\r\n            backbone = mobilenet_v3_small(weights=None)\r\n        \r\n        # Extract feature layers from MobileNetV3-Small\r\n        self.features = backbone.features\r\n        \r\n        # Feature dimensions at tap points:\r\n        # Stage 1 (low):  16 channels, stride 4\r\n        # Stage 3 (mid):  24 channels, stride 8\r\n        # Stage 12 (high): 576 channels, stride 16\r\n        \r\n        # ASPP module for multi-scale context\r\n        self.aspp = ASPP(in_channels=576, out_channels=128, rates=[6, 12, 18])\r\n        \r\n        # Feature projection layers (keep dimensions)\r\n        self.mid_conv = nn.Sequential(\r\n            nn.Conv2d(24, 24, 1, bias=False),\r\n            nn.BatchNorm2d(24),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.low_conv = nn.Sequential(\r\n            nn.Conv2d(16, 16, 1, bias=False),\r\n            nn.BatchNorm2d(16),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Decoder with skip connections\r\n        self.decoder_conv1 = nn.Sequential(\r\n            nn.Conv2d(128 + 24, 128, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(128),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.decoder_conv2 = nn.Sequential(\r\n            nn.Conv2d(128 + 16, 64, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(64),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Classifier head\r\n        self.classifier = nn.Sequential(\r\n            nn.Dropout(0.5),\r\n            nn.Conv2d(64, num_classes, 1)\r\n        )\r\n        \r\n        self._init_weights()\r\n    \r\n    def _init_weights(self):\r\n        \"\"\"Initialize weights for new layers\"\"\"\r\n        for m in [self.aspp, self.mid_conv, self.low_conv, self.decoder_conv1, self.decoder_conv2, self.classifier]:\r\n            for module in m.modules():\r\n                if isinstance(module, nn.Conv2d):\r\n                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\r\n                    if module.bias is not None:\r\n                        nn.init.constant_(module.bias, 0)\r\n                elif isinstance(module, nn.BatchNorm2d):\r\n                    nn.init.constant_(module.weight, 1)\r\n                    nn.init.constant_(module.bias, 0)\r\n    \r\n    def forward(self, x, return_features=False):\r\n        \"\"\"\r\n        Forward pass\r\n        \r\n        Args:\r\n            x: input image tensor (B, 3, H, W)\r\n            return_features: if True, return intermediate features for knowledge distillation\r\n            \r\n        Returns:\r\n            output: segmentation logits (B, num_classes, H, W)\r\n            features (optional): dict of intermediate features for KD\r\n        \"\"\"\r\n        input_shape = x.shape[-2:]\r\n        \r\n        # Extract multi-scale features from backbone\r\n        low_feat = None   # stride 4\r\n        mid_feat = None   # stride 8\r\n        high_feat = None  # stride 16\r\n        \r\n        # Forward through MobileNetV3 stages\r\n        for i, layer in enumerate(self.features):\r\n            x = layer(x)\r\n            \r\n            # Capture features at specific stages\r\n            if i == 1:  # After first inverted residual block (stride 4)\r\n                low_feat = x\r\n            elif i == 3:  # After third inverted residual block (stride 8)\r\n                mid_feat = x\r\n        \r\n        # Final high-level features (stride 16)\r\n        high_feat = x\r\n        \r\n        # Process features\r\n        mid_feat = self.mid_conv(mid_feat)\r\n        low_feat = self.low_conv(low_feat)\r\n        \r\n        # ASPP for multi-scale context\r\n        x = self.aspp(high_feat)\r\n        \r\n        # Decoder with skip connections\r\n        # Upsample and concatenate with mid-level features\r\n        x = F.interpolate(x, size=mid_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, mid_feat], dim=1)\r\n        x = self.decoder_conv1(x)\r\n        \r\n        # Upsample and concatenate with low-level features\r\n        x = F.interpolate(x, size=low_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, low_feat], dim=1)\r\n        x = self.decoder_conv2(x)\r\n        \r\n        # Final upsampling to input resolution\r\n        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\r\n        \r\n        # Classifier\r\n        output = self.classifier(x)\r\n        \r\n        if return_features:\r\n            features = {\r\n                'low': low_feat,\r\n                'mid': mid_feat,\r\n                'high': high_feat\r\n            }\r\n            return output, features\r\n        \r\n        return output\r\n\r\n\r\ndef count_parameters(model):\r\n    \"\"\"Count the number of trainable parameters\"\"\"\r\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(\"=\" * 70)\r\n    print(\"Standard MobileNetV3-Small Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Create model\r\n    model = StandardSegmentationModel(num_classes=21, pretrained=False)\r\n    \r\n    # Count parameters\r\n    params = count_parameters(model)\r\n    print(f\"\\nTotal parameters: {params:,} ({params/1e6:.2f}M)\")\r\n    \r\n    # Test forward pass\r\n    print(\"\\nTesting forward pass...\")\r\n    dummy_input = torch.randn(2, 3, 224, 224)\r\n    \r\n    with torch.no_grad():\r\n        output = model(dummy_input)\r\n        print(f\"  Output shape: {output.shape}\")\r\n        \r\n        # Test with feature extraction\r\n        output, features = model(dummy_input, return_features=True)\r\n        print(f\"\\n  Features for knowledge distillation:\")\r\n        for name, feat in features.items():\r\n            print(f\"    {name}: {feat.shape}\")\r\n    \r\n    # Score estimation\r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Score Estimation: Score = 4 × mIoU / (1 + params_M)\")\r\n    print(\"=\" * 70)\r\n    \r\n    for miou in [0.40, 0.45, 0.50, 0.55]:\r\n        score = 4 * miou / (1 + params/1e6)\r\n        print(f\"  mIoU={miou:.2f} → Score={score:.3f}\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n","time":1762545107282},{"kind":"opened","id":4,"time":1762545107282},{"kind":"documentEncountered","id":5,"relativePath":"train.py","time":1762545107284},{"kind":"setContent","id":5,"v":1,"content":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean IoU\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Resize back to original size\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    return mean_iou\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot\r\n    axes[0].plot(history['train_loss'], label='Train Loss')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU')\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","time":1762622282025},{"kind":"opened","id":5,"time":1762622282025},{"kind":"documentEncountered","id":17,"relativePath":"train.txt","time":1762622441478},{"kind":"setContent","id":17,"v":1,"content":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","time":1762622441593},{"kind":"opened","id":17,"time":1762622441593},{"kind":"changed","id":4,"v":2,"edit":[[50,50," (Optimized)"],[94,94,"1."],[95,95,"6M (reduced from 3"],[99,99,")"],[119,120,"8"],[122,123,"1"],[124,125,"2"]],"time":1762623638975},{"kind":"changed","id":4,"v":3,"edit":[[2834,2834," (Optimized)"],[2919,2920,"Lightweight 2"],[2951,2955,""],[2956,2956," - reduced from 3 rates"],[2964,2972,"Simplifi"],[2973,2973,"d"],[2987,2991,"reduced"],[2993,2994,"ha"],[2997,3002,"l"],[3024,3024,"1."],[3025,3025,"6M (reduced from 3"],[3029,3029,")"],[3767,3767,"Lightweight "],[3779,3782,"(2"],[3783,3786,"ra"],[3787,3787,"es "],[3788,3789,"n"],[3790,3790,"tead of 3, redu"],[3791,3793,""],[3794,3794,"d output"],[3796,3797,"han"],[3798,3799,""],[3800,3802,"ls)"],[3859,3862,"64"],[3876,3880,""],[3931,3933,"r"],[3934,3936,""],[3937,3939,"uc"],[3940,3940,"d cha"],[3941,3944,""],[3945,3945,"el"],[4015,4017,"16"],[4063,4065,"16"],[4191,4193,"8"],[4239,4241,"8"],[4310,4310," Simplified"],[4311,4312,"d"],[4324,4328,"reduced"],[4330,4331,"ha"],[4334,4339,"l"],[4409,4412,"64"],[4415,4417,"16"],[4419,4422,"64"],[4479,4482,"64"],[4609,4612,"64"],[4615,4617,"8"],[4619,4621,"32"],[4678,4680,"32"],[4861,4863,"32"]],"time":1762623641194},{"kind":"changed","id":5,"v":2,"edit":[[10251,10251,"_iou:"],[10252,10252,"mean "],[10255,10255," across all samples"],[10261,10261,"    mean_loss: mean cross-entropy loss\r\n    "],[10307,10307,"all_losses = []"],[10313,10313,"criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    "],[10403,10403,"\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss"]],"time":1762623765832},{"kind":"changed","id":5,"v":3,"edit":[[11970,12883,""]],"time":1762623780348},{"kind":"changed","id":5,"v":4,"edit":[[12136,12136," - now includes both train and validation loss"],[12188,12188,"in Loss', l"],[12190,12190,"ewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val"],[12196,12196,", linewidth=2, linestyle='--'"],[12295,12295," and Validation"],[12434,12434,", linewidth=2"],[12645,12645,", dpi=150"]],"time":1762623791607},{"kind":"changed","id":5,"v":5,"edit":[[19126,19126,"loss': [],\r\n        'val_"]],"time":1762623803059},{"kind":"changed","id":5,"v":6,"edit":[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],"time":1762623816438},{"kind":"changed","id":5,"v":7,"edit":[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]],"time":1762623835010},{"kind":"selectionChanged","id":17,"selection":[[883,883]],"time":1762624017851},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624140244},{"kind":"selectionChanged","id":17,"selection":[[869,870]],"time":1762624140330},{"kind":"selectionChanged","id":17,"selection":[[868,870]],"time":1762624140356},{"kind":"selectionChanged","id":17,"selection":[[867,870]],"time":1762624140378},{"kind":"changed","id":17,"v":7,"edit":[[867,870,"1"]],"time":1762624143421},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624143443},{"kind":"changed","id":17,"v":9,"edit":[[868,869,""]],"time":1762624143557},{"kind":"selectionChanged","id":17,"selection":[[849,849]],"time":1762624144669},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624146159},{"kind":"selectionChanged","id":17,"selection":[[869,869]],"time":1762624146967},{"kind":"changed","id":17,"v":13,"edit":[[868,869,""]],"time":1762624150477},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624150514},{"kind":"changed","id":17,"v":15,"edit":[[868,869,""]],"time":1762624151107},{"kind":"selectionChanged","id":17,"selection":[[849,849]],"time":1762624151684},{"kind":"changed","id":17,"v":17,"edit":[[849,849," "]],"time":1762624152194},{"kind":"selectionChanged","id":17,"selection":[[850,850]],"time":1762624152207},{"kind":"changed","id":17,"v":19,"edit":[[849,850,""]],"time":1762624153751},{"kind":"selectionChanged","id":17,"selection":[[849,849]],"time":1762624153753},{"kind":"selectionChanged","id":17,"selection":[[850,850]],"time":1762624155051},{"kind":"selectionChanged","id":17,"selection":[[859,859]],"time":1762624155211},{"kind":"selectionChanged","id":17,"selection":[[865,865]],"time":1762624155399},{"kind":"selectionChanged","id":17,"selection":[[877,877]],"time":1762624155536},{"kind":"selectionChanged","id":17,"selection":[[878,878]],"time":1762624156832},{"kind":"selectionChanged","id":17,"selection":[[875,878]],"time":1762624156994},{"kind":"selectionChanged","id":17,"selection":[[783,878]],"time":1762624157031},{"kind":"selectionChanged","id":17,"selection":[[771,878]],"time":1762624157100},{"kind":"selectionChanged","id":17,"selection":[[770,878]],"time":1762624157133},{"kind":"selectionChanged","id":17,"selection":[[769,878]],"time":1762624157293},{"kind":"selectionChanged","id":17,"selection":[[849,878]],"time":1762624157382},{"kind":"changed","id":17,"v":32,"edit":[[849,878,""]],"time":1762624160604},{"kind":"selectionChanged","id":17,"selection":[[849,849]],"time":1762624160610},{"kind":"changed","id":17,"v":34,"edit":[[849,849,"# STANDARD MODEL (1arameters)"]],"time":1762624161991},{"kind":"selectionChanged","id":17,"selection":[[849,878]],"time":1762624161993},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624163196},{"kind":"selectionChanged","id":17,"selection":[[867,867]],"time":1762624163555},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624163994},{"kind":"changed","id":17,"v":39,"edit":[[868,868,"."]],"time":1762624165066},{"kind":"selectionChanged","id":17,"selection":[[869,869]],"time":1762624165070},{"kind":"changed","id":17,"v":41,"edit":[[869,869,"7"]],"time":1762624165514},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624165521},{"kind":"changed","id":17,"v":43,"edit":[[870,870,"5"]],"time":1762624165699},{"kind":"selectionChanged","id":17,"selection":[[871,871]],"time":1762624165704}]},"resultEdit":"   16  16 \n   17  17 # ============================================================================\n-  18     # STANDARD MODEL (1.75arameters)\n+      18 # STANDARD MODEL (1.75M parameters)\n   19  19 # ============================================================================\n   20  20 \n   21  21 # Baseline (No Knowledge Distillation)","isCachedResult":false,"prompt":"System\n------\nYour role as an AI assistant is to help developers complete their code tasks by assisting in editing specific sections of code marked by the <|code_to_edit|> and <|/code_to_edit|> tags, while adhering to Microsoft's content policies and avoiding the creation of content that violates copyrights.\n\nYou have access to the following information to help you make informed suggestions:\n\n- recently_viewed_code_snippets: These are code snippets that the developer has recently looked at, which might provide context or examples relevant to the current task. They are listed from oldest to newest, with line numbers in the form #| to help you understand the edit diff history. It's possible these are entirely irrelevant to the developer's change.\n- current_file_content: The content of the file the developer is currently working on, providing the broader context of the code. Line numbers in the form #| are included to help you understand the edit diff history.\n- edit_diff_history: A record of changes made to the code, helping you understand the evolution of the code and the developer's intentions. These changes are listed from oldest to latest. It's possible a lot of old edit diff history is entirely irrelevant to the developer's change.\n- area_around_code_to_edit: The context showing the code surrounding the section to be edited.\n- cursor position marked as <|cursor|>: Indicates where the developer's cursor is currently located, which can be crucial for understanding what part of the code they are focusing on.\n\nYour task is to predict and complete the changes the developer would have made next in the <|code_to_edit|> section. The developer may have stopped in the middle of typing. Your goal is to keep the developer on the path that you think they're following. Some examples include further implementing a class, method, or variable, or improving the quality of the code. Make sure the developer doesn't get distracted and ensure your suggestion is relevant. Consider what changes need to be made next, if any. If you think changes should be made, ask yourself if this is truly what needs to happen. If you are confident about it, then proceed with the changes.\n\n# Steps\n\n1. **Review Context**: Analyze the context from the resources provided, such as recently viewed snippets, edit history, surrounding code, and cursor location.\n2. **Evaluate Current Code**: Determine if the current code within the tags requires any corrections or enhancements.\n3. **Suggest Edits**: If changes are required, ensure they align with the developer's patterns and improve code quality.\n4. **Maintain Consistency**: Ensure indentation and formatting follow the existing code style.\n\n# Output Format\n\n- Provide only the revised code within the tags. If no changes are necessary, simply return the original code from within the <|code_to_edit|> and <|/code_to_edit|> tags.\n- There are line numbers in the form #| in the code displayed to you above, but these are just for your reference. Please do not include the numbers of the form #| in your response.\n- Ensure that you do not output duplicate code that exists outside of these tags. The output should be the revised code that was between these tags and should not include the <|code_to_edit|> or <|/code_to_edit|> tags.\n\n```\n// Your revised code goes here\n```\n\n# Notes\n\n- Apologize with \"Sorry, I can't assist with that.\" for requests that may breach Microsoft content guidelines.\n- Avoid undoing or reverting the developer's last change unless there are obvious typos or errors.\n- Don't include the line numbers of the form #| in your response.\n==================\nUser\n------\n```\n<|recently_viewed_code_snippets|>\n<|recently_viewed_code_snippet|>\ncode_snippet_file_path: /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py (truncated)\n\"\"\"\nTraining Script for Compact Segmentation Models with Knowledge Distillation\nELEC 475 Lab 3\n\nSupports three training modes:\n1. No Knowledge Distillation (baseline)\n2. Response-based Knowledge Distillation\n3. Feature-based Knowledge Distillation\n\nReferences:\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\n\"\"\"\n\nimport argparse\nimport os\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import VOCSegmentation\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\nfrom tqdm import tqdm\n\n# Import custom models\nfrom model_ultracompact import UltraCompactSegmentationModel\nfrom model_standard import StandardSegmentationModel\n\n\nclass VOCNormalize:\n    \"\"\"Custom normalization for VOC dataset\"\"\"\n    def __init__(self, augment=True):\n        self.augment = augment\n        self.normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    \n    def __call__(self, img, target):\n        # Convert PIL to tensor\n        img = transforms.ToTensor()(img)\n        \n        # Apply augmentation if training\n        if self.augment and np.random.rand() > 0.5:\n            # Random horizontal flip\n            img = transforms.functional.hflip(img)\n            target = transforms.functional.hflip(target)\n        \n        # Normalize image\n        img = self.normalize(img)\n        \n        # Convert target to tensor\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n        \n        return img, target\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\n    images, targets = zip(*batch)\n    return list(images), list(targets)\n\n\ndef calculate_miou(pred, target, num_classes=21):\n    \"\"\"\n    Calculate mean Intersection over Union (mIoU)\n    \n    Args:\n        pred: predicted segmentation mask (H, W)\n        target: ground truth segmentation mask (H, W)\n        num_classes: number of classes\n    \n    Returns:\n        mIoU value\n    \"\"\"\n    pred = pred.cpu().numpy()\n    target = target.cpu().numpy()\n    \n    ious = []\n    for cls in range(num_classes):\n        pred_mask = (pred == cls)\n        target_mask = (target == cls)\n        \n        intersection = np.logical_and(pred_mask, target_mask).sum()\n        union = np.logical_or(pred_mask, target_mask).sum()\n        \n        if union == 0:\n            continue\n        \n        iou = intersection / union\n        ious.append(iou)\n    \n    return np.mean(ious) if len(ious) > 0 else 0.0\n\n\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\n    \"\"\"\n    Response-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        teacher_logits: teacher model output logits (B, C, H, W)\n        targets: ground truth labels (B, H, W)\n        temperature: softmax temperature for distillation\n        alpha: weight for cross-entropy loss\n        beta: weight for distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Distillation loss (KL divergence)\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\n    \n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * kd_loss\n    \n    return total_loss, ce_loss.item(), kd_loss.item()\n\n\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\n    \"\"\"\n    Feature-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        student_features: dict of student intermediate features\n        teacher_features: dict of teacher intermediate features\n        targets: ground truth labels (B, H, W)\n        alpha: weight for cross-entropy loss\n        beta: weight for feature distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Feature matching loss (cosine similarity)\n    feat_loss = 0\n    num_levels = 0\n    \n    for level in ['low', 'mid', 'high']:\n        if level in student_features and level in teacher_features:\n            s_feat = student_features[level]\n            t_feat = teacher_features[level]\n            \n            # Resize if needed (teacher features might be different size)\n            if s_feat.shape != t_feat.shape:\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n                # Match channels if needed\n                if s_feat.shape[1] != t_feat.shape[1]:\n                    # Project teacher features to student channel size\n                    continue  # Skip if channel mismatch (or add projection layer)\n            \n            # Cosine similarity loss (1 - cosine_similarity)\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\n            \n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\n            feat_loss += (1 - cosine_sim)\n            num_levels += 1\n    \n    if num_levels > 0:\n        feat_loss = feat_loss / num_levels\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * feat_loss\n    \n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\n\n\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\n    \"\"\"\n    Train for one epoch\n    \n    Args:\n        model: student model\n        teacher_model: teacher model (None if kd_mode='none')\n        dataloader: training data loader\n        optimizer: optimizer\n        device: device to run on\n        kd_mode: 'none', 'response', or 'feature'\n        args: training arguments\n    \n    Returns:\n        average loss, average ce_loss, average kd_loss\n    \"\"\"\n    model.train()\n    if teacher_model is not None:\n        teacher_model.eval()\n    \n    total_loss = 0\n    total_ce_loss = 0\n    total_kd_loss = 0\n    num_batches = 0\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for images, targets in pbar:\n        # Move to device and resize to fixed size for batching\n        batch_imgs = []\n        batch_targets = []\n        \n        for img, target in zip(images, targets):\n            img = img.to(device)\n            target = target.to(device)\n            \n            # Resize to fixed size\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\n            \n            batch_imgs.append(img)\n            batch_targets.append(target)\n        \n        batch_imgs = torch.stack(batch_imgs)\n<|/recently_viewed_code_snippet|>\n<|/recently_viewed_code_snippets|>\n\n<|current_file_content|>\ncurrent_file_path: train.txt\n# ELEC 475 Lab 3 - Training Commands\n# All commands should be run from the Lab 3 directory\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n# STANDARD MODEL (1.75<|cursor|>arameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\n\n# ============================================================================\n# OPTIONAL: HYPERPARAMETER TUNING\n# ============================================================================\n# If you want to experiment with different hyperparameters:\n\n# Adjust learning rate\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\n\n# Adjust KD temperature (higher = softer targets)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\n\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\n\n# Adjust batch size (if memory issues)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\n\n# ============================================================================\n# NOTES\n# ============================================================================\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\n# - Monitor validation mIoU - training should converge after 30-40 epochs\n# - Best model is saved automatically when validation mIoU improves\n\n<|/current_file_content|>\n\n<|edit_diff_history|>\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -342,23 +342,0 @@\n-            for img, target in zip(images, targets):\n-                img = img.unsqueeze(0).to(device)\n-                target = target.to(device)\n-                \n-                # Resize to fixed size\n-                h, w = target.shape\n-                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\n-                \n-                # Forward pass\n-                output = model(img)\n-                \n-                # Resize back to original size\n-                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\n-                \n-                # Get predictions\n-                pred = output.squeeze(0).argmax(0)\n-                \n-                # Calculate IoU\n-                iou = calculate_miou(pred, target, num_classes)\n-                all_ious.append(iou)\n-    \n-    mean_iou = np.mean(all_ious)\n-    return mean_iou\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -348,1 +348,1 @@\n-    # Loss plot\n+    # Loss plot - now includes both train and validation loss\n@@ -349,1 +349,2 @@\n-    axes[0].plot(history['train_loss'], label='Train Loss')\n+    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n+    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\n@@ -352,1 +352,1 @@\n-    axes[0].set_title('Training Loss')\n+    axes[0].set_title('Training and Validation Loss')\n@@ -357,1 +357,1 @@\n-    axes[1].plot(history['val_miou'], label='Validation mIoU')\n+    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\n@@ -365,1 +365,1 @@\n-    plt.savefig(save_path)\n+    plt.savefig(save_path, dpi=150)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -535,1 +535,2 @@\n-        'val_miou': [],\n+        'val_loss': [],\n+        'val_miou': [],\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -581,1 +581,1 @@\n-        val_miou = validate(model, val_loader, device)\n+        val_miou, val_loss = validate(model, val_loader, device)\n@@ -582,1 +582,1 @@\n-        print(f\"Validation mIoU: {val_miou:.4f}\")\n+        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")\n@@ -592,1 +592,2 @@\n-        history['train_kd_loss'].append(train_kd_loss)\n+        history['train_kd_loss'].append(train_kd_loss)\n+        history['val_loss'].append(val_loss)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -652,1 +652,2 @@\n-        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n@@ -17,1 +17,1 @@\n-# STANDARD MODEL (3.4M parameters)\n+# STANDARD MODEL (1.75arameters)\n\n<|/edit_diff_history|>\n\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n# STANDARD MODEL (1.75<|cursor|>arameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n```\n\nThe developer was working on a section of code within the tags `code_to_edit` in the file located at `train.txt`. Using the given `recently_viewed_code_snippets`, `current_file_content`, `edit_diff_history`, `area_around_code_to_edit`, and the cursor position marked as `<|cursor|>`, please continue the developer's work. Update the `code_to_edit` section by predicting and completing the changes they would have made next. Provide the revised code that was between the `<|code_to_edit|>` and `<|/code_to_edit|>` tags with the following format, but do not include the tags themselves.\n```\n// Your revised code goes here\n```\n==================","error":"undefined","response":"```\n\n# ============================================================================\n# STANDARD MODEL (1.75M parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n```","responseResults":"- replaceRange:\n   start: 849\n   endExclusive: 881\n  newText: \"# STANDARD MODEL (1.75M parameters)\"\n","providerStartTime":1762624165845,"providerEndTime":1762624166049,"fetchStartTime":1762624166046,"fetchEndTime":1762624166561,"logs":["```\n{\n\t\"isAsyncCompletions\": true,\n\t\"isRevisedCacheStrategy\": true,\n\t\"isCacheTracksRejections\": true,\n\t\"isRecentlyShownCacheEnabled\": false,\n\t\"debounceUseCoreRequestTime\": false\n}\n```\n","TTFT 240.0409000068903 ms\n","Line 0 emitted with latency 240.46269999444485 ms\n","Line 1 emitted with latency 240.4877000004053 ms\n","Line 2 emitted with latency 318.2352000027895 ms\n","Line 3 emitted with latency 318.7811000049114 ms\n","pushing edit #0:\\n[18,19)->[\"# STANDARD MODEL (1.75M parameters)\"]\n","First edit latency: 567 ms\n","Line 4 emitted with latency 333.2946999967098 ms\n","Line 5 emitted with latency 333.3344999998808 ms\n","Line 6 emitted with latency 411.08630000054836 ms\n","Line 7 emitted with latency 510.18080000579357 ms\n","Line 8 emitted with latency 510.2217999994755 ms\n","Line 9 emitted with latency 515.0558000057936 ms\n"],"isAccepted":true}
{"requestId":10,"time":1762624162081,"filePath":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","version":10,"statelessNextEditProviderId":"XtabProvider","nextEditRequest":{"id":"d2461b87-001a-4b73-8e62-afe19cd44b94","documents":[{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.py","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"python","documentLinesBeforeEdit":["\"\"\"","Training Script for Compact Segmentation Models with Knowledge Distillation","ELEC 475 Lab 3","","Supports three training modes:","1. No Knowledge Distillation (baseline)","2. Response-based Knowledge Distillation","3. Feature-based Knowledge Distillation","","References:","- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)","- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)","\"\"\"","","import argparse","import os","import time","from pathlib import Path","import numpy as np","import matplotlib.pyplot as plt","","import torch","import torch.nn as nn","import torch.nn.functional as F","from torch.utils.data import DataLoader","from torchvision import transforms","from torchvision.datasets import VOCSegmentation","from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights","from tqdm import tqdm","","# Import custom models","from model_ultracompact import UltraCompactSegmentationModel","from model_standard import StandardSegmentationModel","","","class VOCNormalize:","    \"\"\"Custom normalization for VOC dataset\"\"\"","    def __init__(self, augment=True):","        self.augment = augment","        self.normalize = transforms.Normalize(","            mean=[0.485, 0.456, 0.406],","            std=[0.229, 0.224, 0.225]","        )","    ","    def __call__(self, img, target):","        # Convert PIL to tensor","        img = transforms.ToTensor()(img)","        ","        # Apply augmentation if training","        if self.augment and np.random.rand() > 0.5:","            # Random horizontal flip","            img = transforms.functional.hflip(img)","            target = transforms.functional.hflip(target)","        ","        # Normalize image","        img = self.normalize(img)","        ","        # Convert target to tensor","        target = torch.as_tensor(np.array(target), dtype=torch.int64)","        ","        return img, target","","","def collate_fn(batch):","    \"\"\"Custom collate function to handle variable-sized images\"\"\"","    images, targets = zip(*batch)","    return list(images), list(targets)","","","def calculate_miou(pred, target, num_classes=21):","    \"\"\"","    Calculate mean Intersection over Union (mIoU)","    ","    Args:","        pred: predicted segmentation mask (H, W)","        target: ground truth segmentation mask (H, W)","        num_classes: number of classes","    ","    Returns:","        mIoU value","    \"\"\"","    pred = pred.cpu().numpy()","    target = target.cpu().numpy()","    ","    ious = []","    for cls in range(num_classes):","        pred_mask = (pred == cls)","        target_mask = (target == cls)","        ","        intersection = np.logical_and(pred_mask, target_mask).sum()","        union = np.logical_or(pred_mask, target_mask).sum()","        ","        if union == 0:","            continue","        ","        iou = intersection / union","        ious.append(iou)","    ","    return np.mean(ious) if len(ious) > 0 else 0.0","","","def response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):","    \"\"\"","    Response-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        teacher_logits: teacher model output logits (B, C, H, W)","        targets: ground truth labels (B, H, W)","        temperature: softmax temperature for distillation","        alpha: weight for cross-entropy loss","        beta: weight for distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Distillation loss (KL divergence)","    student_soft = F.log_softmax(student_logits / temperature, dim=1)","    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)","    ","    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * kd_loss","    ","    return total_loss, ce_loss.item(), kd_loss.item()","","","def feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):","    \"\"\"","    Feature-based knowledge distillation loss","    ","    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)","    ","    Args:","        student_logits: student model output logits (B, C, H, W)","        student_features: dict of student intermediate features","        teacher_features: dict of teacher intermediate features","        targets: ground truth labels (B, H, W)","        alpha: weight for cross-entropy loss","        beta: weight for feature distillation loss","    ","    Returns:","        total loss","    \"\"\"","    # Standard cross-entropy loss","    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)","    ","    # Feature matching loss (cosine similarity)","    feat_loss = 0","    num_levels = 0","    ","    for level in ['low', 'mid', 'high']:","        if level in student_features and level in teacher_features:","            s_feat = student_features[level]","            t_feat = teacher_features[level]","            ","            # Resize if needed (teacher features might be different size)","            if s_feat.shape != t_feat.shape:","                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)","                # Match channels if needed","                if s_feat.shape[1] != t_feat.shape[1]:","                    # Project teacher features to student channel size","                    continue  # Skip if channel mismatch (or add projection layer)","            ","            # Cosine similarity loss (1 - cosine_similarity)","            s_feat_norm = F.normalize(s_feat, p=2, dim=1)","            t_feat_norm = F.normalize(t_feat, p=2, dim=1)","            ","            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()","            feat_loss += (1 - cosine_sim)","            num_levels += 1","    ","    if num_levels > 0:","        feat_loss = feat_loss / num_levels","    ","    # Combined loss","    total_loss = alpha * ce_loss + beta * feat_loss","    ","    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0","","","def train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):","    \"\"\"","    Train for one epoch","    ","    Args:","        model: student model","        teacher_model: teacher model (None if kd_mode='none')","        dataloader: training data loader","        optimizer: optimizer","        device: device to run on","        kd_mode: 'none', 'response', or 'feature'","        args: training arguments","    ","    Returns:","        average loss, average ce_loss, average kd_loss","    \"\"\"","    model.train()","    if teacher_model is not None:","        teacher_model.eval()","    ","    total_loss = 0","    total_ce_loss = 0","    total_kd_loss = 0","    num_batches = 0","    ","    pbar = tqdm(dataloader, desc='Training')","    for images, targets in pbar:","        # Move to device and resize to fixed size for batching","        batch_imgs = []","        batch_targets = []","        ","        for img, target in zip(images, targets):","            img = img.to(device)","            target = target.to(device)","            ","            # Resize to fixed size","            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)","            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()","            ","            batch_imgs.append(img)","            batch_targets.append(target)","        ","        batch_imgs = torch.stack(batch_imgs)","        batch_targets = torch.stack(batch_targets)","        ","        optimizer.zero_grad()","        ","        # Forward pass based on KD mode","        if kd_mode == 'none':","            # No knowledge distillation","            output = model(batch_imgs)","            loss = F.cross_entropy(output, batch_targets, ignore_index=255)","            ce_loss_val = loss.item()","            kd_loss_val = 0.0","            ","        elif kd_mode == 'response':","            # Response-based KD","            output = model(batch_imgs)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","            ","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","            ","        elif kd_mode == 'feature':","            # Feature-based KD","            output, student_features = model(batch_imgs, return_features=True)","            with torch.no_grad():","                teacher_output = teacher_model(batch_imgs)['out']","                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)","                # For now, use output-based distillation","                teacher_features = {}  # Would need to extract from teacher","            ","            # Fallback to response-based if teacher features not available","            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(","                output, teacher_output, batch_targets,","                temperature=args.temperature,","                alpha=args.alpha,","                beta=args.beta","            )","        ","        # Backward pass","        loss.backward()","        optimizer.step()","        ","        total_loss += loss.item()","        total_ce_loss += ce_loss_val","        total_kd_loss += kd_loss_val","        num_batches += 1","        ","        pbar.set_postfix({'loss': f'{loss.item():.4f}'})","    ","    avg_loss = total_loss / num_batches","    avg_ce_loss = total_ce_loss / num_batches","    avg_kd_loss = total_kd_loss / num_batches","    ","    return avg_loss, avg_ce_loss, avg_kd_loss","","","def validate(model, dataloader, device, num_classes=21):","    \"\"\"","    Validate the model","    ","    Args:","        model: model to validate","        dataloader: validation data loader","        device: device to run on","        num_classes: number of classes","    ","    Returns:","        mean_iou: mean IoU across all samples","        mean_loss: mean cross-entropy loss","    \"\"\"","    model.eval()","    all_ious = []","    all_losses = []","    criterion = nn.CrossEntropyLoss(ignore_index=255)","    ","    with torch.no_grad():","        for images, targets in tqdm(dataloader, desc='Validation'):","            for img, target in zip(images, targets):","                img = img.unsqueeze(0).to(device)","                target = target.to(device)","                ","                # Resize to fixed size","                h, w = target.shape","                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)","                ","                # Forward pass","                output = model(img)","                ","                # Calculate loss at this resolution","                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), ","                                               size=output.shape[-2:], ","                                               mode='nearest').squeeze().long()","                loss = criterion(output, target_resized.unsqueeze(0))","                all_losses.append(loss.item())","                ","                # Resize back to original size for IoU calculation","                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)","                ","                # Get predictions","                pred = output.squeeze(0).argmax(0)","                ","                # Calculate IoU","                iou = calculate_miou(pred, target, num_classes)","                all_ious.append(iou)","    ","    mean_iou = np.mean(all_ious)","    mean_loss = np.mean(all_losses)","    return mean_iou, mean_loss","","","def plot_training_curves(history, save_path):","    \"\"\"Plot and save training curves\"\"\"","    fig, axes = plt.subplots(1, 2, figsize=(12, 4))","    ","    # Loss plot - now includes both train and validation loss","    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)","    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')","    axes[0].set_xlabel('Epoch')","    axes[0].set_ylabel('Loss')","    axes[0].set_title('Training and Validation Loss')","    axes[0].legend()","    axes[0].grid(True)","    ","    # mIoU plot","    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)","    axes[1].set_xlabel('Epoch')","    axes[1].set_ylabel('mIoU')","    axes[1].set_title('Validation mIoU')","    axes[1].legend()","    axes[1].grid(True)","    ","    plt.tight_layout()","    plt.savefig(save_path, dpi=150)","    plt.close()","    print(f\"Training curves saved to {save_path}\")","","","def main(args):","    \"\"\"Main training function\"\"\"","    ","    print(\"=\" * 70)","    print(\"Training Compact Segmentation Model\")","    print(\"=\" * 70)","    print(f\"Model: {args.model}\")","    print(f\"KD Mode: {args.kd_mode}\")","    print(f\"Epochs: {args.epochs}\")","    print(f\"Batch Size: {args.batch_size}\")","    print(f\"Learning Rate: {args.lr}\")","    print(\"=\" * 70)","    ","    # Set device","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","    print(f\"\\nUsing device: {device}\")","    ","    # Create save directory","    save_dir = Path(args.save_dir)","    save_dir.mkdir(parents=True, exist_ok=True)","    ","    # Load student model","    print(\"\\nLoading student model...\")","    if args.model == 'ultracompact':","        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)","    elif args.model == 'standard':","        model = StandardSegmentationModel(num_classes=21, pretrained=True)","    else:","        raise ValueError(f\"Unknown model: {args.model}\")","    ","    model = model.to(device)","    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)","    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")","    ","    # Load teacher model if using KD","    teacher_model = None","    if args.kd_mode != 'none':","        print(\"\\nLoading teacher model (FCN-ResNet50)...\")","        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1","        teacher_model = fcn_resnet50(weights=weights)","        teacher_model = teacher_model.to(device)","        teacher_model.eval()","        for param in teacher_model.parameters():","            param.requires_grad = False","        print(\"Teacher model loaded and frozen\")","    ","    # Prepare datasets","    print(\"\\nPreparing datasets...\")","    ","    # Find dataset path (Google Colab compatible)","    # VOCSegmentation expects: root/VOCdevkit/VOC2012/","    dataset_roots = []","    ","    # Check for Google Colab structure (both with and without VOCdevkit wrapper)","    colab_train_path = Path('/content/data/VOC2012_train_val')","    if colab_train_path.exists():","        # Check if it has VOCdevkit structure","        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(colab_train_path))","        # Check if files are directly in the folder (no VOCdevkit wrapper)","        elif (colab_train_path / 'JPEGImages').exists():","            # Create a symlink or use parent structure","            dataset_roots.append(str(colab_train_path.parent))","    ","    # Check for local structure","    local_data_path = Path('./data')","    if local_data_path.exists():","        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():","            dataset_roots.append(str(local_data_path))","        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():","            dataset_roots.append(str(local_data_path))","    ","    # Check standard paths","    dataset_roots.extend([","        '/content/data',  # Google Colab default","        './data',  # Local path","        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),","    ])","    ","    # Check dataset_path.txt","    dataset_path_file = Path('dataset_path.txt')","    if dataset_path_file.exists():","        with open(dataset_path_file, 'r') as f:","            saved_path = Path(f.read().strip())","            dataset_roots.insert(0, str(saved_path.parent.parent))","    ","    dataset_root = None","    for root in dataset_roots:","        try:","            # Try to create a temporary symlink structure if needed","            root_path = Path(root)","            ","            # Check if we need to create VOCdevkit/VOC2012 structure","            voc_path = root_path / 'VOC2012_train_val'","            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():","                # Files are directly in VOC2012_train_val, create temporary structure","                vocdevkit_path = voc_path / 'VOCdevkit'","                voc2012_path = vocdevkit_path / 'VOC2012'","                if not voc2012_path.exists():","                    print(f\"Creating VOCdevkit structure in {voc_path}\")","                    vocdevkit_path.mkdir(exist_ok=True)","                    voc2012_path.symlink_to(voc_path, target_is_directory=True)","            ","            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)","            dataset_root = root","            print(f\"✓ Successfully loaded dataset from: {root}\")","            break","        except Exception as e:","            continue","    ","    if dataset_root is None:","        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")","    ","    print(f\"Dataset found at: {dataset_root}\")","    ","    # Create datasets","    train_transform = VOCNormalize(augment=True)","    val_transform = VOCNormalize(augment=False)","    ","    train_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='train',","        download=False,","        transforms=train_transform","    )","    ","    val_dataset = VOCSegmentation(","        root=dataset_root,","        year='2012',","        image_set='val',","        download=False,","        transforms=val_transform","    )","    ","    print(f\"Training set size: {len(train_dataset)}\")","    print(f\"Validation set size: {len(val_dataset)}\")","    ","    # Create dataloaders","    train_loader = DataLoader(","        train_dataset,","        batch_size=args.batch_size,","        shuffle=True,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    val_loader = DataLoader(","        val_dataset,","        batch_size=4,","        shuffle=False,","        num_workers=0,","        collate_fn=collate_fn","    )","    ","    # Setup optimizer and scheduler","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)","    ","    # Training history","    history = {","        'train_loss': [],","        'train_ce_loss': [],","        'train_kd_loss': [],","        'val_loss': [],","        'val_miou': [],","        'epoch_times': []","    }","    ","    # Save hyperparameters","    hyperparameters = {","        'model': args.model,","        'kd_mode': args.kd_mode,","        'epochs': args.epochs,","        'batch_size': args.batch_size,","        'learning_rate': args.lr,","        'weight_decay': args.weight_decay,","        'temperature': args.temperature,","        'alpha': args.alpha,","        'beta': args.beta,","        'optimizer': 'Adam',","        'scheduler': 'CosineAnnealingLR',","        'num_parameters': sum(p.numel() for p in model.parameters()),","        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)","    }","    ","    best_miou = 0.0","    training_start_time = time.time()","    ","    # Training loop","    print(\"\\nStarting training...\")","    print(\"=\" * 70)","    print(\"\\nHyperparameters:\")","    for key, value in hyperparameters.items():","        print(f\"  {key}: {value}\")","    ","    for epoch in range(args.epochs):","        epoch_start_time = time.time()","        ","        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")","        print(\"-\" * 70)","        ","        # Train","        train_loss, train_ce_loss, train_kd_loss = train_epoch(","            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args","        )","        ","        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")","        ","        # Validate","        val_miou = validate(model, val_loader, device)","        print(f\"Validation mIoU: {val_miou:.4f}\")","        ","        # Record epoch time","        epoch_time = time.time() - epoch_start_time","        history['epoch_times'].append(epoch_time)","        print(f\"Epoch time: {epoch_time:.2f} seconds\")","        ","        # Update history","        history['train_loss'].append(train_loss)","        history['train_ce_loss'].append(train_ce_loss)","        history['train_kd_loss'].append(train_kd_loss)","        history['val_miou'].append(val_miou)","        ","        # Save best model","        if val_miou > best_miou:","            best_miou = val_miou","            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history","            }, save_path)","            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")","        ","        # Update scheduler","        scheduler.step()","        ","        # Save checkpoint every 10 epochs","        if (epoch + 1) % 10 == 0:","            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"","            torch.save({","                'epoch': epoch,","                'model_state_dict': model.state_dict(),","                'optimizer_state_dict': optimizer.state_dict(),","                'miou': val_miou,","                'history': history,","                'hyperparameters': hyperparameters","            }, checkpoint_path)","            print(f\"Checkpoint saved: {checkpoint_path}\")","    ","    # Calculate total training time","    total_training_time = time.time() - training_start_time","    avg_epoch_time = np.mean(history['epoch_times'])","    ","    # Plot training curves","    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"","    plot_training_curves(history, plot_path)","    ","    # Save final training report","    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"","    with open(report_path, 'w') as f:","        f.write(\"=\" * 70 + \"\\n\")","        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")","        f.write(\"=\" * 70 + \"\\n\\n\")","        ","        f.write(\"HYPERPARAMETERS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        for key, value in hyperparameters.items():","            f.write(f\"  {key:25s}: {value}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"TRAINING STATISTICS:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")","        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")","        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")","        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")","        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")","        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")","        ","        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")","        f.write(\"FILES GENERATED:\\n\")","        f.write(\"-\" * 70 + \"\\n\")","        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")","        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")","        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")","        f.write(\"=\" * 70 + \"\\n\")","    ","    print(\"\\n\" + \"=\" * 70)","    print(\"Training completed!\")","    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")","    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")","    print(f\"Best validation mIoU: {best_miou:.4f}\")","    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")","    print(f\"Training report saved to: {report_path}\")","    ","    # Calculate final score","    score = 4 * best_miou / (1 + num_params / 1e6)","    print(f\"\\nFinal Score: {score:.3f}\")","    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")","    print(\"=\" * 70)","","","if __name__ == '__main__':","    parser = argparse.ArgumentParser(description='Train compact segmentation model')","    ","    # Model arguments","    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],","                        help='Model type to train')","    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],","                        help='Knowledge distillation mode')","    ","    # Training arguments","    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')","    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')","    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')","    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')","    ","    # KD arguments","    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')","    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')","    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')","    ","    # Other arguments","    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')","    ","    args = parser.parse_args()","    ","    main(args)",""],"recentEdit":[[582,583,["        val_miou, val_loss = validate(model, val_loader, device)"]],[583,584,["        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")"]],[594,594,["        history['val_loss'].append(val_loss)"]],[652,652,["        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")"]]],"documentBeforeEdits":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean_iou: mean IoU across all samples\r\n        mean_loss: mean cross-entropy loss\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    all_losses = []\r\n    criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot - now includes both train and validation loss\r\n    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training and Validation Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path, dpi=150)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","recentEdits":[[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]]],"lastSelectionInAfterEdit":[18491,18491]},{"id":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203/train.txt","workspaceRoot":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","languageId":"plaintext","documentLinesBeforeEdit":["# ELEC 475 Lab 3 - Training Commands","# All commands should be run from the Lab 3 directory","","# ============================================================================","# ULTRA-COMPACT MODEL (475K parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# STANDARD MODEL (3.4M parameters)","# ============================================================================","","# Baseline (No Knowledge Distillation)","python train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001","","# Response-based Knowledge Distillation","python train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5","","# Feature-based Knowledge Distillation","python train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5","","# ============================================================================","# TRAINING OUTPUT FILES","# ============================================================================","# For each training run, the following files will be generated in ./checkpoints/:","#   - best_model_{model}_{kd_mode}.pth           (best model weights)","#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)","#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)","#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)","","# ============================================================================","# OPTIONAL: HYPERPARAMETER TUNING","# ============================================================================","# If you want to experiment with different hyperparameters:","","# Adjust learning rate","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005","","# Adjust KD temperature (higher = softer targets)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0","","# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3","","# Adjust batch size (if memory issues)","# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4","","# ============================================================================","# NOTES","# ============================================================================","# - Training on CPU will be slow. Consider using Google Colab with GPU.","# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU","# - Monitor validation mIoU - training should converge after 30-40 epochs","# - Best model is saved automatically when validation mIoU improves",""],"recentEdit":[[18,19,[" # STANDARD MODEL (1arameters)"]]],"documentBeforeEdits":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","recentEdits":[[[867,873,"1"]],[[849,849," "]]],"lastSelectionInAfterEdit":[850,850]}],"activeDocumentIdx":1,"recording":[{"documentType":"workspaceRecording@1.0","kind":"header","repoRootUri":"file:///c%3A/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year%205/ELEC%20475/Lab%203","time":1762624152333,"uuid":"c90b2c0a-008b-47db-b3af-2285c45f081a"},{"kind":"documentEncountered","id":4,"relativePath":"model_standard.py","time":1762545107282},{"kind":"setContent","id":4,"v":1,"content":"\"\"\"\r\nStandard MobileNetV3-Small Segmentation Model\r\nELEC 475 Lab 3 - Option 1\r\n\r\nParameters: ~3.41M\r\nExpected Score: 0.40-0.50\r\n\r\nReferences:\r\n- MobileNetV3: Searching for MobileNetV3 (Howard et al., 2019)\r\n- DeepLabV3+: Encoder-Decoder with Atrous Separable Convolution (Chen et al., 2018)\r\n- ASPP: Rethinking Atrous Convolution for Semantic Image Segmentation (Chen et al., 2017)\r\n\"\"\"\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\r\n\r\n\r\nclass ASPP(nn.Module):\r\n    \"\"\"\r\n    Atrous Spatial Pyramid Pooling (ASPP) module\r\n    Captures multi-scale context using parallel atrous convolutions\r\n    \"\"\"\r\n    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\r\n        super(ASPP, self).__init__()\r\n        \r\n        self.branches = nn.ModuleList()\r\n        \r\n        # 1x1 convolution branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Atrous convolution branches\r\n        for rate in rates:\r\n            self.branches.append(nn.Sequential(\r\n                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\r\n                nn.BatchNorm2d(out_channels),\r\n                nn.ReLU(inplace=True)\r\n            ))\r\n        \r\n        # Global average pooling branch\r\n        self.branches.append(nn.Sequential(\r\n            nn.AdaptiveAvgPool2d(1),\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True)\r\n        ))\r\n        \r\n        # Combine all branches\r\n        num_branches = len(rates) + 2  # 1x1 + atrous branches + global pooling\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(out_channels * num_branches, out_channels, 1, bias=False),\r\n            nn.BatchNorm2d(out_channels),\r\n            nn.ReLU(inplace=True),\r\n            nn.Dropout(0.5)\r\n        )\r\n    \r\n    def forward(self, x):\r\n        h, w = x.shape[2:]\r\n        branch_outputs = []\r\n        \r\n        for i, branch in enumerate(self.branches):\r\n            out = branch(x)\r\n            # Upsample global pooling branch to match spatial dimensions\r\n            if i == len(self.branches) - 1:\r\n                out = F.interpolate(out, size=(h, w), mode='bilinear', align_corners=False)\r\n            branch_outputs.append(out)\r\n        \r\n        # Concatenate all branches\r\n        x = torch.cat(branch_outputs, dim=1)\r\n        x = self.project(x)\r\n        \r\n        return x\r\n\r\n\r\nclass StandardSegmentationModel(nn.Module):\r\n    \"\"\"\r\n    Standard compact segmentation model using MobileNetV3-Small backbone\r\n    \r\n    Architecture:\r\n    - Full MobileNetV3-Small backbone (pretrained)\r\n    - 3-rate ASPP module (rates: 6, 12, 18)\r\n    - Two-stage decoder with skip connections\r\n    - Parameters: ~3.41M\r\n    \"\"\"\r\n    def __init__(self, num_classes=21, pretrained=True):\r\n        super(StandardSegmentationModel, self).__init__()\r\n        \r\n        # Load pretrained MobileNetV3-Small\r\n        if pretrained:\r\n            weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1\r\n            backbone = mobilenet_v3_small(weights=weights)\r\n        else:\r\n            backbone = mobilenet_v3_small(weights=None)\r\n        \r\n        # Extract feature layers from MobileNetV3-Small\r\n        self.features = backbone.features\r\n        \r\n        # Feature dimensions at tap points:\r\n        # Stage 1 (low):  16 channels, stride 4\r\n        # Stage 3 (mid):  24 channels, stride 8\r\n        # Stage 12 (high): 576 channels, stride 16\r\n        \r\n        # ASPP module for multi-scale context\r\n        self.aspp = ASPP(in_channels=576, out_channels=128, rates=[6, 12, 18])\r\n        \r\n        # Feature projection layers (keep dimensions)\r\n        self.mid_conv = nn.Sequential(\r\n            nn.Conv2d(24, 24, 1, bias=False),\r\n            nn.BatchNorm2d(24),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.low_conv = nn.Sequential(\r\n            nn.Conv2d(16, 16, 1, bias=False),\r\n            nn.BatchNorm2d(16),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Decoder with skip connections\r\n        self.decoder_conv1 = nn.Sequential(\r\n            nn.Conv2d(128 + 24, 128, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(128),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        self.decoder_conv2 = nn.Sequential(\r\n            nn.Conv2d(128 + 16, 64, 3, padding=1, bias=False),\r\n            nn.BatchNorm2d(64),\r\n            nn.ReLU(inplace=True)\r\n        )\r\n        \r\n        # Classifier head\r\n        self.classifier = nn.Sequential(\r\n            nn.Dropout(0.5),\r\n            nn.Conv2d(64, num_classes, 1)\r\n        )\r\n        \r\n        self._init_weights()\r\n    \r\n    def _init_weights(self):\r\n        \"\"\"Initialize weights for new layers\"\"\"\r\n        for m in [self.aspp, self.mid_conv, self.low_conv, self.decoder_conv1, self.decoder_conv2, self.classifier]:\r\n            for module in m.modules():\r\n                if isinstance(module, nn.Conv2d):\r\n                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\r\n                    if module.bias is not None:\r\n                        nn.init.constant_(module.bias, 0)\r\n                elif isinstance(module, nn.BatchNorm2d):\r\n                    nn.init.constant_(module.weight, 1)\r\n                    nn.init.constant_(module.bias, 0)\r\n    \r\n    def forward(self, x, return_features=False):\r\n        \"\"\"\r\n        Forward pass\r\n        \r\n        Args:\r\n            x: input image tensor (B, 3, H, W)\r\n            return_features: if True, return intermediate features for knowledge distillation\r\n            \r\n        Returns:\r\n            output: segmentation logits (B, num_classes, H, W)\r\n            features (optional): dict of intermediate features for KD\r\n        \"\"\"\r\n        input_shape = x.shape[-2:]\r\n        \r\n        # Extract multi-scale features from backbone\r\n        low_feat = None   # stride 4\r\n        mid_feat = None   # stride 8\r\n        high_feat = None  # stride 16\r\n        \r\n        # Forward through MobileNetV3 stages\r\n        for i, layer in enumerate(self.features):\r\n            x = layer(x)\r\n            \r\n            # Capture features at specific stages\r\n            if i == 1:  # After first inverted residual block (stride 4)\r\n                low_feat = x\r\n            elif i == 3:  # After third inverted residual block (stride 8)\r\n                mid_feat = x\r\n        \r\n        # Final high-level features (stride 16)\r\n        high_feat = x\r\n        \r\n        # Process features\r\n        mid_feat = self.mid_conv(mid_feat)\r\n        low_feat = self.low_conv(low_feat)\r\n        \r\n        # ASPP for multi-scale context\r\n        x = self.aspp(high_feat)\r\n        \r\n        # Decoder with skip connections\r\n        # Upsample and concatenate with mid-level features\r\n        x = F.interpolate(x, size=mid_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, mid_feat], dim=1)\r\n        x = self.decoder_conv1(x)\r\n        \r\n        # Upsample and concatenate with low-level features\r\n        x = F.interpolate(x, size=low_feat.shape[-2:], mode='bilinear', align_corners=False)\r\n        x = torch.cat([x, low_feat], dim=1)\r\n        x = self.decoder_conv2(x)\r\n        \r\n        # Final upsampling to input resolution\r\n        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\r\n        \r\n        # Classifier\r\n        output = self.classifier(x)\r\n        \r\n        if return_features:\r\n            features = {\r\n                'low': low_feat,\r\n                'mid': mid_feat,\r\n                'high': high_feat\r\n            }\r\n            return output, features\r\n        \r\n        return output\r\n\r\n\r\ndef count_parameters(model):\r\n    \"\"\"Count the number of trainable parameters\"\"\"\r\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(\"=\" * 70)\r\n    print(\"Standard MobileNetV3-Small Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Create model\r\n    model = StandardSegmentationModel(num_classes=21, pretrained=False)\r\n    \r\n    # Count parameters\r\n    params = count_parameters(model)\r\n    print(f\"\\nTotal parameters: {params:,} ({params/1e6:.2f}M)\")\r\n    \r\n    # Test forward pass\r\n    print(\"\\nTesting forward pass...\")\r\n    dummy_input = torch.randn(2, 3, 224, 224)\r\n    \r\n    with torch.no_grad():\r\n        output = model(dummy_input)\r\n        print(f\"  Output shape: {output.shape}\")\r\n        \r\n        # Test with feature extraction\r\n        output, features = model(dummy_input, return_features=True)\r\n        print(f\"\\n  Features for knowledge distillation:\")\r\n        for name, feat in features.items():\r\n            print(f\"    {name}: {feat.shape}\")\r\n    \r\n    # Score estimation\r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Score Estimation: Score = 4 × mIoU / (1 + params_M)\")\r\n    print(\"=\" * 70)\r\n    \r\n    for miou in [0.40, 0.45, 0.50, 0.55]:\r\n        score = 4 * miou / (1 + params/1e6)\r\n        print(f\"  mIoU={miou:.2f} → Score={score:.3f}\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n","time":1762545107282},{"kind":"opened","id":4,"time":1762545107282},{"kind":"documentEncountered","id":5,"relativePath":"train.py","time":1762545107284},{"kind":"setContent","id":5,"v":1,"content":"\"\"\"\r\nTraining Script for Compact Segmentation Models with Knowledge Distillation\r\nELEC 475 Lab 3\r\n\r\nSupports three training modes:\r\n1. No Knowledge Distillation (baseline)\r\n2. Response-based Knowledge Distillation\r\n3. Feature-based Knowledge Distillation\r\n\r\nReferences:\r\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\r\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\r\n\"\"\"\r\n\r\nimport argparse\r\nimport os\r\nimport time\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import VOCSegmentation\r\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\r\nfrom tqdm import tqdm\r\n\r\n# Import custom models\r\nfrom model_ultracompact import UltraCompactSegmentationModel\r\nfrom model_standard import StandardSegmentationModel\r\n\r\n\r\nclass VOCNormalize:\r\n    \"\"\"Custom normalization for VOC dataset\"\"\"\r\n    def __init__(self, augment=True):\r\n        self.augment = augment\r\n        self.normalize = transforms.Normalize(\r\n            mean=[0.485, 0.456, 0.406],\r\n            std=[0.229, 0.224, 0.225]\r\n        )\r\n    \r\n    def __call__(self, img, target):\r\n        # Convert PIL to tensor\r\n        img = transforms.ToTensor()(img)\r\n        \r\n        # Apply augmentation if training\r\n        if self.augment and np.random.rand() > 0.5:\r\n            # Random horizontal flip\r\n            img = transforms.functional.hflip(img)\r\n            target = transforms.functional.hflip(target)\r\n        \r\n        # Normalize image\r\n        img = self.normalize(img)\r\n        \r\n        # Convert target to tensor\r\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\r\n        \r\n        return img, target\r\n\r\n\r\ndef collate_fn(batch):\r\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\r\n    images, targets = zip(*batch)\r\n    return list(images), list(targets)\r\n\r\n\r\ndef calculate_miou(pred, target, num_classes=21):\r\n    \"\"\"\r\n    Calculate mean Intersection over Union (mIoU)\r\n    \r\n    Args:\r\n        pred: predicted segmentation mask (H, W)\r\n        target: ground truth segmentation mask (H, W)\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mIoU value\r\n    \"\"\"\r\n    pred = pred.cpu().numpy()\r\n    target = target.cpu().numpy()\r\n    \r\n    ious = []\r\n    for cls in range(num_classes):\r\n        pred_mask = (pred == cls)\r\n        target_mask = (target == cls)\r\n        \r\n        intersection = np.logical_and(pred_mask, target_mask).sum()\r\n        union = np.logical_or(pred_mask, target_mask).sum()\r\n        \r\n        if union == 0:\r\n            continue\r\n        \r\n        iou = intersection / union\r\n        ious.append(iou)\r\n    \r\n    return np.mean(ious) if len(ious) > 0 else 0.0\r\n\r\n\r\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Response-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        teacher_logits: teacher model output logits (B, C, H, W)\r\n        targets: ground truth labels (B, H, W)\r\n        temperature: softmax temperature for distillation\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Distillation loss (KL divergence)\r\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\r\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\r\n    \r\n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * kd_loss\r\n    \r\n    return total_loss, ce_loss.item(), kd_loss.item()\r\n\r\n\r\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\r\n    \"\"\"\r\n    Feature-based knowledge distillation loss\r\n    \r\n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\r\n    \r\n    Args:\r\n        student_logits: student model output logits (B, C, H, W)\r\n        student_features: dict of student intermediate features\r\n        teacher_features: dict of teacher intermediate features\r\n        targets: ground truth labels (B, H, W)\r\n        alpha: weight for cross-entropy loss\r\n        beta: weight for feature distillation loss\r\n    \r\n    Returns:\r\n        total loss\r\n    \"\"\"\r\n    # Standard cross-entropy loss\r\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\r\n    \r\n    # Feature matching loss (cosine similarity)\r\n    feat_loss = 0\r\n    num_levels = 0\r\n    \r\n    for level in ['low', 'mid', 'high']:\r\n        if level in student_features and level in teacher_features:\r\n            s_feat = student_features[level]\r\n            t_feat = teacher_features[level]\r\n            \r\n            # Resize if needed (teacher features might be different size)\r\n            if s_feat.shape != t_feat.shape:\r\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\r\n                # Match channels if needed\r\n                if s_feat.shape[1] != t_feat.shape[1]:\r\n                    # Project teacher features to student channel size\r\n                    continue  # Skip if channel mismatch (or add projection layer)\r\n            \r\n            # Cosine similarity loss (1 - cosine_similarity)\r\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\r\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\r\n            \r\n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\r\n            feat_loss += (1 - cosine_sim)\r\n            num_levels += 1\r\n    \r\n    if num_levels > 0:\r\n        feat_loss = feat_loss / num_levels\r\n    \r\n    # Combined loss\r\n    total_loss = alpha * ce_loss + beta * feat_loss\r\n    \r\n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\r\n\r\n\r\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\r\n    \"\"\"\r\n    Train for one epoch\r\n    \r\n    Args:\r\n        model: student model\r\n        teacher_model: teacher model (None if kd_mode='none')\r\n        dataloader: training data loader\r\n        optimizer: optimizer\r\n        device: device to run on\r\n        kd_mode: 'none', 'response', or 'feature'\r\n        args: training arguments\r\n    \r\n    Returns:\r\n        average loss, average ce_loss, average kd_loss\r\n    \"\"\"\r\n    model.train()\r\n    if teacher_model is not None:\r\n        teacher_model.eval()\r\n    \r\n    total_loss = 0\r\n    total_ce_loss = 0\r\n    total_kd_loss = 0\r\n    num_batches = 0\r\n    \r\n    pbar = tqdm(dataloader, desc='Training')\r\n    for images, targets in pbar:\r\n        # Move to device and resize to fixed size for batching\r\n        batch_imgs = []\r\n        batch_targets = []\r\n        \r\n        for img, target in zip(images, targets):\r\n            img = img.to(device)\r\n            target = target.to(device)\r\n            \r\n            # Resize to fixed size\r\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\r\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\r\n            \r\n            batch_imgs.append(img)\r\n            batch_targets.append(target)\r\n        \r\n        batch_imgs = torch.stack(batch_imgs)\r\n        batch_targets = torch.stack(batch_targets)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Forward pass based on KD mode\r\n        if kd_mode == 'none':\r\n            # No knowledge distillation\r\n            output = model(batch_imgs)\r\n            loss = F.cross_entropy(output, batch_targets, ignore_index=255)\r\n            ce_loss_val = loss.item()\r\n            kd_loss_val = 0.0\r\n            \r\n        elif kd_mode == 'response':\r\n            # Response-based KD\r\n            output = model(batch_imgs)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n            \r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n            \r\n        elif kd_mode == 'feature':\r\n            # Feature-based KD\r\n            output, student_features = model(batch_imgs, return_features=True)\r\n            with torch.no_grad():\r\n                teacher_output = teacher_model(batch_imgs)['out']\r\n                # Extract teacher features (for FCN-ResNet50, we'd need to modify it)\r\n                # For now, use output-based distillation\r\n                teacher_features = {}  # Would need to extract from teacher\r\n            \r\n            # Fallback to response-based if teacher features not available\r\n            loss, ce_loss_val, kd_loss_val = response_based_kd_loss(\r\n                output, teacher_output, batch_targets,\r\n                temperature=args.temperature,\r\n                alpha=args.alpha,\r\n                beta=args.beta\r\n            )\r\n        \r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        total_loss += loss.item()\r\n        total_ce_loss += ce_loss_val\r\n        total_kd_loss += kd_loss_val\r\n        num_batches += 1\r\n        \r\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    avg_ce_loss = total_ce_loss / num_batches\r\n    avg_kd_loss = total_kd_loss / num_batches\r\n    \r\n    return avg_loss, avg_ce_loss, avg_kd_loss\r\n\r\n\r\ndef validate(model, dataloader, device, num_classes=21):\r\n    \"\"\"\r\n    Validate the model\r\n    \r\n    Args:\r\n        model: model to validate\r\n        dataloader: validation data loader\r\n        device: device to run on\r\n        num_classes: number of classes\r\n    \r\n    Returns:\r\n        mean IoU\r\n    \"\"\"\r\n    model.eval()\r\n    all_ious = []\r\n    \r\n    with torch.no_grad():\r\n        for images, targets in tqdm(dataloader, desc='Validation'):\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Resize back to original size\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    return mean_iou\r\n\r\n\r\ndef plot_training_curves(history, save_path):\r\n    \"\"\"Plot and save training curves\"\"\"\r\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\r\n    \r\n    # Loss plot\r\n    axes[0].plot(history['train_loss'], label='Train Loss')\r\n    axes[0].set_xlabel('Epoch')\r\n    axes[0].set_ylabel('Loss')\r\n    axes[0].set_title('Training Loss')\r\n    axes[0].legend()\r\n    axes[0].grid(True)\r\n    \r\n    # mIoU plot\r\n    axes[1].plot(history['val_miou'], label='Validation mIoU')\r\n    axes[1].set_xlabel('Epoch')\r\n    axes[1].set_ylabel('mIoU')\r\n    axes[1].set_title('Validation mIoU')\r\n    axes[1].legend()\r\n    axes[1].grid(True)\r\n    \r\n    plt.tight_layout()\r\n    plt.savefig(save_path)\r\n    plt.close()\r\n    print(f\"Training curves saved to {save_path}\")\r\n\r\n\r\ndef main(args):\r\n    \"\"\"Main training function\"\"\"\r\n    \r\n    print(\"=\" * 70)\r\n    print(\"Training Compact Segmentation Model\")\r\n    print(\"=\" * 70)\r\n    print(f\"Model: {args.model}\")\r\n    print(f\"KD Mode: {args.kd_mode}\")\r\n    print(f\"Epochs: {args.epochs}\")\r\n    print(f\"Batch Size: {args.batch_size}\")\r\n    print(f\"Learning Rate: {args.lr}\")\r\n    print(\"=\" * 70)\r\n    \r\n    # Set device\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(f\"\\nUsing device: {device}\")\r\n    \r\n    # Create save directory\r\n    save_dir = Path(args.save_dir)\r\n    save_dir.mkdir(parents=True, exist_ok=True)\r\n    \r\n    # Load student model\r\n    print(\"\\nLoading student model...\")\r\n    if args.model == 'ultracompact':\r\n        model = UltraCompactSegmentationModel(num_classes=21, pretrained=True)\r\n    elif args.model == 'standard':\r\n        model = StandardSegmentationModel(num_classes=21, pretrained=True)\r\n    else:\r\n        raise ValueError(f\"Unknown model: {args.model}\")\r\n    \r\n    model = model.to(device)\r\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    print(f\"Student model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\r\n    \r\n    # Load teacher model if using KD\r\n    teacher_model = None\r\n    if args.kd_mode != 'none':\r\n        print(\"\\nLoading teacher model (FCN-ResNet50)...\")\r\n        weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\r\n        teacher_model = fcn_resnet50(weights=weights)\r\n        teacher_model = teacher_model.to(device)\r\n        teacher_model.eval()\r\n        for param in teacher_model.parameters():\r\n            param.requires_grad = False\r\n        print(\"Teacher model loaded and frozen\")\r\n    \r\n    # Prepare datasets\r\n    print(\"\\nPreparing datasets...\")\r\n    \r\n    # Find dataset path (Google Colab compatible)\r\n    # VOCSegmentation expects: root/VOCdevkit/VOC2012/\r\n    dataset_roots = []\r\n    \r\n    # Check for Google Colab structure (both with and without VOCdevkit wrapper)\r\n    colab_train_path = Path('/content/data/VOC2012_train_val')\r\n    if colab_train_path.exists():\r\n        # Check if it has VOCdevkit structure\r\n        if (colab_train_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(colab_train_path))\r\n        # Check if files are directly in the folder (no VOCdevkit wrapper)\r\n        elif (colab_train_path / 'JPEGImages').exists():\r\n            # Create a symlink or use parent structure\r\n            dataset_roots.append(str(colab_train_path.parent))\r\n    \r\n    # Check for local structure\r\n    local_data_path = Path('./data')\r\n    if local_data_path.exists():\r\n        if (local_data_path / 'VOCdevkit' / 'VOC2012').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n        elif (local_data_path / 'VOC2012' / 'JPEGImages').exists():\r\n            dataset_roots.append(str(local_data_path))\r\n    \r\n    # Check standard paths\r\n    dataset_roots.extend([\r\n        '/content/data',  # Google Colab default\r\n        './data',  # Local path\r\n        str(Path.home() / '.cache' / 'kagglehub' / 'datasets' / 'huanghanchina' / 'pascal-voc-2012' / 'versions' / '1'),\r\n    ])\r\n    \r\n    # Check dataset_path.txt\r\n    dataset_path_file = Path('dataset_path.txt')\r\n    if dataset_path_file.exists():\r\n        with open(dataset_path_file, 'r') as f:\r\n            saved_path = Path(f.read().strip())\r\n            dataset_roots.insert(0, str(saved_path.parent.parent))\r\n    \r\n    dataset_root = None\r\n    for root in dataset_roots:\r\n        try:\r\n            # Try to create a temporary symlink structure if needed\r\n            root_path = Path(root)\r\n            \r\n            # Check if we need to create VOCdevkit/VOC2012 structure\r\n            voc_path = root_path / 'VOC2012_train_val'\r\n            if voc_path.exists() and (voc_path / 'JPEGImages').exists() and not (voc_path / 'VOCdevkit').exists():\r\n                # Files are directly in VOC2012_train_val, create temporary structure\r\n                vocdevkit_path = voc_path / 'VOCdevkit'\r\n                voc2012_path = vocdevkit_path / 'VOC2012'\r\n                if not voc2012_path.exists():\r\n                    print(f\"Creating VOCdevkit structure in {voc_path}\")\r\n                    vocdevkit_path.mkdir(exist_ok=True)\r\n                    voc2012_path.symlink_to(voc_path, target_is_directory=True)\r\n            \r\n            test_dataset = VOCSegmentation(root=root, year='2012', image_set='train', download=False)\r\n            dataset_root = root\r\n            print(f\"✓ Successfully loaded dataset from: {root}\")\r\n            break\r\n        except Exception as e:\r\n            continue\r\n    \r\n    if dataset_root is None:\r\n        raise RuntimeError(\"Dataset not found! Please ensure dataset is in /content/data/VOC2012_train_val/ (Colab) or ./data/ (local)\")\r\n    \r\n    print(f\"Dataset found at: {dataset_root}\")\r\n    \r\n    # Create datasets\r\n    train_transform = VOCNormalize(augment=True)\r\n    val_transform = VOCNormalize(augment=False)\r\n    \r\n    train_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='train',\r\n        download=False,\r\n        transforms=train_transform\r\n    )\r\n    \r\n    val_dataset = VOCSegmentation(\r\n        root=dataset_root,\r\n        year='2012',\r\n        image_set='val',\r\n        download=False,\r\n        transforms=val_transform\r\n    )\r\n    \r\n    print(f\"Training set size: {len(train_dataset)}\")\r\n    print(f\"Validation set size: {len(val_dataset)}\")\r\n    \r\n    # Create dataloaders\r\n    train_loader = DataLoader(\r\n        train_dataset,\r\n        batch_size=args.batch_size,\r\n        shuffle=True,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    val_loader = DataLoader(\r\n        val_dataset,\r\n        batch_size=4,\r\n        shuffle=False,\r\n        num_workers=0,\r\n        collate_fn=collate_fn\r\n    )\r\n    \r\n    # Setup optimizer and scheduler\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\r\n    \r\n    # Training history\r\n    history = {\r\n        'train_loss': [],\r\n        'train_ce_loss': [],\r\n        'train_kd_loss': [],\r\n        'val_miou': [],\r\n        'epoch_times': []\r\n    }\r\n    \r\n    # Save hyperparameters\r\n    hyperparameters = {\r\n        'model': args.model,\r\n        'kd_mode': args.kd_mode,\r\n        'epochs': args.epochs,\r\n        'batch_size': args.batch_size,\r\n        'learning_rate': args.lr,\r\n        'weight_decay': args.weight_decay,\r\n        'temperature': args.temperature,\r\n        'alpha': args.alpha,\r\n        'beta': args.beta,\r\n        'optimizer': 'Adam',\r\n        'scheduler': 'CosineAnnealingLR',\r\n        'num_parameters': sum(p.numel() for p in model.parameters()),\r\n        'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    }\r\n    \r\n    best_miou = 0.0\r\n    training_start_time = time.time()\r\n    \r\n    # Training loop\r\n    print(\"\\nStarting training...\")\r\n    print(\"=\" * 70)\r\n    print(\"\\nHyperparameters:\")\r\n    for key, value in hyperparameters.items():\r\n        print(f\"  {key}: {value}\")\r\n    \r\n    for epoch in range(args.epochs):\r\n        epoch_start_time = time.time()\r\n        \r\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\r\n        print(\"-\" * 70)\r\n        \r\n        # Train\r\n        train_loss, train_ce_loss, train_kd_loss = train_epoch(\r\n            model, teacher_model, train_loader, optimizer, device, args.kd_mode, args\r\n        )\r\n        \r\n        print(f\"Train Loss: {train_loss:.4f} (CE: {train_ce_loss:.4f}, KD: {train_kd_loss:.4f})\")\r\n        \r\n        # Validate\r\n        val_miou = validate(model, val_loader, device)\r\n        print(f\"Validation mIoU: {val_miou:.4f}\")\r\n        \r\n        # Record epoch time\r\n        epoch_time = time.time() - epoch_start_time\r\n        history['epoch_times'].append(epoch_time)\r\n        print(f\"Epoch time: {epoch_time:.2f} seconds\")\r\n        \r\n        # Update history\r\n        history['train_loss'].append(train_loss)\r\n        history['train_ce_loss'].append(train_ce_loss)\r\n        history['train_kd_loss'].append(train_kd_loss)\r\n        history['val_miou'].append(val_miou)\r\n        \r\n        # Save best model\r\n        if val_miou > best_miou:\r\n            best_miou = val_miou\r\n            save_path = save_dir / f\"best_model_{args.model}_{args.kd_mode}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history\r\n            }, save_path)\r\n            print(f\"✓ Best model saved (mIoU: {val_miou:.4f})\")\r\n        \r\n        # Update scheduler\r\n        scheduler.step()\r\n        \r\n        # Save checkpoint every 10 epochs\r\n        if (epoch + 1) % 10 == 0:\r\n            checkpoint_path = save_dir / f\"checkpoint_{args.model}_{args.kd_mode}_epoch{epoch+1}.pth\"\r\n            torch.save({\r\n                'epoch': epoch,\r\n                'model_state_dict': model.state_dict(),\r\n                'optimizer_state_dict': optimizer.state_dict(),\r\n                'miou': val_miou,\r\n                'history': history,\r\n                'hyperparameters': hyperparameters\r\n            }, checkpoint_path)\r\n            print(f\"Checkpoint saved: {checkpoint_path}\")\r\n    \r\n    # Calculate total training time\r\n    total_training_time = time.time() - training_start_time\r\n    avg_epoch_time = np.mean(history['epoch_times'])\r\n    \r\n    # Plot training curves\r\n    plot_path = save_dir / f\"training_curves_{args.model}_{args.kd_mode}.png\"\r\n    plot_training_curves(history, plot_path)\r\n    \r\n    # Save final training report\r\n    report_path = save_dir / f\"training_report_{args.model}_{args.kd_mode}.txt\"\r\n    with open(report_path, 'w') as f:\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n        f.write(f\"Training Report: {args.model} with {args.kd_mode} KD\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\\n\")\r\n        \r\n        f.write(\"HYPERPARAMETERS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        for key, value in hyperparameters.items():\r\n            f.write(f\"  {key:25s}: {value}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"TRAINING STATISTICS:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\\n\")\r\n        f.write(f\"  Average epoch time: {avg_epoch_time:.2f} seconds\\n\")\r\n        f.write(f\"  Best validation mIoU: {best_miou:.4f}\\n\")\r\n        f.write(f\"  Final validation mIoU: {history['val_miou'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final CE loss: {history['train_ce_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final KD loss: {history['train_kd_loss'][-1]:.4f}\\n\")\r\n        \r\n        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\r\n        f.write(\"FILES GENERATED:\\n\")\r\n        f.write(\"-\" * 70 + \"\\n\")\r\n        f.write(f\"  Best model: best_model_{args.model}_{args.kd_mode}.pth\\n\")\r\n        f.write(f\"  Training curves: training_curves_{args.model}_{args.kd_mode}.png\\n\")\r\n        f.write(f\"  This report: training_report_{args.model}_{args.kd_mode}.txt\\n\")\r\n        f.write(\"=\" * 70 + \"\\n\")\r\n    \r\n    print(\"\\n\" + \"=\" * 70)\r\n    print(\"Training completed!\")\r\n    print(f\"Total training time: {total_training_time/3600:.2f} hours ({total_training_time:.2f} seconds)\")\r\n    print(f\"Average epoch time: {avg_epoch_time:.2f} seconds\")\r\n    print(f\"Best validation mIoU: {best_miou:.4f}\")\r\n    print(f\"Model saved to: {save_dir / f'best_model_{args.model}_{args.kd_mode}.pth'}\")\r\n    print(f\"Training report saved to: {report_path}\")\r\n    \r\n    # Calculate final score\r\n    score = 4 * best_miou / (1 + num_params / 1e6)\r\n    print(f\"\\nFinal Score: {score:.3f}\")\r\n    print(f\"  (Score = 4 × {best_miou:.4f} / (1 + {num_params/1e6:.2f}) = {score:.3f})\")\r\n    print(\"=\" * 70)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Train compact segmentation model')\r\n    \r\n    # Model arguments\r\n    parser.add_argument('--model', type=str, default='ultracompact', choices=['ultracompact', 'standard'],\r\n                        help='Model type to train')\r\n    parser.add_argument('--kd_mode', type=str, default='none', choices=['none', 'response', 'feature'],\r\n                        help='Knowledge distillation mode')\r\n    \r\n    # Training arguments\r\n    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\r\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\r\n    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\r\n    \r\n    # KD arguments\r\n    parser.add_argument('--temperature', type=float, default=3.0, help='Temperature for response-based KD')\r\n    parser.add_argument('--alpha', type=float, default=0.5, help='Weight for CE loss')\r\n    parser.add_argument('--beta', type=float, default=0.5, help='Weight for KD loss')\r\n    \r\n    # Other arguments\r\n    parser.add_argument('--save_dir', type=str, default='./checkpoints', help='Directory to save models')\r\n    \r\n    args = parser.parse_args()\r\n    \r\n    main(args)\r\n","time":1762622282025},{"kind":"opened","id":5,"time":1762622282025},{"kind":"documentEncountered","id":17,"relativePath":"train.txt","time":1762622441478},{"kind":"setContent","id":17,"v":1,"content":"# ELEC 475 Lab 3 - Training Commands\r\n# All commands should be run from the Lab 3 directory\r\n\r\n# ============================================================================\r\n# ULTRA-COMPACT MODEL (475K parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# STANDARD MODEL (3.4M parameters)\r\n# ============================================================================\r\n\r\n# Baseline (No Knowledge Distillation)\r\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\r\n\r\n# Response-based Knowledge Distillation\r\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\r\n\r\n# Feature-based Knowledge Distillation\r\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\r\n\r\n# ============================================================================\r\n# TRAINING OUTPUT FILES\r\n# ============================================================================\r\n# For each training run, the following files will be generated in ./checkpoints/:\r\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\r\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\r\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\r\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\r\n\r\n# ============================================================================\r\n# OPTIONAL: HYPERPARAMETER TUNING\r\n# ============================================================================\r\n# If you want to experiment with different hyperparameters:\r\n\r\n# Adjust learning rate\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\r\n\r\n# Adjust KD temperature (higher = softer targets)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\r\n\r\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\r\n\r\n# Adjust batch size (if memory issues)\r\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\r\n\r\n# ============================================================================\r\n# NOTES\r\n# ============================================================================\r\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\r\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\r\n# - Monitor validation mIoU - training should converge after 30-40 epochs\r\n# - Best model is saved automatically when validation mIoU improves\r\n","time":1762622441593},{"kind":"opened","id":17,"time":1762622441593},{"kind":"changed","id":4,"v":2,"edit":[[50,50," (Optimized)"],[94,94,"1."],[95,95,"6M (reduced from 3"],[99,99,")"],[119,120,"8"],[122,123,"1"],[124,125,"2"]],"time":1762623638975},{"kind":"changed","id":4,"v":3,"edit":[[2834,2834," (Optimized)"],[2919,2920,"Lightweight 2"],[2951,2955,""],[2956,2956," - reduced from 3 rates"],[2964,2972,"Simplifi"],[2973,2973,"d"],[2987,2991,"reduced"],[2993,2994,"ha"],[2997,3002,"l"],[3024,3024,"1."],[3025,3025,"6M (reduced from 3"],[3029,3029,")"],[3767,3767,"Lightweight "],[3779,3782,"(2"],[3783,3786,"ra"],[3787,3787,"es "],[3788,3789,"n"],[3790,3790,"tead of 3, redu"],[3791,3793,""],[3794,3794,"d output"],[3796,3797,"han"],[3798,3799,""],[3800,3802,"ls)"],[3859,3862,"64"],[3876,3880,""],[3931,3933,"r"],[3934,3936,""],[3937,3939,"uc"],[3940,3940,"d cha"],[3941,3944,""],[3945,3945,"el"],[4015,4017,"16"],[4063,4065,"16"],[4191,4193,"8"],[4239,4241,"8"],[4310,4310," Simplified"],[4311,4312,"d"],[4324,4328,"reduced"],[4330,4331,"ha"],[4334,4339,"l"],[4409,4412,"64"],[4415,4417,"16"],[4419,4422,"64"],[4479,4482,"64"],[4609,4612,"64"],[4615,4617,"8"],[4619,4621,"32"],[4678,4680,"32"],[4861,4863,"32"]],"time":1762623641194},{"kind":"changed","id":5,"v":2,"edit":[[10251,10251,"_iou:"],[10252,10252,"mean "],[10255,10255," across all samples"],[10261,10261,"    mean_loss: mean cross-entropy loss\r\n    "],[10307,10307,"all_losses = []"],[10313,10313,"criterion = nn.CrossEntropyLoss(ignore_index=255)\r\n    \r\n    "],[10403,10403,"\r\n            for img, target in zip(images, targets):\r\n                img = img.unsqueeze(0).to(device)\r\n                target = target.to(device)\r\n                \r\n                # Resize to fixed size\r\n                h, w = target.shape\r\n                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\r\n                \r\n                # Forward pass\r\n                output = model(img)\r\n                \r\n                # Calculate loss at this resolution\r\n                target_resized = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), \r\n                                               size=output.shape[-2:], \r\n                                               mode='nearest').squeeze().long()\r\n                loss = criterion(output, target_resized.unsqueeze(0))\r\n                all_losses.append(loss.item())\r\n                \r\n                # Resize back to original size for IoU calculation\r\n                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\r\n                \r\n                # Get predictions\r\n                pred = output.squeeze(0).argmax(0)\r\n                \r\n                # Calculate IoU\r\n                iou = calculate_miou(pred, target, num_classes)\r\n                all_ious.append(iou)\r\n    \r\n    mean_iou = np.mean(all_ious)\r\n    mean_loss = np.mean(all_losses)\r\n    return mean_iou, mean_loss"]],"time":1762623765832},{"kind":"changed","id":5,"v":3,"edit":[[11970,12883,""]],"time":1762623780348},{"kind":"changed","id":5,"v":4,"edit":[[12136,12136," - now includes both train and validation loss"],[12188,12188,"in Loss', l"],[12190,12190,"ewidth=2)\r\n    axes[0].plot(history['val_loss'], label='Val"],[12196,12196,", linewidth=2, linestyle='--'"],[12295,12295," and Validation"],[12434,12434,", linewidth=2"],[12645,12645,", dpi=150"]],"time":1762623791607},{"kind":"changed","id":5,"v":5,"edit":[[19126,19126,"loss': [],\r\n        'val_"]],"time":1762623803059},{"kind":"changed","id":5,"v":6,"edit":[[20600,20600,","],[20601,20601,"val_loss "],[20687,20687,", Loss: {val_loss:.4f}"],[21079,21079,"d_loss)\r\n        history['val_loss'].appen"],[21080,21080,"(val"]],"time":1762623816438},{"kind":"changed","id":5,"v":7,"edit":[[23689,23689,"validation loss: {history['val_loss'][-1]:.4f}\\n\")\r\n        f.write(f\"  Final "]],"time":1762623835010},{"kind":"selectionChanged","id":17,"selection":[[883,883]],"time":1762624017851},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624140244},{"kind":"selectionChanged","id":17,"selection":[[869,870]],"time":1762624140330},{"kind":"selectionChanged","id":17,"selection":[[868,870]],"time":1762624140356},{"kind":"selectionChanged","id":17,"selection":[[867,870]],"time":1762624140378},{"kind":"changed","id":17,"v":7,"edit":[[867,870,"1"]],"time":1762624143421},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624143443},{"kind":"changed","id":17,"v":9,"edit":[[868,869,""]],"time":1762624143557},{"kind":"selectionChanged","id":17,"selection":[[849,849]],"time":1762624144669},{"kind":"selectionChanged","id":17,"selection":[[870,870]],"time":1762624146159},{"kind":"selectionChanged","id":17,"selection":[[869,869]],"time":1762624146967},{"kind":"changed","id":17,"v":13,"edit":[[868,869,""]],"time":1762624150477},{"kind":"selectionChanged","id":17,"selection":[[868,868]],"time":1762624150514},{"kind":"changed","id":17,"v":15,"edit":[[868,869,""]],"time":1762624151107},{"kind":"selectionChanged","id":17,"selection":[[849,849]],"time":1762624151684},{"kind":"changed","id":17,"v":17,"edit":[[849,849," "]],"time":1762624152194},{"kind":"selectionChanged","id":17,"selection":[[850,850]],"time":1762624152207}]},"resultEdit":"   16  16 \n   17  17 # ============================================================================\n-  18     # STANDARD MODEL (1arameters)\n+      18 # STANDARD MODEL (1.3M parameters)\n   19  19 # ============================================================================\n   20  20 \n   21  21 # Baseline (No Knowledge Distillation)","isCachedResult":true,"prompt":"System\n------\nYour role as an AI assistant is to help developers complete their code tasks by assisting in editing specific sections of code marked by the <|code_to_edit|> and <|/code_to_edit|> tags, while adhering to Microsoft's content policies and avoiding the creation of content that violates copyrights.\n\nYou have access to the following information to help you make informed suggestions:\n\n- recently_viewed_code_snippets: These are code snippets that the developer has recently looked at, which might provide context or examples relevant to the current task. They are listed from oldest to newest, with line numbers in the form #| to help you understand the edit diff history. It's possible these are entirely irrelevant to the developer's change.\n- current_file_content: The content of the file the developer is currently working on, providing the broader context of the code. Line numbers in the form #| are included to help you understand the edit diff history.\n- edit_diff_history: A record of changes made to the code, helping you understand the evolution of the code and the developer's intentions. These changes are listed from oldest to latest. It's possible a lot of old edit diff history is entirely irrelevant to the developer's change.\n- area_around_code_to_edit: The context showing the code surrounding the section to be edited.\n- cursor position marked as <|cursor|>: Indicates where the developer's cursor is currently located, which can be crucial for understanding what part of the code they are focusing on.\n\nYour task is to predict and complete the changes the developer would have made next in the <|code_to_edit|> section. The developer may have stopped in the middle of typing. Your goal is to keep the developer on the path that you think they're following. Some examples include further implementing a class, method, or variable, or improving the quality of the code. Make sure the developer doesn't get distracted and ensure your suggestion is relevant. Consider what changes need to be made next, if any. If you think changes should be made, ask yourself if this is truly what needs to happen. If you are confident about it, then proceed with the changes.\n\n# Steps\n\n1. **Review Context**: Analyze the context from the resources provided, such as recently viewed snippets, edit history, surrounding code, and cursor location.\n2. **Evaluate Current Code**: Determine if the current code within the tags requires any corrections or enhancements.\n3. **Suggest Edits**: If changes are required, ensure they align with the developer's patterns and improve code quality.\n4. **Maintain Consistency**: Ensure indentation and formatting follow the existing code style.\n\n# Output Format\n\n- Provide only the revised code within the tags. If no changes are necessary, simply return the original code from within the <|code_to_edit|> and <|/code_to_edit|> tags.\n- There are line numbers in the form #| in the code displayed to you above, but these are just for your reference. Please do not include the numbers of the form #| in your response.\n- Ensure that you do not output duplicate code that exists outside of these tags. The output should be the revised code that was between these tags and should not include the <|code_to_edit|> or <|/code_to_edit|> tags.\n\n```\n// Your revised code goes here\n```\n\n# Notes\n\n- Apologize with \"Sorry, I can't assist with that.\" for requests that may breach Microsoft content guidelines.\n- Avoid undoing or reverting the developer's last change unless there are obvious typos or errors.\n- Don't include the line numbers of the form #| in your response.\n==================\nUser\n------\n```\n<|recently_viewed_code_snippets|>\n<|recently_viewed_code_snippet|>\ncode_snippet_file_path: /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py (truncated)\n\"\"\"\nTraining Script for Compact Segmentation Models with Knowledge Distillation\nELEC 475 Lab 3\n\nSupports three training modes:\n1. No Knowledge Distillation (baseline)\n2. Response-based Knowledge Distillation\n3. Feature-based Knowledge Distillation\n\nReferences:\n- Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\n- Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2014)\n\"\"\"\n\nimport argparse\nimport os\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import VOCSegmentation\nfrom torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\nfrom tqdm import tqdm\n\n# Import custom models\nfrom model_ultracompact import UltraCompactSegmentationModel\nfrom model_standard import StandardSegmentationModel\n\n\nclass VOCNormalize:\n    \"\"\"Custom normalization for VOC dataset\"\"\"\n    def __init__(self, augment=True):\n        self.augment = augment\n        self.normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    \n    def __call__(self, img, target):\n        # Convert PIL to tensor\n        img = transforms.ToTensor()(img)\n        \n        # Apply augmentation if training\n        if self.augment and np.random.rand() > 0.5:\n            # Random horizontal flip\n            img = transforms.functional.hflip(img)\n            target = transforms.functional.hflip(target)\n        \n        # Normalize image\n        img = self.normalize(img)\n        \n        # Convert target to tensor\n        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n        \n        return img, target\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function to handle variable-sized images\"\"\"\n    images, targets = zip(*batch)\n    return list(images), list(targets)\n\n\ndef calculate_miou(pred, target, num_classes=21):\n    \"\"\"\n    Calculate mean Intersection over Union (mIoU)\n    \n    Args:\n        pred: predicted segmentation mask (H, W)\n        target: ground truth segmentation mask (H, W)\n        num_classes: number of classes\n    \n    Returns:\n        mIoU value\n    \"\"\"\n    pred = pred.cpu().numpy()\n    target = target.cpu().numpy()\n    \n    ious = []\n    for cls in range(num_classes):\n        pred_mask = (pred == cls)\n        target_mask = (target == cls)\n        \n        intersection = np.logical_and(pred_mask, target_mask).sum()\n        union = np.logical_or(pred_mask, target_mask).sum()\n        \n        if union == 0:\n            continue\n        \n        iou = intersection / union\n        ious.append(iou)\n    \n    return np.mean(ious) if len(ious) > 0 else 0.0\n\n\ndef response_based_kd_loss(student_logits, teacher_logits, targets, temperature=3.0, alpha=0.5, beta=0.5):\n    \"\"\"\n    Response-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × KL_Div(student_soft, teacher_soft) × T²\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        teacher_logits: teacher model output logits (B, C, H, W)\n        targets: ground truth labels (B, H, W)\n        temperature: softmax temperature for distillation\n        alpha: weight for cross-entropy loss\n        beta: weight for distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Distillation loss (KL divergence)\n    student_soft = F.log_softmax(student_logits / temperature, dim=1)\n    teacher_soft = F.softmax(teacher_logits / temperature, dim=1)\n    \n    kd_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * kd_loss\n    \n    return total_loss, ce_loss.item(), kd_loss.item()\n\n\ndef feature_based_kd_loss(student_logits, student_features, teacher_features, targets, alpha=0.5, beta=0.5):\n    \"\"\"\n    Feature-based knowledge distillation loss\n    \n    Loss = α × CE(student, targets) + β × Σ CosineLoss(student_feat, teacher_feat)\n    \n    Args:\n        student_logits: student model output logits (B, C, H, W)\n        student_features: dict of student intermediate features\n        teacher_features: dict of teacher intermediate features\n        targets: ground truth labels (B, H, W)\n        alpha: weight for cross-entropy loss\n        beta: weight for feature distillation loss\n    \n    Returns:\n        total loss\n    \"\"\"\n    # Standard cross-entropy loss\n    ce_loss = F.cross_entropy(student_logits, targets, ignore_index=255)\n    \n    # Feature matching loss (cosine similarity)\n    feat_loss = 0\n    num_levels = 0\n    \n    for level in ['low', 'mid', 'high']:\n        if level in student_features and level in teacher_features:\n            s_feat = student_features[level]\n            t_feat = teacher_features[level]\n            \n            # Resize if needed (teacher features might be different size)\n            if s_feat.shape != t_feat.shape:\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n                # Match channels if needed\n                if s_feat.shape[1] != t_feat.shape[1]:\n                    # Project teacher features to student channel size\n                    continue  # Skip if channel mismatch (or add projection layer)\n            \n            # Cosine similarity loss (1 - cosine_similarity)\n            s_feat_norm = F.normalize(s_feat, p=2, dim=1)\n            t_feat_norm = F.normalize(t_feat, p=2, dim=1)\n            \n            cosine_sim = (s_feat_norm * t_feat_norm).sum(dim=1).mean()\n            feat_loss += (1 - cosine_sim)\n            num_levels += 1\n    \n    if num_levels > 0:\n        feat_loss = feat_loss / num_levels\n    \n    # Combined loss\n    total_loss = alpha * ce_loss + beta * feat_loss\n    \n    return total_loss, ce_loss.item(), feat_loss.item() if num_levels > 0 else 0.0\n\n\ndef train_epoch(model, teacher_model, dataloader, optimizer, device, kd_mode='none', args=None):\n    \"\"\"\n    Train for one epoch\n    \n    Args:\n        model: student model\n        teacher_model: teacher model (None if kd_mode='none')\n        dataloader: training data loader\n        optimizer: optimizer\n        device: device to run on\n        kd_mode: 'none', 'response', or 'feature'\n        args: training arguments\n    \n    Returns:\n        average loss, average ce_loss, average kd_loss\n    \"\"\"\n    model.train()\n    if teacher_model is not None:\n        teacher_model.eval()\n    \n    total_loss = 0\n    total_ce_loss = 0\n    total_kd_loss = 0\n    num_batches = 0\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for images, targets in pbar:\n        # Move to device and resize to fixed size for batching\n        batch_imgs = []\n        batch_targets = []\n        \n        for img, target in zip(images, targets):\n            img = img.to(device)\n            target = target.to(device)\n            \n            # Resize to fixed size\n            img = F.interpolate(img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False).squeeze(0)\n            target = F.interpolate(target.unsqueeze(0).unsqueeze(0).float(), size=(256, 256), mode='nearest').squeeze(0).squeeze(0).long()\n            \n            batch_imgs.append(img)\n            batch_targets.append(target)\n        \n        batch_imgs = torch.stack(batch_imgs)\n<|/recently_viewed_code_snippet|>\n<|/recently_viewed_code_snippets|>\n\n<|current_file_content|>\ncurrent_file_path: train.txt\n# ELEC 475 Lab 3 - Training Commands\n# All commands should be run from the Lab 3 directory\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n <|cursor|># STANDARD MODEL (1arameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n#   - best_model_{model}_{kd_mode}.pth           (best model weights)\n#   - training_curves_{model}_{kd_mode}.png      (loss and mIoU plots)\n#   - training_report_{model}_{kd_mode}.txt      (hyperparameters and timing)\n#   - checkpoint_{model}_{kd_mode}_epoch*.pth    (periodic checkpoints)\n\n# ============================================================================\n# OPTIONAL: HYPERPARAMETER TUNING\n# ============================================================================\n# If you want to experiment with different hyperparameters:\n\n# Adjust learning rate\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.0005\n\n# Adjust KD temperature (higher = softer targets)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --temperature 5.0\n\n# Adjust loss weights (higher alpha = more focus on ground truth, higher beta = more focus on teacher)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --alpha 0.7 --beta 0.3\n\n# Adjust batch size (if memory issues)\n# python train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 4\n\n# ============================================================================\n# NOTES\n# ============================================================================\n# - Training on CPU will be slow. Consider using Google Colab with GPU.\n# - Expected training time: 2-4 hours per model on GPU, 10-20 hours on CPU\n# - Monitor validation mIoU - training should converge after 30-40 epochs\n# - Best model is saved automatically when validation mIoU improves\n\n<|/current_file_content|>\n\n<|edit_diff_history|>\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -342,23 +342,0 @@\n-            for img, target in zip(images, targets):\n-                img = img.unsqueeze(0).to(device)\n-                target = target.to(device)\n-                \n-                # Resize to fixed size\n-                h, w = target.shape\n-                img = F.interpolate(img, size=(256, 256), mode='bilinear', align_corners=False)\n-                \n-                # Forward pass\n-                output = model(img)\n-                \n-                # Resize back to original size\n-                output = F.interpolate(output, size=(h, w), mode='bilinear', align_corners=False)\n-                \n-                # Get predictions\n-                pred = output.squeeze(0).argmax(0)\n-                \n-                # Calculate IoU\n-                iou = calculate_miou(pred, target, num_classes)\n-                all_ious.append(iou)\n-    \n-    mean_iou = np.mean(all_ious)\n-    return mean_iou\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -348,1 +348,1 @@\n-    # Loss plot\n+    # Loss plot - now includes both train and validation loss\n@@ -349,1 +349,2 @@\n-    axes[0].plot(history['train_loss'], label='Train Loss')\n+    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n+    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')\n@@ -352,1 +352,1 @@\n-    axes[0].set_title('Training Loss')\n+    axes[0].set_title('Training and Validation Loss')\n@@ -357,1 +357,1 @@\n-    axes[1].plot(history['val_miou'], label='Validation mIoU')\n+    axes[1].plot(history['val_miou'], label='Validation mIoU', linewidth=2)\n@@ -365,1 +365,1 @@\n-    plt.savefig(save_path)\n+    plt.savefig(save_path, dpi=150)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -535,1 +535,2 @@\n-        'val_miou': [],\n+        'val_loss': [],\n+        'val_miou': [],\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -581,1 +581,1 @@\n-        val_miou = validate(model, val_loader, device)\n+        val_miou, val_loss = validate(model, val_loader, device)\n@@ -582,1 +582,1 @@\n-        print(f\"Validation mIoU: {val_miou:.4f}\")\n+        print(f\"Validation mIoU: {val_miou:.4f}, Loss: {val_loss:.4f}\")\n@@ -592,1 +592,2 @@\n-        history['train_kd_loss'].append(train_kd_loss)\n+        history['train_kd_loss'].append(train_kd_loss)\n+        history['val_loss'].append(val_loss)\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.py\n@@ -652,1 +652,2 @@\n-        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\\n\")\n+        f.write(f\"  Final train loss: {history['train_loss'][-1]:.4f}\\n\")\n\n--- /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n+++ /c:/Users/jcube/OneDrive/Desktop/Jacob/School/Queens/Year 5/ELEC 475/Lab 3/train.txt\n@@ -17,1 +17,1 @@\n-# STANDARD MODEL (3.4M parameters)\n+ # STANDARD MODEL (1arameters)\n\n<|/edit_diff_history|>\n\n<|area_around_code_to_edit|>\n\n# ============================================================================\n# ULTRA-COMPACT MODEL (475K parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model ultracompact --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n# Response-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model ultracompact --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n<|code_to_edit|>\n\n# ============================================================================\n <|cursor|># STANDARD MODEL (1arameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n<|/code_to_edit|>\n# Response-based Knowledge Distillation\npython train.py --model standard --kd_mode response --epochs 50 --batch_size 8 --lr 0.001 --temperature 3.0 --alpha 0.5 --beta 0.5\n\n# Feature-based Knowledge Distillation\npython train.py --model standard --kd_mode feature --epochs 50 --batch_size 8 --lr 0.001 --alpha 0.5 --beta 0.5\n\n# ============================================================================\n# TRAINING OUTPUT FILES\n# ============================================================================\n# For each training run, the following files will be generated in ./checkpoints/:\n<|/area_around_code_to_edit|>\n```\n\nThe developer was working on a section of code within the tags `code_to_edit` in the file located at `train.txt`. Using the given `recently_viewed_code_snippets`, `current_file_content`, `edit_diff_history`, `area_around_code_to_edit`, and the cursor position marked as `<|cursor|>`, please continue the developer's work. Update the `code_to_edit` section by predicting and completing the changes they would have made next. Provide the revised code that was between the `<|code_to_edit|>` and `<|/code_to_edit|>` tags with the following format, but do not include the tags themselves.\n```\n// Your revised code goes here\n```\n==================","error":"undefined","response":"```\n\n# ============================================================================\n# STANDARD MODEL (1.3M parameters)\n# ============================================================================\n\n# Baseline (No Knowledge Distillation)\npython train.py --model standard --kd_mode none --epochs 50 --batch_size 8 --lr 0.001\n\n```","responseResults":"- replaceRange:\n   start: 849\n   endExclusive: 879\n  newText: \"# STANDARD MODEL (1.3M parameters)\"\n","logs":["```\n{\n\t\"isAsyncCompletions\": true,\n\t\"isRevisedCacheStrategy\": true,\n\t\"isCacheTracksRejections\": true,\n\t\"isRecentlyShownCacheEnabled\": false,\n\t\"debounceUseCoreRequestTime\": false\n}\n```\n"]}