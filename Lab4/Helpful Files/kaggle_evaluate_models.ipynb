{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELEC 475 Lab 4: Model Evaluation - Recall@K Comparison\n",
    "\n",
    "**Evaluate trained models and compare Recall@K metrics**\n",
    "\n",
    "---\n",
    "\n",
    "## \u26a0\ufe0f Before Running:\n",
    "\n",
    "1. **Upload your model checkpoints** as Kaggle datasets:\n",
    "   - Create dataset: `your-username/clip-baseline-model` with `best_model.pth`\n",
    "   - Create dataset: `your-username/clip-batchnorm-model` with `best_model.pth`\n",
    "   - Create dataset: `your-username/clip-dropout-model` with `best_model.pth` (if trained)\n",
    "\n",
    "2. **Add datasets**:\n",
    "   - `jeffaudi/coco-2014-dataset-for-yolov3`\n",
    "   - `jacobbadali2/elec-475-lab4`\n",
    "   - Your model checkpoint datasets\n",
    "\n",
    "3. **Enable GPU**: T4 or P100\n",
    "4. **Enable Internet**: ON\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ENVIRONMENT CHECK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Kaggle: {'KAGGLE_KERNEL_RUN_TYPE' in os.environ}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install -q transformers torch torchvision tqdm pillow matplotlib\n",
    "print(\"\u2713 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone Repository & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import shutil\n",
    "\n",
    "if os.path.exists('475_ML-CV_Labs'):\n",
    "    shutil.rmtree('475_ML-CV_Labs')\n",
    "    print(\"\u2713 Removed old repo\")\n",
    "\n",
    "!git clone https://github.com/Jcub05/475_ML-CV_Labs.git\n",
    "os.chdir('475_ML-CV_Labs/Lab4')\n",
    "print(f\"\u2713 Fresh clone complete\\nDirectory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kaggle-optimized files\n",
    "import shutil\n",
    "\n",
    "shutil.copy('dataset.py', 'dataset_original.py')\n",
    "\n",
    "# Write fixed dataset.py directly to ensure subset logic is present\n",
    "dataset_code = r'''\"\"\"\nSimplified COCO dataset loader for Kaggle that uses cached text embeddings.\nCompatible with embeddings cached by cache_text_embeddings.py\nUses ALL 5 captions per image for 5x more training data.\n\"\"\"\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom pathlib import Path\nimport os\n\n\nclass COCOCachedDataset(Dataset):\n    \"\"\"Dataset that uses pre-cached text embeddings from cache_text_embeddings.py\"\"\"\n    \n    def __init__(self, images_dir, embeddings_file, transform=None):\n        self.images_dir = Path(images_dir)\n        self.transform = transform\n        \n        # Load cached embeddings\n        print(f\"Loading embeddings from {embeddings_file}...\")\n        embeddings_cache = torch.load(embeddings_file)\n        \n        # embeddings_cache is a dict: {\"image_id_caption_idx\": tensor, ...}\n        # We use ALL captions per image (not just first) for more training data\n        \n        # Store all embeddings with their image_id and caption index\n        all_embeddings = []\n        all_image_ids = []\n        all_caption_indices = []\n        \n        for key, embedding in embeddings_cache.items():\n            # Key format: \"image_id_caption_idx\" (e.g., \"391895_0\")\n            image_id_str, caption_idx = key.rsplit('_', 1)\n            image_id = int(image_id_str)\n            \n            # Use ALL captions (not just caption_idx == 0)\n            all_embeddings.append(embedding)\n            all_image_ids.append(image_id)\n            all_caption_indices.append(int(caption_idx))\n        \n        # Build image paths and filter out missing images\n        split_name = 'train' if 'train' in str(images_dir) else 'val'\n        valid_image_ids = []\n        valid_embeddings = []\n        valid_paths = []\n        valid_caption_indices = []\n        missing_count = 0\n        \n        for i, (img_id, caption_idx) in enumerate(zip(all_image_ids, all_caption_indices)):\n            img_path = self.images_dir / f\"COCO_{split_name}2014_{img_id:012d}.jpg\"\n            \n            # Only include if image file exists\n            if img_path.exists():\n                valid_image_ids.append(img_id)\n                valid_embeddings.append(all_embeddings[i])\n                valid_paths.append(img_path)\n                valid_caption_indices.append(caption_idx)\n            else:\n                missing_count += 1\n        \n        self.image_ids = valid_image_ids\n        self.embeddings = torch.stack(valid_embeddings) if valid_embeddings else torch.empty(0)\n        self.image_paths = valid_paths\n        self.caption_indices = valid_caption_indices\n        \n        print(f\"\u2713 Loaded {len(self)} samples with embeddings\")\n        if missing_count > 0:\n            print(f\"  \u26a0 Skipped {missing_count} samples with missing images\")\n    \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):\n        # Load image\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Get cached text embedding\n        text_embedding = self.embeddings[idx]\n        \n        return {\n            'image': image,\n            'text_embedding': text_embedding,\n            'image_id': self.image_ids[idx]\n        }\n\n\ndef create_dataloaders(config=None, data_root=None, train_images_dir=None, \n                       val_images_dir=None, cache_dir=None, batch_size=64,\n                       num_workers=2, pin_memory=True, **kwargs):\n    \"\"\"\n    Create dataloaders using cached embeddings.\n    Accepts both Config object and individual arguments for compatibility.\n    \"\"\"\n    \n    # Handle Config object or individual arguments\n    if config is not None and hasattr(config, 'train_images_path'):\n        train_images_path = config.train_images_path\n        val_images_path = config.val_images_path\n        cache_path = config.cache_path\n        batch_size = config.batch_size\n        num_workers = config.num_workers\n        pin_memory = config.pin_memory\n        image_size = config.image_size\n        clip_mean = config.clip_mean\n        clip_std = config.clip_std\n    else:\n        # Use individual arguments\n        train_images_path = Path(data_root) / (train_images_dir or \"images/train2014\")\n        val_images_path = Path(data_root) / (val_images_dir or \"images/val2014\")\n        cache_path = Path(cache_dir) if cache_dir else Path(\"/kaggle/input/elec-475-lab4\")\n        image_size = 224\n        clip_mean = (0.48145466, 0.4578275, 0.40821073)\n        clip_std = (0.26862954, 0.26130258, 0.27577711)\n    \n    # CLIP preprocessing\n    transform = transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=clip_mean, std=clip_std)\n    ])\n    \n    # Create datasets\n    train_dataset = COCOCachedDataset(\n        images_dir=train_images_path,\n        embeddings_file=cache_path / \"text_embeddings_train.pt\",\n        transform=transform\n    )\n    \n    val_dataset = COCOCachedDataset(\n        images_dir=val_images_path,\n        embeddings_file=cache_path / \"text_embeddings_val.pt\",\n        transform=transform\n    )\n    \n    # Apply subset if requested\n    use_subset = kwargs.get('use_subset', False) or (config and getattr(config, 'use_subset', False))\n    subset_size = kwargs.get('subset_size', 10000) or (config and getattr(config, 'subset_size', 10000))\n    \n    if use_subset:\n        print(f\"Subsetting validation set to {subset_size} samples...\")\n        if len(val_dataset) > subset_size:\n            indices = torch.randperm(len(val_dataset))[:subset_size]\n            val_dataset = torch.utils.data.Subset(val_dataset, indices)\n            print(f\"\u2713 Validation subset created: {len(val_dataset)} samples\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory\n    )\n    \n    return train_loader, val_loader\n'''\n",
    "\n",
    "with open('dataset.py', 'w') as f:\n",
    "    f.write(dataset_code)\n",
    "\n",
    "print(\"\u2713 Using Kaggle-compatible dataset loader (with subset fix)\")\n",
    "\n",
    "shutil.copy('metrics.py', 'metrics_original.py')\n",
    "shutil.copy('metrics_kaggle.py', 'metrics.py')\n",
    "print(\"\u2713 Using GPU-optimized metrics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Kaggle config\n",
    "from pathlib import Path\n",
    "\n",
    "if os.path.exists('config.py'):\n",
    "    os.remove('config.py')\n",
    "\n",
    "config_code = '''\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    is_kaggle: bool = True\n",
    "    data_root: str = \"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014\"\n",
    "    text_embeddings_path: str = \"/kaggle/input/elec-475-lab4\"\n",
    "    train_images_dir: str = \"images/train2014\"\n",
    "    val_images_dir: str = \"images/val2014\"\n",
    "    train_captions_file: str = \"annotations/instances_train2014.json\"\n",
    "    val_captions_file: str = \"annotations/instances_val2014.json\"\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "    results_dir: str = \"results\"\n",
    "    embed_dim: int = 512\n",
    "    image_size: int = 224\n",
    "    pretrained_resnet: bool = True\n",
    "    clip_mean: tuple = (0.48145466, 0.4578275, 0.40821073)\n",
    "    clip_std: tuple = (0.26862954, 0.26130258, 0.27577711)\n",
    "    clip_model_name: str = \"openai/clip-vit-base-patch32\"\n",
    "    batch_size: int = 32\n",
    "    num_epochs: int = 10\n",
    "    learning_rate: float = 5e-5\n",
    "    weight_decay: float = 0.05\n",
    "    temperature: float = 0.07\n",
    "    use_scheduler: bool = True\n",
    "    scheduler_type: str = \"cosine\"\n",
    "    optimizer_type: str = \"adamw\"\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.999\n",
    "    eps: float = 1e-8\n",
    "    max_grad_norm: float = 1.0\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers: int = 2\n",
    "    pin_memory: bool = True\n",
    "    use_amp: bool = True\n",
    "    use_subset: bool = False\n",
    "    subset_size: int = 10000\n",
    "    use_cached_embeddings: bool = True\n",
    "    eval_every_n_epochs: int = 1\n",
    "    save_best_only: bool = False\n",
    "    recall_k_values: list = None\n",
    "    num_visualization_samples: int = 10\n",
    "    save_visualizations: bool = True\n",
    "    log_interval: int = 100\n",
    "    verbose: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.recall_k_values is None:\n",
    "            self.recall_k_values = [1, 5, 10]\n",
    "        self.data_root = Path(self.data_root)\n",
    "        self.train_images_path = self.data_root / self.train_images_dir\n",
    "        self.val_images_path = self.data_root / self.val_images_dir\n",
    "        self.train_captions_path = self.data_root / self.train_captions_file\n",
    "        self.val_captions_path = self.data_root / self.val_captions_file\n",
    "        self.cache_path = Path(self.text_embeddings_path)\n",
    "        self.checkpoint_path = Path(\"/kaggle/working\") / self.checkpoint_dir\n",
    "        self.results_path = Path(\"/kaggle/working\") / self.results_dir\n",
    "    \n",
    "    def create_directories(self):\n",
    "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "        os.makedirs(self.results_path, exist_ok=True)\n",
    "    \n",
    "    def validate_paths(self):\n",
    "        required = [\n",
    "            self.train_images_path,\n",
    "            self.val_images_path,\n",
    "            self.cache_path / \"text_embeddings_train.pt\",\n",
    "            self.cache_path / \"text_embeddings_val.pt\"\n",
    "        ]\n",
    "        missing = [str(p) for p in required if not p.exists()]\n",
    "        if missing:\n",
    "            raise FileNotFoundError(f\"Missing: {missing}\")\n",
    "        return True\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Config(Kaggle | {self.device} | Batch:{self.batch_size} | Epochs:{self.num_epochs} | LR:{self.learning_rate} | WD:{self.weight_decay})\"\n",
    "\n",
    "def get_config(**kwargs):\n",
    "    config = Config(**kwargs)\n",
    "    config.create_directories()\n",
    "    return config\n",
    "'''\n",
    "\n",
    "with open('config.py', 'w') as f:\n",
    "    f.write(config_code)\n",
    "\n",
    "print(\"\u2713 Kaggle config created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate setup\n",
    "from config import get_config\n",
    "\n",
    "config = get_config()\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(config)\n",
    "print(f\"\\nCheckpoints: {config.checkpoint_path}\")\n",
    "print(f\"Results: {config.results_path}\")\n",
    "print(f\"Text embeddings: {config.cache_path}\")\n",
    "\n",
    "print(f\"\\nValidating paths...\")\n",
    "config.validate_paths()\n",
    "print(\"\u2713 All paths valid!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Model Checkpoints\n",
    "\n",
    "**Update these paths** to match your uploaded checkpoint datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model checkpoints\n",
    "# UPDATE THESE PATHS to match your Kaggle dataset names!\n",
    "models_to_evaluate = {\n",
    "    'Baseline': '/kaggle/input/clip-baseline-model/best_model.pth',\n",
    "    'BatchNorm': '/kaggle/input/clip-batchnorm-model/best_model.pth',\n",
    "    # 'Dropout': '/kaggle/input/YOUR-DROPOUT-DATASET/best_model.pth',  # Uncomment when ready\n",
    "}\n",
    "\n",
    "# Verify checkpoints exist\n",
    "print(\"Checking model checkpoints:\")\n",
    "for name, path in models_to_evaluate.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  \u2713 {name}: {path}\")\n",
    "    else:\n",
    "        print(f\"  \u274c {name}: NOT FOUND - {path}\")\n",
    "        print(f\"     Please update the path or add the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from dataset import create_dataloaders\n",
    "from model import CLIPFineTuneModel\n",
    "from model_modified import create_modified_model\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from metrics import compute_retrieval_metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, device, config, max_samples=20000):\n",
    "    \"\"\"\n",
    "    Evaluate model and compute Recall@K metrics.\n",
    "    Uses a subset of validation data to avoid OOM.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect embeddings (limited to max_samples)\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds = []\n",
    "    \n",
    "    print(f\"Collecting embeddings (max {max_samples} samples)...\")\n",
    "    pbar = tqdm(dataloader, desc=\"Evaluation\")\n",
    "    \n",
    "    samples_collected = 0\n",
    "    for batch in pbar:\n",
    "        if samples_collected >= max_samples:\n",
    "            break\n",
    "            \n",
    "        images = batch['image'].to(device)\n",
    "        text_embeddings = batch['text_embedding'].to(device)\n",
    "        \n",
    "        # Forward pass - handle both 2 and 3 return values\n",
    "        outputs = model(images, text_embeddings=text_embeddings)\n",
    "        if len(outputs) == 3:\n",
    "            image_embeds, text_embeds, _ = outputs\n",
    "        else:\n",
    "            image_embeds, text_embeds = outputs\n",
    "        \n",
    "        # Store embeddings (move to CPU to save GPU memory)\n",
    "        all_image_embeds.append(image_embeds.cpu())\n",
    "        all_text_embeds.append(text_embeds.cpu())\n",
    "        \n",
    "        samples_collected += images.size(0)\n",
    "        \n",
    "        # Clear GPU cache periodically\n",
    "        if len(all_image_embeds) % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Concatenate all embeddings\n",
    "    print(f\"Computing Recall@K metrics on {samples_collected} samples...\")\n",
    "    all_image_embeds = torch.cat(all_image_embeds, dim=0)\n",
    "    all_text_embeds = torch.cat(all_text_embeds, dim=0)\n",
    "    \n",
    "    print(f\"Embedding shapes: Images={all_image_embeds.shape}, Text={all_text_embeds.shape}\")\n",
    "    \n",
    "    # Compute Recall@K metrics (move back to GPU for fast computation)\n",
    "    recall_metrics = compute_retrieval_metrics(\n",
    "        all_image_embeds.to(device),\n",
    "        all_text_embeds.to(device),\n",
    "        k_values=config.recall_k_values\n",
    "    )\n",
    "    \n",
    "    # Clear memory\n",
    "    del all_image_embeds, all_text_embeds\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return recall_metrics\n",
    "\n",
    "print(\"\u2713 Evaluation function defined (memory-efficient)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create validation dataloader with SUBSET to avoid OOM\n",
    "print(\"Creating validation dataloader (20K subset)...\")\n",
    "_, val_loader = create_dataloaders(\n",
    "    data_root=config.data_root,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=2,\n",
    "    pin_memory=config.pin_memory,\n",
    "    use_cached_embeddings=config.use_cached_embeddings,\n",
    "    use_subset=True,  # Use subset to avoid OOM\n",
    "    subset_size=20000\n",
    ")\n",
    "print(f\"\u2713 Validation set: {len(val_loader.dataset)} samples\\n\")\n",
    "\n",
    "# Load text encoder ONCE and reuse for all models\n",
    "print(\"Loading shared CLIP text encoder...\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(config.clip_model_name)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(config.clip_model_name)\n",
    "device = config.device\n",
    "text_encoder = text_encoder.to(device)\n",
    "text_encoder.eval()\n",
    "print(\"\u2713 Text encoder loaded\\n\")\n",
    "\n",
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, checkpoint_path in models_to_evaluate.items():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"EVALUATING: {model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"\u274c Checkpoint not found: {checkpoint_path}\")\n",
    "        print(\"   Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Load checkpoint\n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Determine model type and create model\n",
    "    if 'batchnorm' in model_name.lower():\n",
    "        # BatchNorm model\n",
    "        model = create_modified_model(\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            embed_dim=config.embed_dim,\n",
    "            use_batchnorm=True,\n",
    "            use_dropout=False,\n",
    "            deeper_projection=False,\n",
    "            learnable_temperature=False\n",
    "        ).to(device)\n",
    "    elif 'dropout' in model_name.lower():\n",
    "        # Dropout model\n",
    "        model = create_modified_model(\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            embed_dim=config.embed_dim,\n",
    "            use_batchnorm=False,\n",
    "            use_dropout=True,\n",
    "            dropout_rate=0.1,\n",
    "            deeper_projection=False,\n",
    "            learnable_temperature=False\n",
    "        ).to(device)\n",
    "    else:\n",
    "        # Baseline model\n",
    "        model = CLIPFineTuneModel(\n",
    "            embed_dim=config.embed_dim,\n",
    "            pretrained_resnet=config.pretrained_resnet,\n",
    "            clip_model_name=config.clip_model_name,\n",
    "            freeze_text_encoder=True\n",
    "        ).to(device)\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"\u2713 Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "    \n",
    "    # Evaluate (will use all samples from the 20K subset)\n",
    "    metrics = evaluate_model(model, val_loader, device, config, max_samples=20000)\n",
    "    all_results[model_name] = metrics\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(f\"  Image\u2192Text R@1:  {metrics['img2txt_r1']:.2f}%\")\n",
    "    print(f\"  Image\u2192Text R@5:  {metrics['img2txt_r5']:.2f}%\")\n",
    "    print(f\"  Image\u2192Text R@10: {metrics['img2txt_r10']:.2f}%\")\n",
    "    print(f\"  Text\u2192Image R@1:  {metrics['txt2img_r1']:.2f}%\")\n",
    "    print(f\"  Text\u2192Image R@5:  {metrics['txt2img_r5']:.2f}%\")\n",
    "    print(f\"  Text\u2192Image R@10: {metrics['txt2img_r10']:.2f}%\")\n",
    "    print(f\"  Average Recall:  {metrics['avg_recall']:.2f}%\")\n",
    "    \n",
    "    # Clean up model but keep text encoder\n",
    "    del model, checkpoint\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Clean up text encoder\n",
    "del text_encoder, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL EVALUATIONS COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECALL@K COMPARISON\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Build comparison dataframe\n",
    "comparison_data = []\n",
    "for model_name, metrics in all_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'I\u2192T R@1': f\"{metrics['img2txt_r1']:.2f}%\",\n",
    "        'I\u2192T R@5': f\"{metrics['img2txt_r5']:.2f}%\",\n",
    "        'I\u2192T R@10': f\"{metrics['img2txt_r10']:.2f}%\",\n",
    "        'T\u2192I R@1': f\"{metrics['txt2img_r1']:.2f}%\",\n",
    "        'T\u2192I R@5': f\"{metrics['txt2img_r5']:.2f}%\",\n",
    "        'T\u2192I R@10': f\"{metrics['txt2img_r10']:.2f}%\",\n",
    "        'Avg': f\"{metrics['avg_recall']:.2f}%\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Save comparison table\n",
    "df.to_csv('/kaggle/working/model_comparison.csv', index=False)\n",
    "print(\"\\n\u2713 Saved: /kaggle/working/model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = list(all_results.keys())\n",
    "k_values = [1, 5, 10]\n",
    "\n",
    "# Image to Text\n",
    "for model_name in models:\n",
    "    metrics = all_results[model_name]\n",
    "    i2t_scores = [metrics['img2txt_r1'], metrics['img2txt_r5'], metrics['img2txt_r10']]\n",
    "    axes[0].plot(k_values, i2t_scores, marker='o', label=model_name, linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('K', fontsize=12)\n",
    "axes[0].set_ylabel('Recall@K (%)', fontsize=12)\n",
    "axes[0].set_title('Image \u2192 Text Retrieval', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(k_values)\n",
    "\n",
    "# Text to Image\n",
    "for model_name in models:\n",
    "    metrics = all_results[model_name]\n",
    "    t2i_scores = [metrics['txt2img_r1'], metrics['txt2img_r5'], metrics['txt2img_r10']]\n",
    "    axes[1].plot(k_values, t2i_scores, marker='o', label=model_name, linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('K', fontsize=12)\n",
    "axes[1].set_ylabel('Recall@K (%)', fontsize=12)\n",
    "axes[1].set_title('Text \u2192 Image Retrieval', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks(k_values)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/recall_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Saved: /kaggle/working/recall_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all metrics to JSON\n",
    "with open('/kaggle/working/all_model_metrics.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OUTPUT FILES\")\n",
    "print(\"=\" * 80)\n",
    "!ls -lh /kaggle/working/\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DOWNLOAD INSTRUCTIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. Click 'Output' tab at top\n",
    "2. Download all files\n",
    "\n",
    "Key files:\n",
    "  - model_comparison.csv (table of results)\n",
    "  - recall_comparison.png (comparison plot)\n",
    "  - all_model_metrics.json (detailed metrics)\n",
    "\n",
    "Use these results in your lab report!\n",
    "\"\"\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \u2705 Done!\n",
    "\n",
    "**You now have:**\n",
    "- Recall@K scores for all models\n",
    "- Comparison table (CSV)\n",
    "- Comparison plot (PNG)\n",
    "- Detailed metrics (JSON)\n",
    "\n",
    "**Next steps:**\n",
    "1. Download the results\n",
    "2. Analyze which modification worked best\n",
    "3. Include in your lab report!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}