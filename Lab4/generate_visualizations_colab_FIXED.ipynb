{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ELEC 475 Lab 4 - CLIP Visualization Generator (FIXED)\n",
                "\n",
                "This notebook generates all required visualizations for Lab 4 Section 2.4:\n",
                "1. Textâ†’Image retrieval (including 'sport' and 'a dog playing')\n",
                "2. Zero-shot image classification\n",
                "\n",
                "**Features:**\n",
                "- Clones Lab4 code from GitHub\n",
                "- Downloads ONLY val2014 images (~6GB)\n",
                "- **FIXED: Handles text_encoder key mismatch automatically**\n",
                "- Handles different model architectures (base, batchnorm, dropout)\n",
                "- Saves outputs to Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers torch torchvision tqdm pillow matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "if not os.path.exists('475_ML-CV_Labs'):\n",
                "    !git clone https://github.com/Jcub05/475_ML-CV_Labs.git\n",
                "\n",
                "os.chdir('/content/475_ML-CV_Labs/Lab4')\n",
                "print(f\"âœ“ Current directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import urllib.request\n",
                "import zipfile\n",
                "from pathlib import Path\n",
                "\n",
                "data_dir = Path('/content/coco_data')\n",
                "data_dir.mkdir(exist_ok=True)\n",
                "val_dir = data_dir / 'val2014'\n",
                "\n",
                "if not val_dir.exists() or len(list(val_dir.glob('*.jpg'))) == 0:\n",
                "    val_url = 'http://images.cocodataset.org/zips/val2014.zip'\n",
                "    val_zip = data_dir / 'val2014.zip'\n",
                "    urllib.request.urlretrieve(val_url, val_zip)\n",
                "    with zipfile.ZipFile(val_zip, 'r') as z:\n",
                "        z.extractall(data_dir)\n",
                "    val_zip.unlink()\n",
                "\n",
                "print(f\"âœ“ Found {len(list(val_dir.glob('*.jpg')))} validation images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "# CONFIGURE MODEL TYPE\n",
                "MODEL_TYPE = 'batchnorm'  # Options: 'base', 'batchnorm', 'dropout', 'batchnorm_dropout'\n",
                "\n",
                "# UPLOAD OR SPECIFY MODEL PATH\n",
                "# Option 1: Upload\n",
                "# uploaded = files.upload()\n",
                "# model_checkpoint_path = list(uploaded.keys())[0]\n",
                "\n",
                "# Option 2: Google Drive path\n",
                "model_checkpoint_path = '/content/drive/MyDrive/elec475_lab4/models/best_model_batch_norm.pth'\n",
                "\n",
                "print(f\"Model type: {MODEL_TYPE}\")\n",
                "print(f\"Checkpoint: {model_checkpoint_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import CLIPProcessor, CLIPModel\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "\n",
                "from model import CLIPFineTuneModel\n",
                "from model_modified import CLIPImageEncoderModified, CLIPFineTuneModelModified\n",
                "from visualize import visualize_text_to_image_retrieval, zero_shot_classification, create_retrieval_grid\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "output_dir = Path('/content/Visualizations')\n",
                "output_dir.mkdir(exist_ok=True)\n",
                "\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_clip_transform():\n",
                "    return transforms.Compose([\n",
                "        transforms.Resize((224, 224)),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize(\n",
                "            mean=(0.48145466, 0.4578275, 0.40821073),\n",
                "            std=(0.26862954, 0.26130258, 0.27577711)\n",
                "        )\n",
                "    ])\n",
                "\n",
                "def load_model_with_architecture(model_path, model_type, device):\n",
                "    \"\"\"\n",
                "    Load model with AUTOMATIC KEY FIXING for text_encoder mismatch.\n",
                "    \"\"\"\n",
                "    print(f\"\\nLoading {model_type} model from {model_path}...\")\n",
                "    \n",
                "    MODEL_CONFIGS = {\n",
                "        'base': {'use_batchnorm': False, 'use_dropout': False, 'deeper_projection': False},\n",
                "        'batchnorm': {'use_batchnorm': True, 'use_dropout': False, 'deeper_projection': False},\n",
                "        'dropout': {'use_batchnorm': False, 'use_dropout': True, 'dropout_rate': 0.1, 'deeper_projection': False},\n",
                "        'batchnorm_dropout': {'use_batchnorm': True, 'use_dropout': True, 'dropout_rate': 0.1, 'deeper_projection': False},\n",
                "    }\n",
                "    \n",
                "    # Create model\n",
                "    if model_type == 'base':\n",
                "        model = CLIPFineTuneModel(\n",
                "            embed_dim=512,\n",
                "            pretrained_resnet=True,\n",
                "            clip_model_name=\"openai/clip-vit-base-patch32\",\n",
                "            freeze_text_encoder=True\n",
                "        ).to(device)\n",
                "    else:\n",
                "        config = MODEL_CONFIGS[model_type]\n",
                "        image_encoder = CLIPImageEncoderModified(embed_dim=512, **config)\n",
                "        clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
                "        model = CLIPFineTuneModelModified(\n",
                "            image_encoder=image_encoder,\n",
                "            text_encoder=clip_model.text_model,\n",
                "            tokenizer=None\n",
                "        ).to(device)\n",
                "    \n",
                "    # Load checkpoint\n",
                "    checkpoint = torch.load(model_path, map_location=device)\n",
                "    state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
                "    \n",
                "    # ðŸ”§ FIX: Remove extra 'text_model' prefix from text_encoder keys\n",
                "    fixed_state_dict = {}\n",
                "    num_fixed = 0\n",
                "    for key, value in state_dict.items():\n",
                "        if key.startswith('text_encoder.text_model.'):\n",
                "            new_key = key.replace('text_encoder.text_model.', 'text_encoder.')\n",
                "            fixed_state_dict[new_key] = value\n",
                "            num_fixed += 1\n",
                "        else:\n",
                "            fixed_state_dict[key] = value\n",
                "    \n",
                "    if num_fixed > 0:\n",
                "        print(f\"âœ“ Fixed {num_fixed} text_encoder keys\")\n",
                "    \n",
                "    # Load weights\n",
                "    missing, unexpected = model.load_state_dict(fixed_state_dict, strict=False)\n",
                "    \n",
                "    if missing:\n",
                "        print(f\"âš  Missing keys: {len(missing)} (expected for frozen layers)\")\n",
                "    if unexpected:\n",
                "        print(f\"âš  Unexpected keys: {len(unexpected)}\")\n",
                "    \n",
                "    model.eval()\n",
                "    print(f\"âœ“ Model loaded successfully\\n\")\n",
                "    return model\n",
                "\n",
                "def precompute_image_embeddings(model, image_paths, transform, device, batch_size=32):\n",
                "    print(f\"Precomputing embeddings for {len(image_paths)} images...\")\n",
                "    all_embeds = []\n",
                "    model.eval()\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for i in range(0, len(image_paths), batch_size):\n",
                "            batch_paths = image_paths[i:i+batch_size]\n",
                "            images = [transform(Image.open(p).convert('RGB')) for p in batch_paths]\n",
                "            images = torch.stack(images).to(device)\n",
                "            embeds = model.encode_image(images).cpu()\n",
                "            all_embeds.append(embeds)\n",
                "            \n",
                "            if (i // batch_size + 1) % 10 == 0:\n",
                "                print(f\"  {i+len(batch_paths)}/{len(image_paths)}\")\n",
                "    \n",
                "    all_embeds = torch.cat(all_embeds, dim=0)\n",
                "    print(f\"âœ“ Embeddings: {all_embeds.shape}\")\n",
                "    return all_embeds\n",
                "\n",
                "class ModifiedModelWrapper:\n",
                "    def __init__(self, model, processor):\n",
                "        self.model = model\n",
                "        self.processor = processor\n",
                "        \n",
                "    def eval(self):\n",
                "        self.model.eval()\n",
                "        return self\n",
                "    \n",
                "    def encode_text(self, input_ids, attention_mask):\n",
                "        with torch.no_grad():\n",
                "            if hasattr(self.model, 'text_encoder'):\n",
                "                outputs = self.model.text_encoder(\n",
                "                    input_ids=input_ids,\n",
                "                    attention_mask=attention_mask\n",
                "                )\n",
                "                import torch.nn.functional as F\n",
                "                return F.normalize(outputs.pooler_output, p=2, dim=-1)\n",
                "            else:\n",
                "                return self.model.encode_text(input_ids, attention_mask)\n",
                "    \n",
                "    def encode_image(self, images):\n",
                "        return self.model.encode_image(images)\n",
                "\n",
                "def generate_visualizations(model, model_name, image_paths, image_embeds, processor, transform, device, output_dir):\n",
                "    print(f\"\\nGenerating visualizations for: {model_name}\")\n",
                "    model_output_dir = output_dir / model_name\n",
                "    model_output_dir.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    wrapped_model = ModifiedModelWrapper(model, processor) if isinstance(model, CLIPFineTuneModelModified) else model\n",
                "    \n",
                "    # Text-to-Image\n",
                "    text_queries = [\"sport\", \"a dog playing\", \"a person eating\", \"a beautiful sunset\", \"a cat on a couch\"]\n",
                "    \n",
                "    for query in text_queries:\n",
                "        print(f\"  Query: '{query}'\")\n",
                "        visualize_text_to_image_retrieval(\n",
                "            query_text=query,\n",
                "            model=wrapped_model,\n",
                "            image_paths=image_paths,\n",
                "            image_embeds=image_embeds,\n",
                "            clip_processor=processor,\n",
                "            device=device,\n",
                "            top_k=5,\n",
                "            save_path=model_output_dir / f\"text2img_{query.replace(' ', '_')}.png\"\n",
                "        )\n",
                "    \n",
                "    # Grid\n",
                "    create_retrieval_grid(\n",
                "        queries=text_queries[:4],\n",
                "        model=wrapped_model,\n",
                "        image_paths=image_paths,\n",
                "        image_embeds=image_embeds,\n",
                "        clip_processor=processor,\n",
                "        device=device,\n",
                "        images_per_query=5,\n",
                "        save_path=model_output_dir / \"text2img_grid.png\"\n",
                "    )\n",
                "    \n",
                "    # Zero-shot classification\n",
                "    class_labels = ['a person', 'an animal', 'a landscape']\n",
                "    for idx, img_path in enumerate(image_paths[:5]):\n",
                "        print(f\"  Classifying image {idx+1}/5\")\n",
                "        predicted_class, confidence = zero_shot_classification(\n",
                "            query_image_path=img_path,\n",
                "            class_labels=class_labels,\n",
                "            model=wrapped_model,\n",
                "            clip_processor=processor,\n",
                "            transform=transform,\n",
                "            device=device,\n",
                "            save_path=model_output_dir / f\"classification_example_{idx+1}.png\"\n",
                "        )\n",
                "        print(f\"    â†’ {predicted_class} ({confidence*100:.1f}%)\")\n",
                "    \n",
                "    print(f\"âœ“ Saved to: {model_output_dir}\")\n",
                "\n",
                "print(\"âœ“ Functions loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
                "transform = get_clip_transform()\n",
                "all_image_paths = sorted(list(val_dir.glob(\"*.jpg\")))[:1000]\n",
                "print(f\"âœ“ Using {len(all_image_paths)} images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LOAD MODEL (with automatic key fixing)\n",
                "model = load_model_with_architecture(model_checkpoint_path, MODEL_TYPE, device)\n",
                "\n",
                "# Generate embeddings\n",
                "image_embeds = precompute_image_embeddings(model, all_image_paths, transform, device)\n",
                "\n",
                "# Generate visualizations\n",
                "generate_visualizations(\n",
                "    model=model,\n",
                "    model_name=MODEL_TYPE,\n",
                "    image_paths=all_image_paths,\n",
                "    image_embeds=image_embeds,\n",
                "    processor=processor,\n",
                "    transform=transform,\n",
                "    device=device,\n",
                "    output_dir=output_dir\n",
                ")\n",
                "\n",
                "del model, image_embeds\n",
                "torch.cuda.empty_cache()\n",
                "print(\"\\nâœ… Complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download results\n",
                "import shutil\n",
                "archive_name = f'/content/Lab4_Visualizations_{MODEL_TYPE}'\n",
                "shutil.make_archive(archive_name, 'zip', output_dir)\n",
                "files.download(f'{archive_name}.zip')\n",
                "print(\"âœ“ Download started!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copy to Drive (optional)\n",
                "import shutil\n",
                "drive_dir = f'/content/drive/MyDrive/Lab4_Visualizations_{MODEL_TYPE}'\n",
                "if os.path.exists(drive_dir):\n",
                "    shutil.rmtree(drive_dir)\n",
                "shutil.copytree(output_dir, drive_dir)\n",
                "print(f\"âœ“ Copied to: {drive_dir}\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}