{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YgpFeksHE-H"
      },
      "source": [
        "# ELEC 475 Lab 4 - CLIP Visualization Generator\n",
        "\n",
        "This notebook generates all required visualizations for Lab 4 Section 2.4:\n",
        "1. Textâ†’Image retrieval (including 'sport' and 'a dog playing')\n",
        "2. Zero-shot image classification\n",
        "\n",
        "**Features:**\n",
        "- Clones Lab4 code from GitHub\n",
        "- Downloads ONLY val2014 images (~6GB, not entire 20+GB dataset)\n",
        "- Handles different model architectures (base, batchnorm, dropout)\n",
        "- Saves outputs to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFRxEnytHE-K"
      },
      "source": [
        "## Setup: Mount Google Drive and Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8kOVGTXHE-K",
        "outputId": "6d276f0e-ec6f-4e99-f5cd-f8cfd6748e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Fis_PiASHE-L"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers torch torchvision tqdm pillow matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8p_TKq2HE-M"
      },
      "source": [
        "## Clone Lab4 Code from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IiVcqBfHE-M",
        "outputId": "303108e0-8404-4d5f-a3c0-5f1e21f31f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Repository already exists\n",
            "âœ“ Current directory: /content/475_ML-CV_Labs/Lab4\n",
            "âœ“ model.py found\n",
            "âœ“ model_modified.py found\n",
            "âœ“ visualize.py found\n",
            "âœ“ metrics.py found\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Clone repository\n",
        "if not os.path.exists('475_ML-CV_Labs'):\n",
        "    print(\"Cloning repository from GitHub...\")\n",
        "    !git clone https://github.com/Jcub05/475_ML-CV_Labs.git\n",
        "    print(\"âœ“ Repository cloned\")\n",
        "else:\n",
        "    print(\"âœ“ Repository already exists\")\n",
        "\n",
        "# Navigate to Lab4 directory\n",
        "os.chdir('/content/475_ML-CV_Labs/Lab4')\n",
        "print(f\"âœ“ Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Verify required files exist\n",
        "required_files = ['model.py', 'model_modified.py', 'visualize.py', 'metrics.py']\n",
        "for f in required_files:\n",
        "    if os.path.exists(f):\n",
        "        print(f\"âœ“ {f} found\")\n",
        "    else:\n",
        "        print(f\"âœ— {f} missing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1ZcMgGOHE-N"
      },
      "source": [
        "## Download ONLY val2014 Images (~6GB)\n",
        "\n",
        "This downloads only the validation images, not the entire 20+GB dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3pfHSkgHE-N",
        "outputId": "b394bab7-c051-4cff-e268-b8e8b9d436c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ val2014 images already downloaded\n",
            "\n",
            "âœ“ Validation images found: 40504\n",
            "âœ“ Image directory: /content/coco_data/val2014\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup directories\n",
        "data_dir = Path('/content/coco_data')\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "val_zip_path = data_dir / 'val2014.zip'\n",
        "val_dir = data_dir / 'val2014'\n",
        "\n",
        "# Download val2014 images if not already downloaded\n",
        "if not val_dir.exists() or len(list(val_dir.glob('*.jpg'))) == 0:\n",
        "    print(\"Downloading COCO val2014 images (~6GB)...\")\n",
        "    print(\"This will take 5-10 minutes depending on connection speed.\")\n",
        "\n",
        "    # Download from official COCO website\n",
        "    val_url = 'http://images.cocodataset.org/zips/val2014.zip'\n",
        "\n",
        "    # Download with progress\n",
        "    def download_progress(block_num, block_size, total_size):\n",
        "        downloaded = block_num * block_size\n",
        "        percent = min(100, downloaded * 100 / total_size)\n",
        "        print(f'\\rDownloading: {percent:.1f}% ({downloaded / 1e9:.2f} GB / {total_size / 1e9:.2f} GB)', end='')\n",
        "\n",
        "    urllib.request.urlretrieve(val_url, val_zip_path, download_progress)\n",
        "    print(\"\\nâœ“ Download complete\")\n",
        "\n",
        "    # Extract\n",
        "    print(\"\\nExtracting val2014.zip...\")\n",
        "    with zipfile.ZipFile(val_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    print(\"âœ“ Extraction complete\")\n",
        "\n",
        "    # Remove zip file to save space\n",
        "    val_zip_path.unlink()\n",
        "    print(\"âœ“ Removed zip file to save space\")\n",
        "else:\n",
        "    print(\"âœ“ val2014 images already downloaded\")\n",
        "\n",
        "# Verify\n",
        "num_val_images = len(list(val_dir.glob('*.jpg')))\n",
        "print(f\"\\nâœ“ Validation images found: {num_val_images}\")\n",
        "print(f\"âœ“ Image directory: {val_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B0bc8qnHE-O"
      },
      "source": [
        "## Upload Your Trained Model Checkpoint & Configure Model Type\n",
        "\n",
        "**IMPORTANT:** Set `MODEL_TYPE` to match your checkpoint:\n",
        "- `'base'` - Standard model (no batchnorm, no dropout)\n",
        "- `'batchnorm'` - Model with BatchNorm in projection\n",
        "- `'dropout'` - Model with Dropout in projection\n",
        "- `'batchnorm_dropout'` - Model with both BatchNorm and Dropout\n",
        "- `'deeper'` - Model with 3-layer projection\n",
        "- `'all_combined'` - All modifications combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Gpdc2hB-HE-O"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# # ============================================\n",
        "# # CONFIGURATION: Set this to match your model!\n",
        "# # ============================================\n",
        "# MODEL_TYPE = 'batchnorm'  # Change to: 'base', 'batchnorm', 'dropout', 'batchnorm_dropout', 'deeper', 'all_combined'\n",
        "\n",
        "# print(f\"Model type set to: {MODEL_TYPE}\")\n",
        "# print(\"\\nUpload your trained model checkpoint (e.g., best_model.pth)\")\n",
        "# uploaded = files.upload()\n",
        "# model_checkpoint_path = list(uploaded.keys())[0]\n",
        "# print(f\"âœ“ Model checkpoint: {model_checkpoint_path}\")\n",
        "\n",
        "# Alternative: If checkpoint is in Google Drive, uncomment and modify:\n",
        "MODEL_TYPE = 'batchnorm'\n",
        "model_checkpoint_path = '/content/drive/MyDrive/elec475_lab4/models/best_model_batch_norm.pth'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fZeTaLwHE-P"
      },
      "source": [
        "## Visualization Code with Multi-Architecture Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OmvHYdgHE-P",
        "outputId": "f0a1e527-7ea5-4c83-8d15-d8a059eb473e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Output directory: /content/Visualizations\n",
            "Model type: batchnorm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from model import CLIPFineTuneModel\n",
        "from model_modified import CLIPImageEncoderModified, CLIPFineTuneModelModified\n",
        "from visualize import (\n",
        "    visualize_text_to_image_retrieval,\n",
        "    zero_shot_classification,\n",
        "    create_retrieval_grid\n",
        ")\n",
        "\n",
        "# Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "output_dir = Path('/content/Visualizations')\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "num_images = 1000  # Number of val images to use\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "print(f\"Model type: {MODEL_TYPE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9m2Pr7DHE-Q",
        "outputId": "9a324fd9-b3f2-4a12-d673-e7a6d770e45e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ All functions defined\n"
          ]
        }
      ],
      "source": [
        "def get_clip_transform():\n",
        "    \"\"\"Get CLIP preprocessing transform.\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "            std=(0.26862954, 0.26130258, 0.27577711)\n",
        "        )\n",
        "    ])\n",
        "\n",
        "\n",
        "def load_model_with_architecture(model_path, model_type, device):\n",
        "    \"\"\"\n",
        "    Load model with correct architecture based on model_type.\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to checkpoint\n",
        "        model_type: One of 'base', 'batchnorm', 'dropout', etc.\n",
        "        device: Device to load model on\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading {model_type} model from {model_path}...\")\n",
        "\n",
        "    # Model architecture configurations\n",
        "    MODEL_CONFIGS = {\n",
        "        'base': {\n",
        "            'use_batchnorm': False,\n",
        "            'use_dropout': False,\n",
        "            'deeper_projection': False,\n",
        "            'learnable_temperature': False,\n",
        "        },\n",
        "        'batchnorm': {\n",
        "            'use_batchnorm': True,\n",
        "            'use_dropout': False,\n",
        "            'deeper_projection': False,\n",
        "            'learnable_temperature': False,\n",
        "        },\n",
        "        'dropout': {\n",
        "            'use_batchnorm': False,\n",
        "            'use_dropout': True,\n",
        "            'dropout_rate': 0.1,\n",
        "            'deeper_projection': False,\n",
        "            'learnable_temperature': False,\n",
        "        },\n",
        "        'batchnorm_dropout': {\n",
        "            'use_batchnorm': True,\n",
        "            'use_dropout': True,\n",
        "            'dropout_rate': 0.1,\n",
        "            'deeper_projection': False,\n",
        "            'learnable_temperature': False,\n",
        "        },\n",
        "        'deeper': {\n",
        "            'use_batchnorm': False,\n",
        "            'use_dropout': False,\n",
        "            'deeper_projection': True,\n",
        "            'learnable_temperature': False,\n",
        "        },\n",
        "        'all_combined': {\n",
        "            'use_batchnorm': True,\n",
        "            'use_dropout': True,\n",
        "            'dropout_rate': 0.1,\n",
        "            'deeper_projection': True,\n",
        "            'learnable_temperature': True,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if model_type == 'base':\n",
        "        # Use standard CLIPFineTuneModel (no modifications)\n",
        "        model = CLIPFineTuneModel(\n",
        "            embed_dim=512,\n",
        "            pretrained_resnet=True,\n",
        "            clip_model_name=\"openai/clip-vit-base-patch32\",\n",
        "            freeze_text_encoder=True\n",
        "        ).to(device)\n",
        "    else:\n",
        "        # Use modified architecture\n",
        "        if model_type not in MODEL_CONFIGS:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}. Choose from {list(MODEL_CONFIGS.keys())}\")\n",
        "\n",
        "        config = MODEL_CONFIGS[model_type]\n",
        "\n",
        "        # Create modified image encoder\n",
        "        image_encoder = CLIPImageEncoderModified(\n",
        "            embed_dim=512,\n",
        "            **config\n",
        "        )\n",
        "\n",
        "        # Load CLIP text encoder\n",
        "        clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "        # Create modified model\n",
        "        model = CLIPFineTuneModelModified(\n",
        "            image_encoder=image_encoder,\n",
        "            text_encoder=clip_model.text_model,\n",
        "            tokenizer=None\n",
        "        ).to(device)\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
        "\n",
        "    # Load with relaxed matching (in case of minor differences)\n",
        "    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    if missing_keys:\n",
        "        print(f\"âš  Warning: Missing keys in checkpoint: {missing_keys[:3]}...\")\n",
        "    if unexpected_keys:\n",
        "        print(f\"âš  Warning: Unexpected keys in checkpoint: {unexpected_keys[:3]}...\")\n",
        "\n",
        "    # CRITICAL: Set to eval mode (especially important for BatchNorm/Dropout!)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"âœ“ {model_type} model loaded and set to eval mode\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def precompute_image_embeddings(model, image_paths, transform, device, batch_size=32):\n",
        "    \"\"\"Precompute embeddings for all images.\"\"\"\n",
        "    print(f\"\\nPrecomputing embeddings for {len(image_paths)} images...\")\n",
        "\n",
        "    all_embeds = []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(image_paths), batch_size):\n",
        "            batch_paths = image_paths[i:i+batch_size]\n",
        "\n",
        "            images = []\n",
        "            for path in batch_paths:\n",
        "                img = Image.open(path).convert('RGB')\n",
        "                images.append(transform(img))\n",
        "\n",
        "            images = torch.stack(images).to(device)\n",
        "\n",
        "            # Use encode_image method (works for both base and modified models)\n",
        "            embeds = model.encode_image(images).cpu()\n",
        "            all_embeds.append(embeds)\n",
        "\n",
        "            if (i // batch_size + 1) % 10 == 0:\n",
        "                print(f\"  Processed {i+len(batch_paths)}/{len(image_paths)}\")\n",
        "\n",
        "    all_embeds = torch.cat(all_embeds, dim=0)\n",
        "    print(f\"âœ“ Embeddings computed: {all_embeds.shape}\")\n",
        "    return all_embeds\n",
        "\n",
        "\n",
        "# Wrapper class for modified models to work with visualize.py\n",
        "class ModifiedModelWrapper:\n",
        "    \"\"\"Wrapper to make modified models compatible with visualization functions.\"\"\"\n",
        "    def __init__(self, model, processor):\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "\n",
        "    def eval(self):\n",
        "        self.model.eval()\n",
        "        return self\n",
        "\n",
        "    def encode_text(self, input_ids, attention_mask):\n",
        "        \"\"\"Encode text using the model's text encoder.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # For modified models, use the text_encoder directly\n",
        "            if hasattr(self.model, 'text_encoder'):\n",
        "                outputs = self.model.text_encoder(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask\n",
        "                )\n",
        "                import torch.nn.functional as F\n",
        "                text_embeds = F.normalize(outputs.pooler_output, p=2, dim=-1)\n",
        "                return text_embeds\n",
        "            else:\n",
        "                # Fallback for base model\n",
        "                return self.model.encode_text(input_ids, attention_mask)\n",
        "\n",
        "    def encode_image(self, images):\n",
        "        return self.model.encode_image(images)\n",
        "\n",
        "\n",
        "def generate_visualizations(model, model_name, image_paths, image_embeds,\n",
        "                          processor, transform, device, output_dir):\n",
        "    \"\"\"Generate all visualizations for a model.\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Generating visualizations for: {model_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    model_output_dir = output_dir / model_name\n",
        "    model_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Wrap model if it's a modified model\n",
        "    if isinstance(model, CLIPFineTuneModelModified):\n",
        "        wrapped_model = ModifiedModelWrapper(model, processor)\n",
        "    else:\n",
        "        wrapped_model = model\n",
        "\n",
        "    # Textâ†’Image Retrieval\n",
        "    print(\"\\nREQUIREMENT 1: Textâ†’Image Retrieval\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    text_queries = [\n",
        "        \"sport\",\n",
        "        \"a dog playing\",\n",
        "        \"a person eating\",\n",
        "        \"a beautiful sunset\",\n",
        "        \"a cat on a couch\"\n",
        "    ]\n",
        "\n",
        "    for query in text_queries:\n",
        "        print(f\"  Query: '{query}'\")\n",
        "        save_path = model_output_dir / f\"text2img_{query.replace(' ', '_')}.png\"\n",
        "\n",
        "        visualize_text_to_image_retrieval(\n",
        "            query_text=query,\n",
        "            model=wrapped_model,\n",
        "            image_paths=image_paths,\n",
        "            image_embeds=image_embeds,\n",
        "            clip_processor=processor,\n",
        "            device=device,\n",
        "            top_k=5,\n",
        "            save_path=save_path\n",
        "        )\n",
        "\n",
        "    # Grid view\n",
        "    print(f\"  Creating retrieval grid...\")\n",
        "    grid_path = model_output_dir / \"text2img_grid.png\"\n",
        "    create_retrieval_grid(\n",
        "        queries=text_queries[:4],\n",
        "        model=wrapped_model,\n",
        "        image_paths=image_paths,\n",
        "        image_embeds=image_embeds,\n",
        "        clip_processor=processor,\n",
        "        device=device,\n",
        "        images_per_query=5,\n",
        "        save_path=grid_path\n",
        "    )\n",
        "\n",
        "    # Zero-Shot Classification\n",
        "    print(\"\\nREQUIREMENT 2: Zero-Shot Image Classification\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    class_labels = ['a person', 'an animal', 'a landscape']\n",
        "    num_examples = 5\n",
        "    sample_images = image_paths[:num_examples]\n",
        "\n",
        "    for idx, img_path in enumerate(sample_images):\n",
        "        print(f\"  Classifying image {idx+1}/{num_examples}: {img_path.name}\")\n",
        "        save_path = model_output_dir / f\"classification_example_{idx+1}.png\"\n",
        "\n",
        "        predicted_class, confidence = zero_shot_classification(\n",
        "            query_image_path=img_path,\n",
        "            class_labels=class_labels,\n",
        "            model=wrapped_model,\n",
        "            clip_processor=processor,\n",
        "            transform=transform,\n",
        "            device=device,\n",
        "            save_path=save_path\n",
        "        )\n",
        "\n",
        "        print(f\"    â†’ Predicted: {predicted_class} ({confidence*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nâœ“ All visualizations for {model_name} saved to: {model_output_dir}\")\n",
        "\n",
        "\n",
        "print(\"âœ“ All functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxskxVCSHE-R"
      },
      "source": [
        "## Load Model and Generate Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmkdTbGUHE-S",
        "outputId": "fe852207-65b4-44f3-f868-87e956065a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Using 1000 validation images\n"
          ]
        }
      ],
      "source": [
        "# Load CLIP processor\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "transform = get_clip_transform()\n",
        "\n",
        "# Get validation image paths\n",
        "all_image_paths = sorted(list(val_dir.glob(\"*.jpg\")))[:num_images]\n",
        "print(f\"\\nâœ“ Using {len(all_image_paths)} validation images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eFlflWRHE-S",
        "outputId": "5d819964-1391-4033-fa2a-d1d8d04b22c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Loading BATCHNORM Model\n",
            "======================================================================\n",
            "\n",
            "Loading batchnorm model from /content/drive/MyDrive/elec475_lab4/models/best_model_batch_norm.pth...\n",
            "âš  Warning: Missing keys in checkpoint: ['text_encoder.embeddings.token_embedding.weight', 'text_encoder.embeddings.position_embedding.weight', 'text_encoder.encoder.layers.0.self_attn.k_proj.weight']...\n",
            "âš  Warning: Unexpected keys in checkpoint: ['text_encoder.text_model.embeddings.token_embedding.weight', 'text_encoder.text_model.embeddings.position_embedding.weight', 'text_encoder.text_model.encoder.layers.0.self_attn.k_proj.weight']...\n",
            "âœ“ batchnorm model loaded and set to eval mode\n",
            "\n",
            "Precomputing embeddings for 1000 images...\n",
            "  Processed 320/1000\n",
            "  Processed 640/1000\n",
            "  Processed 960/1000\n",
            "âœ“ Embeddings computed: torch.Size([1000, 512])\n",
            "\n",
            "======================================================================\n",
            "Generating visualizations for: batchnorm\n",
            "======================================================================\n",
            "\n",
            "REQUIREMENT 1: Textâ†’Image Retrieval\n",
            "----------------------------------------------------------------------\n",
            "  Query: 'sport'\n",
            "Saved visualization to /content/Visualizations/batchnorm/text2img_sport.png\n",
            "  Query: 'a dog playing'\n",
            "Saved visualization to /content/Visualizations/batchnorm/text2img_a_dog_playing.png\n",
            "  Query: 'a person eating'\n",
            "Saved visualization to /content/Visualizations/batchnorm/text2img_a_person_eating.png\n",
            "  Query: 'a beautiful sunset'\n",
            "Saved visualization to /content/Visualizations/batchnorm/text2img_a_beautiful_sunset.png\n",
            "  Query: 'a cat on a couch'\n",
            "Saved visualization to /content/Visualizations/batchnorm/text2img_a_cat_on_a_couch.png\n",
            "  Creating retrieval grid...\n",
            "Saved grid to /content/Visualizations/batchnorm/text2img_grid.png\n",
            "\n",
            "REQUIREMENT 2: Zero-Shot Image Classification\n",
            "----------------------------------------------------------------------\n",
            "  Classifying image 1/5: COCO_val2014_000000000042.jpg\n",
            "Saved visualization to /content/Visualizations/batchnorm/classification_example_1.png\n",
            "\n",
            "Zero-Shot Classification Results:\n",
            "==================================================\n",
            "1. a person             86.87%\n",
            "2. an animal             9.15%\n",
            "3. a landscape           3.98%\n",
            "==================================================\n",
            "    â†’ Predicted: a person (86.9%)\n",
            "  Classifying image 2/5: COCO_val2014_000000000073.jpg\n",
            "Saved visualization to /content/Visualizations/batchnorm/classification_example_2.png\n",
            "\n",
            "Zero-Shot Classification Results:\n",
            "==================================================\n",
            "1. an animal            55.61%\n",
            "2. a person             43.09%\n",
            "3. a landscape           1.30%\n",
            "==================================================\n",
            "    â†’ Predicted: an animal (55.6%)\n",
            "  Classifying image 3/5: COCO_val2014_000000000074.jpg\n",
            "Saved visualization to /content/Visualizations/batchnorm/classification_example_3.png\n",
            "\n",
            "Zero-Shot Classification Results:\n",
            "==================================================\n",
            "1. an animal            86.04%\n",
            "2. a person              8.12%\n",
            "3. a landscape           5.84%\n",
            "==================================================\n",
            "    â†’ Predicted: an animal (86.0%)\n",
            "  Classifying image 4/5: COCO_val2014_000000000133.jpg\n",
            "Saved visualization to /content/Visualizations/batchnorm/classification_example_4.png\n",
            "\n",
            "Zero-Shot Classification Results:\n",
            "==================================================\n",
            "1. a person             68.88%\n",
            "2. a landscape          25.08%\n",
            "3. an animal             6.04%\n",
            "==================================================\n",
            "    â†’ Predicted: a person (68.9%)\n",
            "  Classifying image 5/5: COCO_val2014_000000000136.jpg\n",
            "Saved visualization to /content/Visualizations/batchnorm/classification_example_5.png\n",
            "\n",
            "Zero-Shot Classification Results:\n",
            "==================================================\n",
            "1. a person             77.25%\n",
            "2. an animal            15.92%\n",
            "3. a landscape           6.83%\n",
            "==================================================\n",
            "    â†’ Predicted: a person (77.3%)\n",
            "\n",
            "âœ“ All visualizations for batchnorm saved to: /content/Visualizations/batchnorm\n",
            "\n",
            "âœ… batchnorm model visualizations complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(f\"Loading {MODEL_TYPE.upper()} Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load model with correct architecture\n",
        "model = load_model_with_architecture(model_checkpoint_path, MODEL_TYPE, device)\n",
        "\n",
        "# Precompute embeddings\n",
        "image_embeds = precompute_image_embeddings(model, all_image_paths, transform, device)\n",
        "\n",
        "# Generate visualizations\n",
        "generate_visualizations(\n",
        "    model=model,\n",
        "    model_name=MODEL_TYPE,\n",
        "    image_paths=all_image_paths,\n",
        "    image_embeds=image_embeds,\n",
        "    processor=processor,\n",
        "    transform=transform,\n",
        "    device=device,\n",
        "    output_dir=output_dir\n",
        ")\n",
        "\n",
        "# Clean up\n",
        "del model, image_embeds\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\nâœ… {MODEL_TYPE} model visualizations complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDzU47u1HE-T"
      },
      "source": [
        "## Summary and Download Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynAWbhtmHE-T",
        "outputId": "2c6e2a4c-27b0-4f47-f4f4-5b52673f58b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "âœ… VISUALIZATION GENERATION COMPLETE\n",
            "======================================================================\n",
            "\n",
            "ðŸ“ Output directory: /content/Visualizations\n",
            "\n",
            "Generated visualizations:\n",
            "\n",
            "  batchnorm/ (11 images)\n",
            "    â€¢ classification_example_1.png (920.0 KB)\n",
            "    â€¢ classification_example_2.png (1309.3 KB)\n",
            "    â€¢ classification_example_3.png (698.1 KB)\n",
            "    â€¢ classification_example_4.png (765.3 KB)\n",
            "    â€¢ classification_example_5.png (712.0 KB)\n",
            "    ... and 6 more\n",
            "\n",
            "======================================================================\n",
            "Lab 4 Section 2.4 Requirements:\n",
            "  âœ“ Textâ†’Image retrieval (including 'sport' and 'a dog playing')\n",
            "  âœ“ Image classification with ['a person', 'an animal', 'a landscape']\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… VISUALIZATION GENERATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nðŸ“ Output directory: {output_dir}\")\n",
        "print(f\"\\nGenerated visualizations:\")\n",
        "\n",
        "for subdir in output_dir.iterdir():\n",
        "    if subdir.is_dir():\n",
        "        num_files = len(list(subdir.glob(\"*.png\")))\n",
        "        print(f\"\\n  {subdir.name}/ ({num_files} images)\")\n",
        "        for file_path in sorted(subdir.glob(\"*.png\"))[:5]:\n",
        "            size_kb = file_path.stat().st_size / 1024\n",
        "            print(f\"    â€¢ {file_path.name} ({size_kb:.1f} KB)\")\n",
        "        if num_files > 5:\n",
        "            print(f\"    ... and {num_files - 5} more\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Lab 4 Section 2.4 Requirements:\")\n",
        "print(\"  âœ“ Textâ†’Image retrieval (including 'sport' and 'a dog playing')\")\n",
        "print(\"  âœ“ Image classification with ['a person', 'an animal', 'a landscape']\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "NSPX8iwCHE-T",
        "outputId": "862444e3-e0a0-4210-c6fe-7c06bd2b7a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Created archive: /content/Lab4_Visualizations_batchnorm.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a7110e3b-8f6c-4ce5-a42c-22846d3431ab\", \"Lab4_Visualizations_batchnorm.zip\", 19976583)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Download started! Check your browser downloads.\n"
          ]
        }
      ],
      "source": [
        "# Archive visualizations for download\n",
        "import shutil\n",
        "\n",
        "archive_name = f'/content/Lab4_Visualizations_{MODEL_TYPE}'\n",
        "shutil.make_archive(archive_name, 'zip', output_dir)\n",
        "print(f\"\\nâœ“ Created archive: {archive_name}.zip\")\n",
        "\n",
        "# Download the archive\n",
        "files.download(f'{archive_name}.zip')\n",
        "print(\"\\nâœ“ Download started! Check your browser downloads.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uChD2UMSHE-U"
      },
      "source": [
        "## Optional: Copy to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B17kkZM6HE-U",
        "outputId": "8b04c975-edfc-4efa-bba4-918a04e7cb3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Visualizations copied to Google Drive: /content/drive/MyDrive/Lab4_Visualizations_batchnorm\n"
          ]
        }
      ],
      "source": [
        "# Copy visualizations to Google Drive\n",
        "import shutil\n",
        "\n",
        "drive_output_dir = f'/content/drive/MyDrive/Lab4_Visualizations_{MODEL_TYPE}'\n",
        "\n",
        "if os.path.exists(drive_output_dir):\n",
        "    shutil.rmtree(drive_output_dir)\n",
        "\n",
        "shutil.copytree(output_dir, drive_output_dir)\n",
        "print(f\"âœ“ Visualizations copied to Google Drive: {drive_output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}