{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ELEC 475 Lab 4 - CLIP Visualization Generator\n",
                "\n",
                "This notebook generates all required visualizations for Lab 4 Section 2.4:\n",
                "1. Textâ†’Image retrieval (including 'sport' and 'a dog playing')\n",
                "2. Zero-shot image classification\n",
                "3. Comparison between base CLIP and fine-tuned models\n",
                "\n",
                "**Features:**\n",
                "- Clones Lab4 code from GitHub\n",
                "- Downloads ONLY val2014 images (~6GB, not entire 20+GB dataset)\n",
                "- Generates visualizations for both base and fine-tuned models\n",
                "- Saves outputs to Google Drive"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup: Mount Google Drive and Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q transformers torch torchvision tqdm pillow matplotlib"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Clone Lab4 Code from GitHub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Clone repository\n",
                "if not os.path.exists('475_ML-CV_Labs'):\n",
                "    print(\"Cloning repository from GitHub...\")\n",
                "    !git clone https://github.com/Jcub05/475_ML-CV_Labs.git\n",
                "    print(\"âœ“ Repository cloned\")\n",
                "else:\n",
                "    print(\"âœ“ Repository already exists\")\n",
                "\n",
                "# Navigate to Lab4 directory\n",
                "os.chdir('/content/475_ML-CV_Labs/Lab4')\n",
                "print(f\"âœ“ Current directory: {os.getcwd()}\")\n",
                "\n",
                "# Verify required files exist\n",
                "required_files = ['model.py', 'visualize.py', 'metrics.py']\n",
                "for f in required_files:\n",
                "    if os.path.exists(f):\n",
                "        print(f\"âœ“ {f} found\")\n",
                "    else:\n",
                "        print(f\"âœ— {f} missing!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Download ONLY val2014 Images (~6GB)\n",
                "\n",
                "This downloads only the validation images, not the entire 20+GB dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import urllib.request\n",
                "import zipfile\n",
                "from pathlib import Path\n",
                "\n",
                "# Setup directories\n",
                "data_dir = Path('/content/coco_data')\n",
                "data_dir.mkdir(exist_ok=True)\n",
                "\n",
                "val_zip_path = data_dir / 'val2014.zip'\n",
                "val_dir = data_dir / 'val2014'\n",
                "\n",
                "# Download val2014 images if not already downloaded\n",
                "if not val_dir.exists() or len(list(val_dir.glob('*.jpg'))) == 0:\n",
                "    print(\"Downloading COCO val2014 images (~6GB)...\")\n",
                "    print(\"This will take 5-10 minutes depending on connection speed.\")\n",
                "    \n",
                "    # Download from official COCO website\n",
                "    val_url = 'http://images.cocodataset.org/zips/val2014.zip'\n",
                "    \n",
                "    # Download with progress\n",
                "    def download_progress(block_num, block_size, total_size):\n",
                "        downloaded = block_num * block_size\n",
                "        percent = min(100, downloaded * 100 / total_size)\n",
                "        print(f'\\rDownloading: {percent:.1f}% ({downloaded / 1e9:.2f} GB / {total_size / 1e9:.2f} GB)', end='')\n",
                "    \n",
                "    urllib.request.urlretrieve(val_url, val_zip_path, download_progress)\n",
                "    print(\"\\nâœ“ Download complete\")\n",
                "    \n",
                "    # Extract\n",
                "    print(\"\\nExtracting val2014.zip...\")\n",
                "    with zipfile.ZipFile(val_zip_path, 'r') as zip_ref:\n",
                "        zip_ref.extractall(data_dir)\n",
                "    print(\"âœ“ Extraction complete\")\n",
                "    \n",
                "    # Remove zip file to save space\n",
                "    val_zip_path.unlink()\n",
                "    print(\"âœ“ Removed zip file to save space\")\n",
                "else:\n",
                "    print(\"âœ“ val2014 images already downloaded\")\n",
                "\n",
                "# Verify\n",
                "num_val_images = len(list(val_dir.glob('*.jpg')))\n",
                "print(f\"\\nâœ“ Validation images found: {num_val_images}\")\n",
                "print(f\"âœ“ Image directory: {val_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Upload Your Trained Model Checkpoint\n",
                "\n",
                "Upload `best_model.pth` or your checkpoint file from Google Drive or local machine."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "# Option 1: Upload from local machine\n",
                "print(\"Upload your trained model checkpoint (e.g., best_model.pth)\")\n",
                "uploaded = files.upload()\n",
                "model_checkpoint_path = list(uploaded.keys())[0]\n",
                "print(f\"âœ“ Model checkpoint: {model_checkpoint_path}\")\n",
                "\n",
                "# Option 2: If checkpoint is in Google Drive, uncomment and modify:\n",
                "# model_checkpoint_path = '/content/drive/MyDrive/datasets_Lab4/Lab4_checkpoints/best_model.pth'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualization Code\n",
                "\n",
                "Complete visualization generation script with base and fine-tuned model support."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import CLIPProcessor, CLIPModel\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from model import CLIPFineTuneModel\n",
                "from visualize import (\n",
                "    visualize_text_to_image_retrieval,\n",
                "    zero_shot_classification,\n",
                "    create_retrieval_grid\n",
                ")\n",
                "\n",
                "# Configuration\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "output_dir = Path('/content/Visualizations')\n",
                "output_dir.mkdir(exist_ok=True)\n",
                "num_images = 1000  # Number of val images to use\n",
                "\n",
                "print(f\"Device: {device}\")\n",
                "print(f\"Output directory: {output_dir}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_clip_transform():\n",
                "    \"\"\"Get CLIP preprocessing transform.\"\"\"\n",
                "    return transforms.Compose([\n",
                "        transforms.Resize((224, 224)),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize(\n",
                "            mean=(0.48145466, 0.4578275, 0.40821073),\n",
                "            std=(0.26862954, 0.26130258, 0.27577711)\n",
                "        )\n",
                "    ])\n",
                "\n",
                "\n",
                "def load_base_clip_model(device):\n",
                "    \"\"\"Load the base OpenAI CLIP model.\"\"\"\n",
                "    print(\"Loading base OpenAI CLIP model...\")\n",
                "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
                "    model.eval()\n",
                "    print(\"âœ“ Base CLIP model loaded\")\n",
                "    return model\n",
                "\n",
                "\n",
                "def load_finetuned_model(model_path, device):\n",
                "    \"\"\"Load fine-tuned CLIP model.\"\"\"\n",
                "    print(f\"Loading fine-tuned model from {model_path}...\")\n",
                "    \n",
                "    model = CLIPFineTuneModel(\n",
                "        embed_dim=512,\n",
                "        pretrained_resnet=True,\n",
                "        clip_model_name=\"openai/clip-vit-base-patch32\",\n",
                "        freeze_text_encoder=True\n",
                "    ).to(device)\n",
                "    \n",
                "    checkpoint = torch.load(model_path, map_location=device)\n",
                "    state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
                "    model.load_state_dict(state_dict, strict=False)\n",
                "    model.eval()\n",
                "    \n",
                "    print(\"âœ“ Fine-tuned model loaded\")\n",
                "    return model\n",
                "\n",
                "\n",
                "def precompute_image_embeddings_finetuned(model, image_paths, transform, device, batch_size=32):\n",
                "    \"\"\"Precompute embeddings using fine-tuned model.\"\"\"\n",
                "    print(f\"\\nPrecomputing embeddings for {len(image_paths)} images (fine-tuned)...\")\n",
                "    \n",
                "    all_embeds = []\n",
                "    model.eval()\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for i in range(0, len(image_paths), batch_size):\n",
                "            batch_paths = image_paths[i:i+batch_size]\n",
                "            \n",
                "            images = []\n",
                "            for path in batch_paths:\n",
                "                img = Image.open(path).convert('RGB')\n",
                "                images.append(transform(img))\n",
                "            \n",
                "            images = torch.stack(images).to(device)\n",
                "            embeds = model.encode_image(images).cpu()\n",
                "            all_embeds.append(embeds)\n",
                "            \n",
                "            if (i // batch_size + 1) % 10 == 0:\n",
                "                print(f\"  Processed {i+len(batch_paths)}/{len(image_paths)}\")\n",
                "    \n",
                "    all_embeds = torch.cat(all_embeds, dim=0)\n",
                "    print(f\"âœ“ Embeddings computed: {all_embeds.shape}\")\n",
                "    return all_embeds\n",
                "\n",
                "\n",
                "def precompute_image_embeddings_base(model, image_paths, processor, device, batch_size=32):\n",
                "    \"\"\"Precompute embeddings using base CLIP model.\"\"\"\n",
                "    print(f\"\\nPrecomputing embeddings for {len(image_paths)} images (base CLIP)...\")\n",
                "    \n",
                "    all_embeds = []\n",
                "    model.eval()\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for i in range(0, len(image_paths), batch_size):\n",
                "            batch_paths = image_paths[i:i+batch_size]\n",
                "            \n",
                "            images = []\n",
                "            for path in batch_paths:\n",
                "                img = Image.open(path).convert('RGB')\n",
                "                images.append(img)\n",
                "            \n",
                "            inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
                "            embeds = model.get_image_features(**inputs)\n",
                "            embeds = embeds / embeds.norm(dim=-1, keepdim=True)\n",
                "            all_embeds.append(embeds.cpu())\n",
                "            \n",
                "            if (i // batch_size + 1) % 10 == 0:\n",
                "                print(f\"  Processed {i+len(batch_paths)}/{len(image_paths)}\")\n",
                "    \n",
                "    all_embeds = torch.cat(all_embeds, dim=0)\n",
                "    print(f\"âœ“ Embeddings computed: {all_embeds.shape}\")\n",
                "    return all_embeds\n",
                "\n",
                "\n",
                "class BaseCLIPWrapper:\n",
                "    \"\"\"Wrapper for base CLIP to match fine-tuned model interface.\"\"\"\n",
                "    def __init__(self, clip_model, processor):\n",
                "        self.clip_model = clip_model\n",
                "        self.processor = processor\n",
                "        \n",
                "    def eval(self):\n",
                "        self.clip_model.eval()\n",
                "        return self\n",
                "    \n",
                "    def encode_text(self, input_ids, attention_mask):\n",
                "        with torch.no_grad():\n",
                "            outputs = self.clip_model.get_text_features(\n",
                "                input_ids=input_ids,\n",
                "                attention_mask=attention_mask\n",
                "            )\n",
                "            outputs = outputs / outputs.norm(dim=-1, keepdim=True)\n",
                "        return outputs\n",
                "    \n",
                "    def encode_image(self, images):\n",
                "        with torch.no_grad():\n",
                "            outputs = self.clip_model.vision_model(pixel_values=images).pooler_output\n",
                "            outputs = self.clip_model.visual_projection(outputs)\n",
                "            outputs = outputs / outputs.norm(dim=-1, keepdim=True)\n",
                "        return outputs\n",
                "\n",
                "\n",
                "def generate_visualizations_for_model(model, model_name, image_paths, image_embeds, \n",
                "                                     processor, transform, device, output_dir):\n",
                "    \"\"\"Generate all visualizations for a model.\"\"\"\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*70)\n",
                "    print(f\"Generating visualizations for: {model_name}\")\n",
                "    print(\"=\"*70)\n",
                "    \n",
                "    model_output_dir = output_dir / model_name\n",
                "    model_output_dir.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    # Textâ†’Image Retrieval\n",
                "    print(\"\\nREQUIREMENT 1: Textâ†’Image Retrieval\")\n",
                "    print(\"-\" * 70)\n",
                "    \n",
                "    text_queries = [\n",
                "        \"sport\",\n",
                "        \"a dog playing\",\n",
                "        \"a person eating\",\n",
                "        \"a beautiful sunset\",\n",
                "        \"a cat on a couch\"\n",
                "    ]\n",
                "    \n",
                "    for query in text_queries:\n",
                "        print(f\"  Query: '{query}'\")\n",
                "        save_path = model_output_dir / f\"text2img_{query.replace(' ', '_')}.png\"\n",
                "        \n",
                "        visualize_text_to_image_retrieval(\n",
                "            query_text=query,\n",
                "            model=model,\n",
                "            image_paths=image_paths,\n",
                "            image_embeds=image_embeds,\n",
                "            clip_processor=processor,\n",
                "            device=device,\n",
                "            top_k=5,\n",
                "            save_path=save_path\n",
                "        )\n",
                "    \n",
                "    # Grid view\n",
                "    print(f\"  Creating retrieval grid...\")\n",
                "    grid_path = model_output_dir / \"text2img_grid.png\"\n",
                "    create_retrieval_grid(\n",
                "        queries=text_queries[:4],\n",
                "        model=model,\n",
                "        image_paths=image_paths,\n",
                "        image_embeds=image_embeds,\n",
                "        clip_processor=processor,\n",
                "        device=device,\n",
                "        images_per_query=5,\n",
                "        save_path=grid_path\n",
                "    )\n",
                "    \n",
                "    # Zero-Shot Classification\n",
                "    print(\"\\nREQUIREMENT 2: Zero-Shot Image Classification\")\n",
                "    print(\"-\" * 70)\n",
                "    \n",
                "    class_labels = ['a person', 'an animal', 'a landscape']\n",
                "    num_examples = 5\n",
                "    sample_images = image_paths[:num_examples]\n",
                "    \n",
                "    for idx, img_path in enumerate(sample_images):\n",
                "        print(f\"  Classifying image {idx+1}/{num_examples}: {img_path.name}\")\n",
                "        save_path = model_output_dir / f\"classification_example_{idx+1}.png\"\n",
                "        \n",
                "        predicted_class, confidence = zero_shot_classification(\n",
                "            query_image_path=img_path,\n",
                "            class_labels=class_labels,\n",
                "            model=model,\n",
                "            clip_processor=processor,\n",
                "            transform=transform,\n",
                "            device=device,\n",
                "            save_path=save_path\n",
                "        )\n",
                "        \n",
                "        print(f\"    â†’ Predicted: {predicted_class} ({confidence*100:.1f}%)\")\n",
                "    \n",
                "    print(f\"\\nâœ“ All visualizations for {model_name} saved to: {model_output_dir}\")\n",
                "\n",
                "\n",
                "print(\"âœ“ All functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Processor and Get Image Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load CLIP processor\n",
                "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
                "transform = get_clip_transform()\n",
                "\n",
                "# Get validation image paths\n",
                "all_image_paths = sorted(list(val_dir.glob(\"*.jpg\")))[:num_images]\n",
                "print(f\"\\nâœ“ Using {len(all_image_paths)} validation images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generate Visualizations for Base CLIP Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"PART 1: BASE CLIP MODEL (OpenAI Frozen)\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "base_clip = load_base_clip_model(device)\n",
                "base_clip_wrapped = BaseCLIPWrapper(base_clip, processor)\n",
                "\n",
                "# Precompute embeddings\n",
                "base_image_embeds = precompute_image_embeddings_base(\n",
                "    base_clip, all_image_paths, processor, device\n",
                ")\n",
                "\n",
                "# Generate visualizations\n",
                "generate_visualizations_for_model(\n",
                "    model=base_clip_wrapped,\n",
                "    model_name=\"base_model\",\n",
                "    image_paths=all_image_paths,\n",
                "    image_embeds=base_image_embeds,\n",
                "    processor=processor,\n",
                "    transform=transform,\n",
                "    device=device,\n",
                "    output_dir=output_dir\n",
                ")\n",
                "\n",
                "# Clean up\n",
                "del base_clip, base_clip_wrapped, base_image_embeds\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "print(\"\\nâœ… Base model visualizations complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generate Visualizations for Fine-Tuned Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"PART 2: FINE-TUNED MODEL (Your Trained Model)\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "finetuned_model = load_finetuned_model(model_checkpoint_path, device)\n",
                "\n",
                "# Precompute embeddings\n",
                "finetuned_image_embeds = precompute_image_embeddings_finetuned(\n",
                "    finetuned_model, all_image_paths, transform, device\n",
                ")\n",
                "\n",
                "# Generate visualizations\n",
                "generate_visualizations_for_model(\n",
                "    model=finetuned_model,\n",
                "    model_name=\"finetuned_model\",\n",
                "    image_paths=all_image_paths,\n",
                "    image_embeds=finetuned_image_embeds,\n",
                "    processor=processor,\n",
                "    transform=transform,\n",
                "    device=device,\n",
                "    output_dir=output_dir\n",
                ")\n",
                "\n",
                "# Clean up\n",
                "del finetuned_model, finetuned_image_embeds\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "print(\"\\nâœ… Fine-tuned model visualizations complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary and Download Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"âœ… VISUALIZATION GENERATION COMPLETE\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nðŸ“ Output directory: {output_dir}\")\n",
                "print(f\"\\nGenerated visualizations:\")\n",
                "\n",
                "for subdir in output_dir.iterdir():\n",
                "    if subdir.is_dir():\n",
                "        num_files = len(list(subdir.glob(\"*.png\")))\n",
                "        print(f\"\\n  {subdir.name}/ ({num_files} images)\")\n",
                "        for file_path in sorted(subdir.glob(\"*.png\"))[:5]:\n",
                "            size_kb = file_path.stat().st_size / 1024\n",
                "            print(f\"    â€¢ {file_path.name} ({size_kb:.1f} KB)\")\n",
                "        if num_files > 5:\n",
                "            print(f\"    ... and {num_files - 5} more\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"Lab 4 Section 2.4 Requirements:\")\n",
                "print(\"  âœ“ Textâ†’Image retrieval (including 'sport' and 'a dog playing')\")\n",
                "print(\"  âœ“ Image classification with ['a person', 'an animal', 'a landscape']\")\n",
                "print(\"  âœ“ Comparison between base and fine-tuned models\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Archive visualizations for download\n",
                "import shutil\n",
                "\n",
                "archive_name = '/content/Lab4_Visualizations'\n",
                "shutil.make_archive(archive_name, 'zip', output_dir)\n",
                "print(f\"\\nâœ“ Created archive: {archive_name}.zip\")\n",
                "\n",
                "# Download the archive\n",
                "files.download(f'{archive_name}.zip')\n",
                "print(\"\\nâœ“ Download started! Check your browser downloads.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optional: Copy to Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copy visualizations to Google Drive\n",
                "import shutil\n",
                "\n",
                "drive_output_dir = '/content/drive/MyDrive/Lab4_Visualizations'\n",
                "\n",
                "if os.path.exists(drive_output_dir):\n",
                "    shutil.rmtree(drive_output_dir)\n",
                "\n",
                "shutil.copytree(output_dir, drive_output_dir)\n",
                "print(f\"âœ“ Visualizations copied to Google Drive: {drive_output_dir}\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}