{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CLIP Model Evaluation on Google Colab (FAST VERSION)\n",
                "\n",
                "**Evaluates ALL models with Recall@K metrics**\n",
                "\n",
                "‚úÖ **Copies images to local storage (100x faster!)**\n",
                "‚úÖ **No zipping needed - just upload folder**\n",
                "‚úÖ **Progress bars & saves to Drive**\n",
                "\n",
                "---\n",
                "\n",
                "## üìã Upload to Google Drive:\n",
                "\n",
                "```\n",
                "My Drive/elec475_lab4/\n",
                "  models/\n",
                "    *.pth files\n",
                "  data/\n",
                "    text_embeddings_val.pt\n",
                "    coco_val/              ‚Üê Just the folder!\n",
                "      COCO_val2014_*.jpg\n",
                "```\n",
                "\n",
                "**No zipping needed!** Just upload the folder as-is.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Mount Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "os.chdir('/content')\n",
                "\n",
                "import torch\n",
                "print(\"=\" * 80)\n",
                "print(\"GPU CHECK\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers torch torchvision tqdm pillow matplotlib pandas\n",
                "print(\"‚úì Dependencies installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Clone Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "if os.path.exists('475_ML-CV_Labs'):\n",
                "    shutil.rmtree('475_ML-CV_Labs')\n",
                "\n",
                "!git clone https://github.com/Jcub05/475_ML-CV_Labs.git\n",
                "os.chdir('475_ML-CV_Labs/Lab4')\n",
                "print(f\"‚úì Directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Configure Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "\n",
                "# Drive paths (slow)\n",
                "DRIVE_ROOT = Path(\"/content/drive/MyDrive/elec475_lab4\")\n",
                "MODELS_DIR = DRIVE_ROOT / \"models\"\n",
                "DATA_DIR = DRIVE_ROOT / \"data\"\n",
                "VAL_EMBEDDINGS_DRIVE = DATA_DIR / \"text_embeddings_val.pt\"\n",
                "VAL_IMAGES_DRIVE = DATA_DIR / \"coco_val\"  # ‚Üê Folder in Drive\n",
                "\n",
                "# Local Colab paths (FAST!)\n",
                "LOCAL_DATA = Path(\"/content/data\")\n",
                "LOCAL_DATA.mkdir(exist_ok=True)\n",
                "VAL_IMAGES_LOCAL = LOCAL_DATA / \"coco_val\"  # ‚Üê Copy to local\n",
                "VAL_EMBEDDINGS_LOCAL = LOCAL_DATA / \"text_embeddings_val.pt\"\n",
                "\n",
                "# Results (save to Drive)\n",
                "RESULTS_DIR = DRIVE_ROOT / \"results\"\n",
                "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"PATH CONFIGURATION\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"Models (Drive): {MODELS_DIR}\")\n",
                "print(f\"Images (Drive): {VAL_IMAGES_DRIVE}\")\n",
                "print(f\"Images (Local): {VAL_IMAGES_LOCAL} ‚Üê FAST!\")\n",
                "print(f\"Results (Drive): {RESULTS_DIR}\")\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Copy Images to Local Storage (FAST!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "\n",
                "if not VAL_IMAGES_DRIVE.exists():\n",
                "    print(f\"‚ùå ERROR: {VAL_IMAGES_DRIVE} not found!\")\n",
                "    print(\"Please upload coco_val/ folder to your Drive data/ folder\")\n",
                "else:\n",
                "    img_count_drive = len(list(VAL_IMAGES_DRIVE.glob(\"*.jpg\")))\n",
                "    print(f\"üìÅ Found in Drive: {img_count_drive} images\")\n",
                "    \n",
                "    if VAL_IMAGES_LOCAL.exists():\n",
                "        img_count_local = len(list(VAL_IMAGES_LOCAL.glob(\"*.jpg\")))\n",
                "        if img_count_local == img_count_drive:\n",
                "            print(f\"‚úì Already in local storage ({img_count_local} images)\")\n",
                "        else:\n",
                "            print(f\"‚ö† Partial copy detected, removing...\")\n",
                "            shutil.rmtree(VAL_IMAGES_LOCAL)\n",
                "    \n",
                "    if not VAL_IMAGES_LOCAL.exists():\n",
                "        print(f\"\\n‚è≥ Copying images to local storage...\")\n",
                "        print(f\"   From: {VAL_IMAGES_DRIVE}\")\n",
                "        print(f\"   To: {VAL_IMAGES_LOCAL}\")\n",
                "        print(f\"   This takes ~5-8 minutes for ~40K images...\")\n",
                "        \n",
                "        # Use rsync for faster copying with progress\n",
                "        !rsync -ah --progress \"{VAL_IMAGES_DRIVE}/\" \"{VAL_IMAGES_LOCAL}/\"\n",
                "        \n",
                "        img_count_local = len(list(VAL_IMAGES_LOCAL.glob(\"*.jpg\")))\n",
                "        print(f\"\\n‚úì Copied {img_count_local} images to local storage\")\n",
                "    \n",
                "    print(f\"\\nüöÄ Reading from local = 100x faster than Drive!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Copy Embeddings to Local Storage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not VAL_EMBEDDINGS_LOCAL.exists():\n",
                "    print(f\"Copying embeddings...\")\n",
                "    shutil.copy(VAL_EMBEDDINGS_DRIVE, VAL_EMBEDDINGS_LOCAL)\n",
                "    print(\"‚úì Copied\")\n",
                "else:\n",
                "    print(\"‚úì Embeddings already local\")\n",
                "\n",
                "print(f\"Size: {VAL_EMBEDDINGS_LOCAL.stat().st_size / 1e6:.1f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Find All Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_files = sorted(MODELS_DIR.glob(\"*.pth\"))\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(f\"FOUND {len(model_files)} MODEL(S)\")\n",
                "print(\"=\" * 80)\n",
                "for i, mf in enumerate(model_files, 1):\n",
                "    print(f\"{i}. {mf.name} ({mf.stat().st_size / 1e6:.1f} MB)\")\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Load Model & Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from model import CLIPFineTuneModel\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "from tqdm.auto import tqdm\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "def load_model(checkpoint_path):\n",
                "    print(f\"\\nLoading: {checkpoint_path.name}\")\n",
                "    model = CLIPFineTuneModel(\n",
                "        embed_dim=512,\n",
                "        pretrained_resnet=True,\n",
                "        clip_model_name=\"openai/clip-vit-base-patch32\",\n",
                "        freeze_text_encoder=True\n",
                "    ).to(device)\n",
                "    \n",
                "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
                "    if 'model_state_dict' in checkpoint:\n",
                "        model.load_state_dict(checkpoint['model_state_dict'])\n",
                "    else:\n",
                "        model.load_state_dict(checkpoint)\n",
                "    \n",
                "    model.eval()\n",
                "    print(\"‚úì Loaded\")\n",
                "    return model\n",
                "\n",
                "class ValidationDataset(Dataset):\n",
                "    def __init__(self, images_dir, embeddings_file, transform=None):\n",
                "        self.images_dir = Path(images_dir)\n",
                "        self.transform = transform\n",
                "        \n",
                "        print(f\"Loading embeddings...\")\n",
                "        embeddings_cache = torch.load(embeddings_file)\n",
                "        \n",
                "        all_embeddings = []\n",
                "        all_image_ids = []\n",
                "        \n",
                "        for key, embedding in embeddings_cache.items():\n",
                "            image_id_str, caption_idx = key.rsplit('_', 1)\n",
                "            if caption_idx == '0':\n",
                "                all_embeddings.append(embedding)\n",
                "                all_image_ids.append(int(image_id_str))\n",
                "        \n",
                "        valid_embeddings = []\n",
                "        valid_paths = []\n",
                "        valid_ids = []\n",
                "        \n",
                "        print(\"Building dataset...\")\n",
                "        for img_id, emb in tqdm(zip(all_image_ids, all_embeddings), total=len(all_image_ids), desc=\"Checking\"):\n",
                "            img_path = self.images_dir / f\"COCO_val2014_{img_id:012d}.jpg\"\n",
                "            if img_path.exists():\n",
                "                valid_embeddings.append(emb)\n",
                "                valid_paths.append(img_path)\n",
                "                valid_ids.append(img_id)\n",
                "        \n",
                "        self.embeddings = torch.stack(valid_embeddings)\n",
                "        self.image_paths = valid_paths\n",
                "        self.image_ids = valid_ids\n",
                "        print(f\"‚úì {len(self)} samples\")\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.image_ids)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "        return {\n",
                "            'image': image,\n",
                "            'text_embedding': self.embeddings[idx],\n",
                "            'image_id': self.image_ids[idx]\n",
                "        }\n",
                "\n",
                "print(\"‚úì Functions ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(\n",
                "        mean=(0.48145466, 0.4578275, 0.40821073),\n",
                "        std=(0.26862954, 0.26130258, 0.27577711)\n",
                "    )\n",
                "])\n",
                "\n",
                "val_dataset = ValidationDataset(\n",
                "    images_dir=VAL_IMAGES_LOCAL,  # ‚Üê LOCAL!\n",
                "    embeddings_file=VAL_EMBEDDINGS_LOCAL,\n",
                "    transform=transform\n",
                ")\n",
                "\n",
                "val_loader = DataLoader(\n",
                "    val_dataset,\n",
                "    batch_size=128,\n",
                "    shuffle=False,\n",
                "    num_workers=2,\n",
                "    pin_memory=True\n",
                ")\n",
                "\n",
                "print(f\"\\n‚úì Dataloader ready ({len(val_dataset)} samples)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Metrics & Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_recall_at_k_gpu(similarity_matrix, k_values=[1, 5, 10]):\n",
                "    n = similarity_matrix.shape[0]\n",
                "    device = similarity_matrix.device\n",
                "    metrics = {}\n",
                "    max_k = max(k_values)\n",
                "    \n",
                "    _, top_k_indices = torch.topk(similarity_matrix, k=max_k, dim=1)\n",
                "    correct_indices = torch.arange(n, device=device).unsqueeze(1)\n",
                "    \n",
                "    for k in k_values:\n",
                "        correct_in_top_k = (top_k_indices[:, :k] == correct_indices).any(dim=1)\n",
                "        metrics[f'img2txt_r{k}'] = correct_in_top_k.float().mean().item() * 100\n",
                "    \n",
                "    _, top_k_indices = torch.topk(similarity_matrix.T, k=max_k, dim=1)\n",
                "    \n",
                "    for k in k_values:\n",
                "        correct_in_top_k = (top_k_indices[:, :k] == correct_indices).any(dim=1)\n",
                "        metrics[f'txt2img_r{k}'] = correct_in_top_k.float().mean().item() * 100\n",
                "    \n",
                "    avg_img2txt = np.mean([metrics[f'img2txt_r{k}'] for k in k_values])\n",
                "    avg_txt2img = np.mean([metrics[f'txt2img_r{k}'] for k in k_values])\n",
                "    metrics['avg_img2txt'] = avg_img2txt\n",
                "    metrics['avg_txt2img'] = avg_txt2img\n",
                "    metrics['avg_recall'] = (avg_img2txt + avg_txt2img) / 2\n",
                "    \n",
                "    return metrics\n",
                "\n",
                "def evaluate_model(model, dataloader, model_name=\"Model\"):\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"EVALUATING: {model_name}\")\n",
                "    print(f\"{'='*80}\")\n",
                "    \n",
                "    model.eval()\n",
                "    all_image_embeds = []\n",
                "    all_text_embeds = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in tqdm(dataloader, desc=\"Extracting\"):\n",
                "            images = batch['image'].to(device)\n",
                "            text_embeddings = batch['text_embedding'].to(device)\n",
                "            image_embeds, text_embeds = model(images, text_embeddings=text_embeddings)\n",
                "            all_image_embeds.append(image_embeds)\n",
                "            all_text_embeds.append(text_embeds)\n",
                "    \n",
                "    all_image_embeds = torch.cat(all_image_embeds, dim=0)\n",
                "    all_text_embeds = torch.cat(all_text_embeds, dim=0)\n",
                "    \n",
                "    similarity = all_image_embeds @ all_text_embeds.T\n",
                "    metrics = compute_recall_at_k_gpu(similarity, k_values=[1, 5, 10])\n",
                "    \n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"RESULTS: {model_name}\")\n",
                "    print(f\"{'='*80}\")\n",
                "    print(f\"\\nüìä Image ‚Üí Text:\")\n",
                "    print(f\"  R@1:  {metrics['img2txt_r1']:.2f}%\")\n",
                "    print(f\"  R@5:  {metrics['img2txt_r5']:.2f}%\")\n",
                "    print(f\"  R@10: {metrics['img2txt_r10']:.2f}%\")\n",
                "    print(f\"\\nüìä Text ‚Üí Image:\")\n",
                "    print(f\"  R@1:  {metrics['txt2img_r1']:.2f}%\")\n",
                "    print(f\"  R@5:  {metrics['txt2img_r5']:.2f}%\")\n",
                "    print(f\"  R@10: {metrics['txt2img_r10']:.2f}%\")\n",
                "    print(f\"\\nüéØ Avg: {metrics['avg_recall']:.2f}%\")\n",
                "    print(f\"{'='*80}\\n\")\n",
                "    \n",
                "    return metrics\n",
                "\n",
                "print(\"‚úì Ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Evaluate ALL Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "\n",
                "import json\n",
                "\n",
                "all_results = {}\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(f\"EVALUATING {len(model_files)} MODEL(S)\")\n",
                "print(f\"{'='*80}\\n\")\n",
                "\n",
                "for idx, model_file in enumerate(tqdm(model_files, desc=\"Overall\"), 1):\n",
                "    model_name = model_file.stem\n",
                "    print(f\"\\n[{idx}/{len(model_files)}] {model_name}\")\n",
                "    print(\"-\" * 80)\n",
                "    \n",
                "    model = load_model(model_file)\n",
                "    metrics = evaluate_model(model, val_loader, model_name=model_name)\n",
                "    all_results[model_name] = metrics\n",
                "    \n",
                "    temp_file = RESULTS_DIR / f\"{model_name}_metrics.json\"\n",
                "    with open(temp_file, 'w') as f:\n",
                "        json.dump(metrics, f, indent=2)\n",
                "    print(f\"üíæ {temp_file.name}\")\n",
                "    \n",
                "    del model\n",
                "    torch.cuda.empty_cache()\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(f\"‚úÖ DONE\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "summary_data = []\n",
                "for model_name, metrics in all_results.items():\n",
                "    summary_data.append({\n",
                "        'Model': model_name,\n",
                "        'I2T R@1': f\"{metrics['img2txt_r1']:.2f}%\",\n",
                "        'I2T R@5': f\"{metrics['img2txt_r5']:.2f}%\",\n",
                "        'I2T R@10': f\"{metrics['img2txt_r10']:.2f}%\",\n",
                "        'T2I R@1': f\"{metrics['txt2img_r1']:.2f}%\",\n",
                "        'T2I R@5': f\"{metrics['txt2img_r5']:.2f}%\",\n",
                "        'T2I R@10': f\"{metrics['txt2img_r10']:.2f}%\",\n",
                "        'Avg': f\"{metrics['avg_recall']:.2f}%\"\n",
                "    })\n",
                "\n",
                "summary_df = pd.DataFrame(summary_data)\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "print(summary_df.to_string(index=False))\n",
                "print(\"=\"*80)\n",
                "\n",
                "csv_path = RESULTS_DIR / \"evaluation_results.csv\"\n",
                "summary_df.to_csv(csv_path, index=False)\n",
                "json_path = RESULTS_DIR / \"detailed_metrics.json\"\n",
                "with open(json_path, 'w') as f:\n",
                "    json.dump(all_results, f, indent=2)\n",
                "\n",
                "print(f\"\\nüíæ Saved to Drive: {RESULTS_DIR}\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}